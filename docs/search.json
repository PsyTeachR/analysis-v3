[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analysis",
    "section": "",
    "text": "Overview\nThis course covers data skills R Markdown, data wrangling, and data visualisation. This book also introduces learners to the most common statistical analyses such as t-test, correlations, regressions, and ANOVAs. The main idea of this book is being reproducible in our data analysis approach.\nHow To Use This Book\nEach chapter in the book has the same structure:\n\n\nIndividual walkthrough: Complete this section at your own pace. Take your time to complete the guided activities. Plan for time pockets in your week to work through this section.\n\nPair coding: During the lab we are reserving time for you to work through activities with a peer. If you get stuck, try to solve the issue. A GTA and your tutor will be available to help you, too. Note: We have added a pair coding activity to every chapter in the book - even for chapters that are not associated with a lab. Feel free to get together with a peer to complete the tasks.\n\nTest your knowledge: The final section in each chapter is a short quiz that assesses your knowledge and skills covered in the chapter.\n\nTo support you to work through the book chapters continuously, we have scheduled each chapter so that it aligns as well as possible with the stats lectures and the lab content. We also made sure that no chapters are scheduled during busy assessment times (such as during the run-up to the research report deadline). You will have chapters in weeks 1-5 and then again weeks 9-11.\nIf you get stuck, seek help during your lab, GTA or PAL support sessions, and/or by posting on the Data Skills and R channel on Teams.\nStatement on use of AI\nChatGPT 4.0 was used to assist in the writing of these materials in the following ways:\n\nTo suggest multiple-choice questions in the “Test your knowledge and challenge yourself” section\nTo proof-read and check for typos\nTo suggest improvements to the text\n\nAny information provided by ChatGPT was verified, for example, where code was used, the syntax and output was checked to ensure it was correct and where theoretical or conceptual information was provided, only that which the author could verify from their pre-existing expertise was included.\nNote & Contact: This book is currently being updated which means that chapters are being published on a rolling basis. We regularly check and update for improvements. If you have any feedback or suggestions, please submit them to our Analysis R Book Improvement form. For people outwith University of Glasgow: You are welcome to share feedback by emailing Gaby Mahrholz.\nAcknowledgement of previous versions: This version of the book was adapted from a previous version written by Phil McAleer, Carolina E. Kuepper-Tetzel, & Helena M. Paterson\nR Version: This book has been written with R version 4.4.1 (2024-06-14 ucrt) (Race for Your Life) and RStudio version 2023.12.1+402 “Ocean Storm”."
  },
  {
    "objectID": "01-basics.html#intended-learning-outcomes",
    "href": "01-basics.html#intended-learning-outcomes",
    "title": "1  Projects and R Markdown",
    "section": "Intended Learning Outcomes",
    "text": "Intended Learning Outcomes\nBy the end of this chapter, you should be able to:\n\nRe-familiarise yourself with setting up projects\nRe-familiarise yourself with RMarkdown documents\nRecap and apply data wrangling procedures to analyse data"
  },
  {
    "objectID": "01-basics.html#individual-walkthrough",
    "href": "01-basics.html#individual-walkthrough",
    "title": "1  Projects and R Markdown",
    "section": "Individual Walkthrough",
    "text": "Individual Walkthrough"
  },
  {
    "objectID": "01-basics.html#r-and-r-studio",
    "href": "01-basics.html#r-and-r-studio",
    "title": "1  Projects and R Markdown",
    "section": "\n1.1 R and R Studio",
    "text": "1.1 R and R Studio\nRemember, R is a programming language that you will write code in and RStudio is an Integrated Development Environment (IDE) which makes working with R easier as it’s more user friendly. You need both components for this course.\nIf this is not ringing any bells yet, have a quick browse through the materials from year 1 to refresh your memory.\n\n1.1.1 R server\nUse the server only if you are unable to install R and RStudio on your computer (e.g., if you are using a Chromebook) or if you encounter issues while installing R on your own machine. Otherwise, you should install R and RStudio directly on your own computer. R and RStudio are already installed on the R server.\nYou will find the link to the server on Moodle.\n\n1.1.2 Installing R and RStudio on your computer\nThe RSetGo book provides detailed instructions on how to install R and RStudio on your computer. It also includes links to walkthroughs for installing R on different types of computers and operating systems.\nIf you had R and RStudio installed on your computer last year, we recommend updating to the latest versions. In fact, it’s a good practice to update them at the start of each academic year. Detailed guidance can be found in Appendix B.\nOnce you have installed or updated R and RStudio, return to this chapter.\n\n1.1.3 Settings for Reproducibility\nBy now, you should be aware that the Psychology department at the University of Glasgow places a strong emphasis on reproducibility, open science, and raising awareness about questionable research practices (QRPs) and how to avoid them. Therefore, it’s important that you work in a reproducible manner so that others (and your future self) can understand and check your work. This also makes it easier for you to reuse your work in the future.\nAlways start with a clear workspace. If your Global Environment contains anything from a previous session, you can’t be certain whether your current code is working as intended or if it’s using objects created earlier.\nTo ensure a clean and reproducible workflow, there are a few settings you should adjust immediately after installing or updating RStudio. In Tools &gt; Global Options… General tab\n\nUncheck the box labelled Restore .RData into workspace at startup to make sure no data from a previous session is loaded into the environment\nset Save workspace to .RData on exit to Never to prevent your workspace from being saved when you exit RStudio.\n\n\n\nReproducibility settings in Global Options\n\n\n\n\n\n\n\nTip for keeping taps on parentheses\n\n\n\n\n\nR has included rainbow parentheses to help with keeping count on the brackets.\nTo enable the feature, go to Tools &gt; Global Options… Code tab &gt; Display tab and tick the last checkbox “Use rainbow parentheses”\n\n\nEnable Rainbow parenthesis\n\n\n\n\n\n1.1.4 RStudio panes\nRStudio has four main panes each in a quadrant of your screen:\n\nSource pane\nEnvironment pane\nConsole pane\nOutput pane\n\n\n\n\n\n\n\nYour Turn\n\n\n\nAre you ready for a quick quiz to see what you remember about the RStudio panes from last year? Click on Quiz to see the questions.\n\n\n\n\n\n\nQuiz\n\n\n\n\n\nWhat is their purpose?\nThe Source pane…\n\nallows users to view and edit various code-related files, such as .Rmd filescontains the Files, Plots, R Packages, Help, Tutorial, Viewer, and Presentation tabsincludes the Environment tab that displays currently saved objects, and the History tab that displays the commands that were executed in the current session along a search functionprovides an area to interactively execute code\n\nThe Environment pane…\n\nallows users to view and edit various code-related files, such as .Rmd filescontains the Files, Plots, R Packages, Help, Tutorial, Viewer, and Presentation tabsincludes the Environment tab that displays currently saved objects, and the History tab that displays the commands that were executed in the current session along a search functionprovides an area to interactively execute code\n\nThe Console pane…\n\nallows users to view and edit various code-related files, such as .Rmd filescontains the Files, Plots, R Packages, Help, Tutorial, Viewer, and Presentation tabsincludes the Environment tab that displays currently saved objects, and the History tab that displays the commands that were executed in the current session along a search functionprovides an area to interactively execute code\n\nThe Output pane…\n\nallows users to view and edit various code-related files, such as .Rmd filescontains the Files, Plots, R Packages, Help, Tutorial, Viewer, and Presentation tabsincludes the Environment tab that displays currently saved objects, and the History tab that displays the commands that were executed in the current session along a search functionprovides an area to interactively execute code\n\nWhere are these panes located by default?\n\nThe Source pane is located? \ntop right\nbottom left\nbottom right\ntop left\n\nThe Environment pane is located? \nbottom right\ntop left\ntop right\nbottom left\n\nThe Console pane is located? \nbottom right\ntop left\nbottom left\ntop right\n\nThe Output pane is located? \ntop right\nbottom left\nbottom right\ntop left\n\n\n\n\n\n\n\nIf you were not quite sure about one/any of the panes, check out the materials from Level 1. If you want to know more about them, there is the RStudio guide on posit"
  },
  {
    "objectID": "01-basics.html#sec-project",
    "href": "01-basics.html#sec-project",
    "title": "1  Projects and R Markdown",
    "section": "\n1.2 Activity 1: Creating a new project",
    "text": "1.2 Activity 1: Creating a new project\nIt’s important to create a new RStudio project whenever you start a new project. This practice makes it easier to work in multiple contexts, such as when analysing different datasets simultaneously. Each RStudio project has its own folder location, workspace, and working directories, which keeps all your data and RMarkdown documents organised in one place.\nLast year, you learnt how to create projects on the server, so you already know the steps. If cannot quite recall how that was done, go back to the Level 1 materials.\nOn your own computer, open RStudio, and complete the following steps in this order:\n\nClick on File &gt; New Project…\n\nThen, click on “New Directory”\nThen, click on “New Project”\nName the directory something meaningful (e.g., “2A_chapter1”), and save it in a location that makes sense, for example, a dedicated folder you have for your level 2 Psychology labs - you can either select a folder you have already in place or create a new one (e.g., I named my new folder “Level 2 labs”)\nClick “Create Project”. RStudio will restart itself and open with this new project directory as the working directory. If you accidentally close it, you can open it by double-clicking on the project icon in your folder\nYou can also check in your folder structure that everything was created as intended\n\n\n\nCreating a new project\n\n\n\n\n\n\n\nWhy is the Colour scheme in the gif different to my version?\n\n\n\n\n\nIn case anyone is wondering why my colour scheme in the gif above looks different to yours, I’ve set mine to “Pastel On Dark” in Tools &gt; Global Options… &gt; Appearances. And my computer lives in “dark mode”.\n\n\n\n\n\n\n\n\n\nDon’t nest projects\n\n\n\nDon’t ever save a new project inside another project directory. This can cause some hard-to-resolve problems."
  },
  {
    "objectID": "01-basics.html#sec-rmd",
    "href": "01-basics.html#sec-rmd",
    "title": "1  Projects and R Markdown",
    "section": "\n1.3 Activity 2: Create a new R Markdown file",
    "text": "1.3 Activity 2: Create a new R Markdown file\n\nOpen a new R Markdown document: click File &gt; New File &gt; R Markdown or click on the little page icon with a green plus sign (top left).\nGive it a meaningful Title (e.g., Level 2 chapter 1) - you can also change the title later. Feel free to add your name or GUID in the Author field author name. Keep the Default Output Format as HTML.\nOnce the .Rmd opened, you need to save the file.\nTo save it, click File &gt; Save As… or click on the little disc icon. Name it something meaningful (e.g., “chapter_01.Rmd”, “01_intro.Rmd”). Make sure there are no spaces in the name - R is not very fond of spaces… This file will automatically be saved in your project folder (i.e., your working directory) so you should now see this file appear in your file viewer pane.\n\n\n\nCreating a new .Rmd file\n\nRemember, an R Markdown document or .Rmd has “white space” (i.e., the markdown for formatted text) and “grey parts” (i.e., code chunks) in the default colour scheme (see Figure 1.1). R Markdown is a powerful tool for creating dynamic documents because it allows you to integrate code and regular text seamlessly. You can then knit your .Rmd using the knitr package to create a final document as either a webpage (HTML), a PDF, or a Word document (.docx). We’ll only knit to HTML documents in this course.\n\n\nR markdown anatomy (image from https://intro2r.com/r-markdown-anatomy.html)\n\n\n1.3.1 Markdown\nThe markdown space in an .Rmd is ideal for writing notes that explain your code and document your thought process. Use this space to clarify what your code is doing, why certain decisions were made, and any insights or conclusions you have drawn along the way. These notes are invaluable when revisiting your work later, helping you (or others) understand the rationale behind key decisions, such as setting inclusion/exclusion criteria or interpreting the results of assumption tests. Effectively documenting your work in the markdown space enhances both the clarity and reproducibility of your analysis.\nThe markdown space offers a variety of formatting options to help you organise and present your notes effectively. Here are a few of them that can enhance your documentation:\nHeading levels\nThere is a variety of heading levels to make use of, using the # symbol.\n\n\nYou would incorporate this into your text as:\n# Heading level 1\n## Heading level 2\n### Heading level 3\n#### Heading level 4\n##### Heading level 5\n###### Heading level 6\n\n\nAnd it will be displayed in your knitted html file as:\n\n\n\n\n\n\n\n\n\n\nERROR: My heading levels don’t render properly when knitting\n\n\n\n\n\nYou need a space between the # and the first letter. If the space is missing, the heading will be displayed in the HTML file as …\n#Heading 1\n\n\n\nUnordered and ordered lists\nYou can also include unordered lists and ordered lists. Click on the tabs below to see how they are incorporated\n\n\nunordered lists\nordered lists\nordered lists magic\n\n\n\nYou can add bullet points using either *, - or + and they will turn into:\n\nbullet point (created with *)\nbullet point (created with -)\nbullet point (created with +)\n\nor use bullet points of different levels using 1 tab key press or 2 spaces (for sub-item 1) or 2 tabs/4 spaces (for sub-sub-item 1):\n\nbullet point item 1\n\nsub-item 1\n\nsub-sub-item 1\nsub-sub-item 2\n\n\n\n\nbullet point item 2\n\n\n\n\n\n\n\nERROR: My bullet points don’t render properly when knitting\n\n\n\n\n\nYou need an empty row before your bullet points start. If I delete the empty row before the bullet points, they will be displayed in the HTML as …\nText without the empty row: * bullet point created with * - bullet point created with - + bullet point created with +\n\n\n\n\n\nStart the line with 1., 2., etc. When you want to include sub-items, either use the tab key twice or add 4 spaces. Same goes for the sub-sub-item: include either 2 tabs (or 4 manual spaces) from the last item or 4 tabs/ 8 spaces from the start of the line.\n\nlist item 1\nlist item 2\n\nsub-item 1 (with 4 spaces) A. sub-sub-item 1 (with an additional 4 spaces from the last indent)\n\n\n\n\n\n\n\n\n\nMy list items don’t render properly when knitting\n\n\n\n\n\nIf you don’t leave enough spaces, the list won’t be recognised, and your output looks like this:\n\nlist item 3\n\n\nsub-item 1 (with only 2 spaces) A. sub-sub-item 1 (with an additional 2 spaces from the last indent)\n\n\n\n\n\n\nThe great thing though is that you don’t need to know your alphabet or number sequences. R markdown will fix that for you\nIf I type into my .Rmd…\n\n…it will be rendered in the knitted HTML output as…\n\nlist item 3\nlist item 1\n\nsub-item labelled “a)”\nsub-item labelled “i)”\n\nsub-item labelled “C)”\nsub-item labelled “Z)”\n\n\n\n\nlist item 7\n\n\n\n\n\n\n\nERROR: The labels of the sub-items are not what I thought they would be. You said they are fixing themselves…\n\n\n\n\n\nYes, they do but you need to label your sub-item lists accordingly. The first label you list in each level is set as the baseline. If they are labelled 1) instead of i) or A., the output will show as follows, but the automatic-item-fixing still works:\n\nlist item 7\n\nlist item “1)” with 4 spaces\n\nlist item “1)” with 8 spaces\nthis is an item labelled “6)” (magically corrected to “2.”)\n\n\n\n\n\n\n\n\n\n\n\nEmphasis\nInclude emphasis to draw attention to keywords in your text:\n\n\nR markdown syntax\nDisplayed in the knitted HTML file\n\n\n\n**bold text**\nbold text\n\n\n*italic text*\nitalic text\n\n\n***bold and italic***\nbold and italic\n\n\n\nOther examples can be found in the R Markdown Cheat Sheet\n\n1.3.2 Code chunks\nEverything you write inside the code chunks will be interpreted as code and executed by R. Code chunks start with ``` followed by an {r} which specifies the coding language R, some space for code, and ends with ```. If you accidentally delete one of those backticks, your code won’t run and/or your text parts will be interpreted as part of the code chunks or vice versa. This should be evident from the colour change - more white than expected typically indicates missing starting backticks, whilst too much grey/not enough white suggests missing ending backticks. But no need to fret if that happens - just add the missing backticks manually.\nYou can insert a new code chunk in several ways:\n\nClick the Insert a new code chunk button in the RStudio Toolbar (green icon at the top right corner of the Source pane).\nSelect Code &gt; Insert Chunk from the menu.\nUsing the shortcut Ctrl + Alt + I for Windows or Cmd + Option + I on MacOSX.\nType ```{r} and ``` manually\n\n\n\n\n\nFigure 1.1: Default .Rmd with highlighting - names in pink and knitr display options in purple\n\n\n\nWithin the curly brackets of a code chunk, you can specify a name for the code chunk (see pink highlighting in Figure 1.1). The chunk name is not necessarily required; however, it is good practice to give each chunk a unique name to support more advanced knitting approaches. It also makes it easier to reference and manage chunks.\nWithin the curly brackets, you can also place rules and arguments (see purple highlighting in Figure 1.1) to control how your code is executed and what is displayed in your final HTML output. The most common knitr display options include:\n\n\n\n\n\n\n\n\nCode\nDoes code run\nDoes code show\nDo results show\n\n\n\neval=FALSE\nNO\nYES\nNO\n\n\necho=TRUE (default)\nYES\nYES\nYES\n\n\necho=FALSE\nYES\nNO\nYES\n\n\nresults=‘hide’\nYES\nYES\nNO\n\n\ninclude=FALSE\nYES\nNO\nNO\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe table above will be incredibly important for the data skills homework II. When solving error mode items you will need to pay attention to the first one eval = FALSE.\n\n\nOne last thing: In your newly created .Rmd file, delete everything below line 12 (keep the set-up code chunk) and save your .Rmd by clicking on the disc symbol.\n\n\nDelete everything below line 12\n\n\n\n\n\n\n\nYour Turn\n\n\n\nThat was quite a long section about what Markdown can do. I promise, we’ll practice that more later. For the minute, we want you to create a new level 2 heading on line 12 and give it a meaningful heading title (something like “Loading packages and reading in data” or “Chapter 1”).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn line 12, you should have typed ## Loading packages and reading in data (or whatever meaningful title you chose). This will create level 2 heading once we knit the .Rmd."
  },
  {
    "objectID": "01-basics.html#sec-download_data_ch1",
    "href": "01-basics.html#sec-download_data_ch1",
    "title": "1  Projects and R Markdown",
    "section": "\n1.4 Activity 3: Download the data",
    "text": "1.4 Activity 3: Download the data\nThe data for chapters 1-3. Download it here: data_ch1.zip. There are 2 csv files contained in a zip folder. One is the data file we are going to use today prp_data_reduced.csv and the other is an Excel file prp_codebook that explains the variables in the data.\nThe first step is to unzip the zip folder so that the files are placed within the same folder as your project.\n\nPlace the zip folder within your 2A_chapter1 folder\nRight mouse click –&gt; Extract All...\n\nCheck the folder location is the one to extract the files to\nCheck the extracted files are placed next to the project icon\nFiles and project should be visible in the Output pane in RStudio\n\n\n\n\n\n\n\nScreenshots for “unzipping a zip folder”\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnzipping a zip folder\n\n\n\n\n\n\nThe paper by Pownall et al. was a registered report published in 2023, and the original data can be found on OSF (https://osf.io/5qshg/).\nCitation\n\nPownall, M., Pennington, C. R., Norris, E., Juanchich, M., Smailes, D., Russell, S., Gooch, D., Evans, T. R., Persson, S., Mak, M. H. C., Tzavella, L., Monk, R., Gough, T., Benwell, C. S. Y., Elsherif, M., Farran, E., Gallagher-Mitchell, T., Kendrick, L. T., Bahnmueller, J., . . . Clark, K. (2023). Evaluating the Pedagogical Effectiveness of Study Preregistration in the Undergraduate Dissertation. Advances in Methods and Practices in Psychological Science, 6(4). https://doi.org/10.1177/25152459231202724\n\nAbstract\n\nResearch shows that questionable research practices (QRPs) are present in undergraduate final-year dissertation projects. One entry-level Open Science practice proposed to mitigate QRPs is “study preregistration,” through which researchers outline their research questions, design, method, and analysis plans before data collection and/or analysis. In this study, we aimed to empirically test the effectiveness of preregistration as a pedagogic tool in undergraduate dissertations using a quasi-experimental design. A total of 89 UK psychology students were recruited, including students who preregistered their empirical quantitative dissertation (n = 52; experimental group) and students who did not (n = 37; control group). Attitudes toward statistics, acceptance of QRPs, and perceived understanding of Open Science were measured both before and after dissertation completion. Exploratory measures included capability, opportunity, and motivation to engage with preregistration, measured at Time 1 only. This study was conducted as a Registered Report; Stage 1 protocol: https://osf.io/9hjbw (date of in-principle acceptance: September 21, 2021). Study preregistration did not significantly affect attitudes toward statistics or acceptance of QRPs. However, students who preregistered reported greater perceived understanding of Open Science concepts from Time 1 to Time 2 compared with students who did not preregister. Exploratory analyses indicated that students who preregistered reported significantly greater capability, opportunity, and motivation to preregister. Qualitative responses revealed that preregistration was perceived to improve clarity and organization of the dissertation, prevent QRPs, and promote rigor. Disadvantages and barriers included time, perceived rigidity, and need for training. These results contribute to discussions surrounding embedding Open Science principles into research training.\n\nChanges made to the dataset\nWe made some changes to the dataset for the purpose of increasing difficulty for data wrangling (Chapter 2 and Chapter 3) and data visualisation (Chapter 4 and ?sec-dataviz2). This will ensure some “teachable moments”. The changes are as follows:\n\nWe removed some of the variables to make the data more manageable for teaching purposes.\nWe recoded some values from numeric responses to labels (e.g., understanding).\nWe added the word “years” to one of the Age entries.\nWe tidied a messy column Ethnicity but introduced a similar but easier-to-solve “messiness pattern” when recoding the understanding data.\nThe scores in the original file were already corrected from reverse-coded responses. We reversed that process to present raw data here."
  },
  {
    "objectID": "01-basics.html#activity-4-loading-packages-and-reading-in-data",
    "href": "01-basics.html#activity-4-loading-packages-and-reading-in-data",
    "title": "1  Projects and R Markdown",
    "section": "\n1.5 Activity 4: Loading packages and reading in data",
    "text": "1.5 Activity 4: Loading packages and reading in data\nThe first step is to load in the packages we need and read in the data. Today, we’ll only be using tidyverse, and read_csv() will help us store the data from prp_data_reduced.csv in an object called data_prp.\nCopy the code into a code chunk in your .Rmd file and run it. You can either click the green error to run the entire code chunk, or use the shortcut Ctrl + Enter (Windows) or Cmd + Enter (Mac) to run a line of code/ pipe from the Rmd.\n\nlibrary(tidyverse)\ndata_prp &lt;- read_csv(\"prp_data_reduced.csv\")\n\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nRows: 89 Columns: 91\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (17): Code, Age, Ethnicity, Opptional_mod_1_TEXT, Research_exp_1_TEXT, U...\ndbl (74): Gender, Secondyeargrade, Opptional_mod, Research_exp, Plan_prereg,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "01-basics.html#sec-familiarise",
    "href": "01-basics.html#sec-familiarise",
    "title": "1  Projects and R Markdown",
    "section": "\n1.6 Activity 5: Familiarise yourself with the data",
    "text": "1.6 Activity 5: Familiarise yourself with the data\n\nLook at the Codebook to get a feel of the variables in the dataset and how they have been measured. Note that some of the columns were deleted in the dataset you have been given.\nYou’ll notice that some questionnaire data was collected at 2 different time points (i.e., SATS28, QRPs, Understanding_OS)\nsome of the data was only collected at one time point (i.e., supervisor judgements, OS_behav items, and Included_prereg variables are t2-only variables)\n\n\n1.6.1 First glimpse at the data\nBefore you start wrangling your data, it is important to understand what kind of data you’re working with and what the format of your dataframe looks like.\nAs you may have noticed, read_csv() provides a message listing the data types in your dataset and how many columns are of each type. Plus, it shows a few examples columns for each data type.\nTo obtain more detailed information about your data, you have several options. Click on the individual tabs to see the different options available. Test them out in your own .Rmd file and use whichever method you prefer (but do it).\n\n\n\n\n\n\nWarning\n\n\n\nSome of the output is a bit long because we do have quite a few variables in the data file.\n\n\n\n\nvisual inspection 1\nglimpse()\nspec()\nvisual inspection 2\n\n\n\nIn the Global Environment, click the blue arrow icon next to the object name data_prp. This action will expand the object, revealing details about its columns. The $ symbol is commonly used in Base R to access a specific column within your dataframe.\n\n\nVisual inspection of the data\n\nCon: When you have quite a few variables, not all of them are shown.\n\n\nUse glimpse() if you want a more detailed overview you can see on your screen. The output will display rows and column numbers, and some examples of the first couple of observations for each variable.\n\nglimpse(data_prp)\n\nRows: 89\nColumns: 91\n$ Code                                  &lt;chr&gt; \"Tr10\", \"Bi07\", \"SK03\", \"SM95\", …\n$ Gender                                &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2,…\n$ Age                                   &lt;chr&gt; \"22\", \"20\", \"22\", \"26\", \"22\", \"2…\n$ Ethnicity                             &lt;chr&gt; \"White European\", \"White British…\n$ Secondyeargrade                       &lt;dbl&gt; 2, 3, 1, 2, 2, 2, 2, 2, 1, 1, 1,…\n$ Opptional_mod                         &lt;dbl&gt; 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2,…\n$ Opptional_mod_1_TEXT                  &lt;chr&gt; \"Research methods in first year\"…\n$ Research_exp                          &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ Research_exp_1_TEXT                   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Plan_prereg                           &lt;dbl&gt; 1, 3, 1, 2, 1, 1, 3, 3, 2, 2, 2,…\n$ SATS28_1_Affect_Time1                 &lt;dbl&gt; 4, 5, 5, 6, 2, 1, 6, 3, 2, 5, 2,…\n$ SATS28_2_Affect_Time1                 &lt;dbl&gt; 5, 6, 3, 3, 6, 1, 2, 2, 7, 3, 4,…\n$ SATS28_3_Affect_Time1                 &lt;dbl&gt; 3, 2, 5, 2, 6, 7, 2, 6, 6, 5, 2,…\n$ SATS28_4_Affect_Time1                 &lt;dbl&gt; 4, 5, 2, 2, 6, 6, 5, 5, 5, 5, 2,…\n$ SATS28_5_Affect_Time1                 &lt;dbl&gt; 5, 5, 5, 6, 1, 1, 5, 1, 2, 5, 2,…\n$ SATS28_6_Affect_Time1                 &lt;dbl&gt; 5, 6, 2, 5, 6, 7, 4, 5, 5, 3, 5,…\n$ SATS28_7_CognitiveCompetence_Time1    &lt;dbl&gt; 4, 2, 2, 5, 6, 7, 2, 5, 5, 2, 2,…\n$ SATS28_8_CognitiveCompetence_Time1    &lt;dbl&gt; 2, 2, 2, 1, 6, 7, 2, 5, 3, 2, 3,…\n$ SATS28_9_CognitiveCompetence_Time1    &lt;dbl&gt; 2, 2, 2, 3, 3, 7, 2, 6, 3, 3, 1,…\n$ SATS28_10_CognitiveCompetence_Time1   &lt;dbl&gt; 6, 7, 6, 6, 4, 2, 6, 4, 5, 6, 5,…\n$ SATS28_11_CognitiveCompetence_Time1   &lt;dbl&gt; 4, 3, 5, 5, 3, 1, 6, 2, 5, 6, 5,…\n$ SATS28_12_CognitiveCompetence_Time1   &lt;dbl&gt; 3, 5, 3, 5, 5, 7, 3, 4, 7, 2, 3,…\n$ SATS28_13_Value_Time1                 &lt;dbl&gt; 1, 1, 2, 1, 3, 7, 1, 2, 1, 2, 4,…\n$ SATS28_14_Value_Time1                 &lt;dbl&gt; 7, 7, 6, 6, 5, 1, 6, 5, 7, 6, 2,…\n$ SATS28_15_Value_Time1                 &lt;dbl&gt; 7, 7, 6, 6, 3, 5, 6, 6, 6, 5, 5,…\n$ SATS28_16_Value_Time1                 &lt;dbl&gt; 2, 1, 3, 2, 6, 5, 3, 7, 2, 2, 2,…\n$ SATS28_17_Value_Time1                 &lt;dbl&gt; 1, 1, 3, 3, 7, 7, 2, 7, 2, 2, 5,…\n$ SATS28_18_Value_Time1                 &lt;dbl&gt; 3, 6, 5, 3, 1, 1, 5, 1, 5, 2, 2,…\n$ SATS28_19_Value_Time1                 &lt;dbl&gt; 3, 3, 3, 3, 7, 7, 4, 5, 3, 5, 6,…\n$ SATS28_20_Value_Time1                 &lt;dbl&gt; 2, 1, 4, 2, 7, 7, 2, 4, 2, 2, 7,…\n$ SATS28_21_Value_Time1                 &lt;dbl&gt; 2, 1, 3, 2, 6, 7, 2, 5, 1, 3, 5,…\n$ SATS28_22_Difficulty_Time1            &lt;dbl&gt; 3, 2, 5, 3, 2, 1, 4, 2, 2, 5, 3,…\n$ SATS28_23_Difficulty_Time1            &lt;dbl&gt; 5, 6, 5, 6, 6, 7, 4, 6, 7, 5, 6,…\n$ SATS28_24_Difficulty_Time1            &lt;dbl&gt; 2, 2, 2, 3, 1, 4, 4, 2, 2, 2, 2,…\n$ SATS28_25_Difficulty_Time1            &lt;dbl&gt; 6, 7, 5, 5, 6, 7, 5, 6, 5, 5, 5,…\n$ SATS28_26_Difficulty_Time1            &lt;dbl&gt; 4, 2, 2, 2, 6, 7, 4, 5, 3, 5, 3,…\n$ SATS28_27_Difficulty_Time1            &lt;dbl&gt; 4, 5, 5, 3, 6, 7, 4, 3, 5, 3, 6,…\n$ SATS28_28_Difficulty_Time1            &lt;dbl&gt; 1, 7, 5, 5, 6, 6, 5, 4, 4, 4, 2,…\n$ QRPs_1_Time1                          &lt;dbl&gt; 7, 7, 7, 7, 7, 7, 6, 2, 7, 6, 7,…\n$ QRPs_2_Time1                          &lt;dbl&gt; 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 5,…\n$ QRPs_3_Time1                          &lt;dbl&gt; 5, 2, 6, 2, 6, 4, 6, 3, 7, 3, 3,…\n$ QRPs_4_Time1                          &lt;dbl&gt; 7, 7, 6, 6, 7, 4, 6, 7, 7, 7, 6,…\n$ QRPs_5_Time1                          &lt;dbl&gt; 3, 3, 7, 7, 2, 7, 4, 6, 7, 3, 2,…\n$ QRPs_6_Time1                          &lt;dbl&gt; 4, 7, 6, 5, 7, 4, 4, 5, 7, 6, 5,…\n$ QRPs_7_Time1                          &lt;dbl&gt; 5, 7, 7, 7, 7, 4, 5, 6, 7, 7, 5,…\n$ QRPs_8_Time1                          &lt;dbl&gt; 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7,…\n$ QRPs_9_Time1                          &lt;dbl&gt; 6, 7, 7, 4, 7, 7, 3, 7, 6, 6, 2,…\n$ QRPs_10_Time1                         &lt;dbl&gt; 7, 6, 5, 2, 5, 4, 2, 6, 7, 7, 2,…\n$ QRPs_11_Time1                         &lt;dbl&gt; 7, 7, 7, 4, 7, 7, 4, 6, 7, 7, 5,…\n$ QRPs_12NotQRP_Time1                   &lt;dbl&gt; 2, 2, 1, 4, 1, 4, 2, 4, 2, 2, 1,…\n$ QRPs_13NotQRP_Time1                   &lt;dbl&gt; 1, 1, 1, 1, 1, 4, 2, 4, 1, 1, 1,…\n$ QRPs_14NotQRP_Time1                   &lt;dbl&gt; 1, 4, 3, 4, 1, 4, 2, 3, 3, 4, 3,…\n$ QRPs_15NotQRP_Time1                   &lt;dbl&gt; 2, 4, 2, 2, 1, 4, 2, 1, 4, 4, 2,…\n$ Understanding_OS_1_Time1              &lt;chr&gt; \"2\", \"2\", \"6\", \"2\", \"6\", \"Not at…\n$ Understanding_OS_2_Time1              &lt;chr&gt; \"2\", \"Not at all confident\", \"2\"…\n$ Understanding_OS_3_Time1              &lt;chr&gt; \"2\", \"Not at all confident\", \"3\"…\n$ Understanding_OS_4_Time1              &lt;chr&gt; \"6\", \"Not at all confident\", \"6\"…\n$ Understanding_OS_5_Time1              &lt;chr&gt; \"Entirely confident\", \"6\", \"6\", …\n$ Understanding_OS_6_Time1              &lt;chr&gt; \"Entirely confident\", \"Entirely …\n$ Understanding_OS_7_Time1              &lt;chr&gt; \"6\", \"Not at all confident\", \"2\"…\n$ Understanding_OS_8_Time1              &lt;chr&gt; \"6\", \"3\", \"5\", \"3\", \"5\", \"Not at…\n$ Understanding_OS_9_Time1              &lt;chr&gt; \"Entirely confident\", \"6\", \"5\", …\n$ Understanding_OS_10_Time1             &lt;chr&gt; \"Entirely confident\", \"6\", \"5\", …\n$ Understanding_OS_11_Time1             &lt;chr&gt; \"Entirely confident\", \"2\", \"4\", …\n$ Understanding_OS_12_Time1             &lt;chr&gt; \"Entirely confident\", \"2\", \"5\", …\n$ Pre_reg_group                         &lt;dbl&gt; 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 2,…\n$ Other_OS_behav_2                      &lt;dbl&gt; 1, NA, NA, NA, 1, NA, NA, 1, NA,…\n$ Other_OS_behav_4                      &lt;dbl&gt; 1, NA, NA, NA, NA, NA, NA, NA, N…\n$ Other_OS_behav_5                      &lt;dbl&gt; NA, NA, NA, NA, 1, 1, NA, NA, NA…\n$ Closely_follow                        &lt;dbl&gt; 2, 2, 2, NA, 3, 3, 3, NA, NA, 2,…\n$ SATS28_Affect_Time2_mean              &lt;dbl&gt; 3.500000, 3.166667, 4.833333, 4.…\n$ SATS28_CognitiveCompetence_Time2_mean &lt;dbl&gt; 4.166667, 4.666667, 6.166667, 5.…\n$ SATS28_Value_Time2_mean               &lt;dbl&gt; 3.000000, 6.222222, 6.000000, 4.…\n$ SATS28_Difficulty_Time2_mean          &lt;dbl&gt; 2.857143, 2.857143, 4.000000, 2.…\n$ QRPs_Acceptance_Time2_mean            &lt;dbl&gt; 5.636364, 5.454545, 6.272727, 5.…\n$ Time2_Understanding_OS                &lt;dbl&gt; 5.583333, 3.333333, 5.416667, 4.…\n$ Supervisor_1                          &lt;dbl&gt; 5, 7, 7, 1, 7, 1, 7, 6, 7, 5, 6,…\n$ Supervisor_2                          &lt;dbl&gt; 5, 6, 7, 4, 6, 2, 7, 5, 6, 5, 5,…\n$ Supervisor_3                          &lt;dbl&gt; 6, 7, 7, 1, 7, 1, 7, 5, 6, 6, 7,…\n$ Supervisor_4                          &lt;dbl&gt; 6, 7, 7, 1, 7, 1, 7, 6, 7, 6, 6,…\n$ Supervisor_5                          &lt;dbl&gt; 5, 7, 7, 4, 7, 3, 7, 7, 6, 6, 6,…\n$ Supervisor_6                          &lt;dbl&gt; 5, 7, 7, 4, 6, 3, 7, 6, 7, 6, 6,…\n$ Supervisor_7                          &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Supervisor_8                          &lt;dbl&gt; 5, 5, 7, 1, 7, 1, 7, 5, 7, 5, 6,…\n$ Supervisor_9                          &lt;dbl&gt; 6, 7, 7, 4, 7, 3, 7, 5, 7, 6, 7,…\n$ Supervisor_10                         &lt;dbl&gt; 5, 7, 7, 1, 7, 1, 7, 6, 7, 6, 6,…\n$ Supervisor_11                         &lt;dbl&gt; NA, 7, 7, NA, 7, 1, 7, 5, 7, 6, …\n$ Supervisor_12                         &lt;dbl&gt; 4, 5, 7, 1, 4, 1, 7, 3, 6, 6, 5,…\n$ Supervisor_13                         &lt;dbl&gt; 4, 2, 5, 1, 2, 1, 6, 3, 5, 6, 5,…\n$ Supervisor_14                         &lt;dbl&gt; 5, 7, 7, 1, 7, 1, 7, 5, 7, 6, 6,…\n$ Supervisor_15_R                       &lt;dbl&gt; 1, 1, 1, 4, 1, 7, 1, 2, 1, 2, 1,…\n\n\n\n\nYou can also use spec() as suggested in the message above and then it shows you a list of the data type in every single column. But it doesn’t show you the number of rows and columns.\n\nspec(data_prp)\n\ncols(\n  Code = col_character(),\n  Gender = col_double(),\n  Age = col_character(),\n  Ethnicity = col_character(),\n  Secondyeargrade = col_double(),\n  Opptional_mod = col_double(),\n  Opptional_mod_1_TEXT = col_character(),\n  Research_exp = col_double(),\n  Research_exp_1_TEXT = col_character(),\n  Plan_prereg = col_double(),\n  SATS28_1_Affect_Time1 = col_double(),\n  SATS28_2_Affect_Time1 = col_double(),\n  SATS28_3_Affect_Time1 = col_double(),\n  SATS28_4_Affect_Time1 = col_double(),\n  SATS28_5_Affect_Time1 = col_double(),\n  SATS28_6_Affect_Time1 = col_double(),\n  SATS28_7_CognitiveCompetence_Time1 = col_double(),\n  SATS28_8_CognitiveCompetence_Time1 = col_double(),\n  SATS28_9_CognitiveCompetence_Time1 = col_double(),\n  SATS28_10_CognitiveCompetence_Time1 = col_double(),\n  SATS28_11_CognitiveCompetence_Time1 = col_double(),\n  SATS28_12_CognitiveCompetence_Time1 = col_double(),\n  SATS28_13_Value_Time1 = col_double(),\n  SATS28_14_Value_Time1 = col_double(),\n  SATS28_15_Value_Time1 = col_double(),\n  SATS28_16_Value_Time1 = col_double(),\n  SATS28_17_Value_Time1 = col_double(),\n  SATS28_18_Value_Time1 = col_double(),\n  SATS28_19_Value_Time1 = col_double(),\n  SATS28_20_Value_Time1 = col_double(),\n  SATS28_21_Value_Time1 = col_double(),\n  SATS28_22_Difficulty_Time1 = col_double(),\n  SATS28_23_Difficulty_Time1 = col_double(),\n  SATS28_24_Difficulty_Time1 = col_double(),\n  SATS28_25_Difficulty_Time1 = col_double(),\n  SATS28_26_Difficulty_Time1 = col_double(),\n  SATS28_27_Difficulty_Time1 = col_double(),\n  SATS28_28_Difficulty_Time1 = col_double(),\n  QRPs_1_Time1 = col_double(),\n  QRPs_2_Time1 = col_double(),\n  QRPs_3_Time1 = col_double(),\n  QRPs_4_Time1 = col_double(),\n  QRPs_5_Time1 = col_double(),\n  QRPs_6_Time1 = col_double(),\n  QRPs_7_Time1 = col_double(),\n  QRPs_8_Time1 = col_double(),\n  QRPs_9_Time1 = col_double(),\n  QRPs_10_Time1 = col_double(),\n  QRPs_11_Time1 = col_double(),\n  QRPs_12NotQRP_Time1 = col_double(),\n  QRPs_13NotQRP_Time1 = col_double(),\n  QRPs_14NotQRP_Time1 = col_double(),\n  QRPs_15NotQRP_Time1 = col_double(),\n  Understanding_OS_1_Time1 = col_character(),\n  Understanding_OS_2_Time1 = col_character(),\n  Understanding_OS_3_Time1 = col_character(),\n  Understanding_OS_4_Time1 = col_character(),\n  Understanding_OS_5_Time1 = col_character(),\n  Understanding_OS_6_Time1 = col_character(),\n  Understanding_OS_7_Time1 = col_character(),\n  Understanding_OS_8_Time1 = col_character(),\n  Understanding_OS_9_Time1 = col_character(),\n  Understanding_OS_10_Time1 = col_character(),\n  Understanding_OS_11_Time1 = col_character(),\n  Understanding_OS_12_Time1 = col_character(),\n  Pre_reg_group = col_double(),\n  Other_OS_behav_2 = col_double(),\n  Other_OS_behav_4 = col_double(),\n  Other_OS_behav_5 = col_double(),\n  Closely_follow = col_double(),\n  SATS28_Affect_Time2_mean = col_double(),\n  SATS28_CognitiveCompetence_Time2_mean = col_double(),\n  SATS28_Value_Time2_mean = col_double(),\n  SATS28_Difficulty_Time2_mean = col_double(),\n  QRPs_Acceptance_Time2_mean = col_double(),\n  Time2_Understanding_OS = col_double(),\n  Supervisor_1 = col_double(),\n  Supervisor_2 = col_double(),\n  Supervisor_3 = col_double(),\n  Supervisor_4 = col_double(),\n  Supervisor_5 = col_double(),\n  Supervisor_6 = col_double(),\n  Supervisor_7 = col_double(),\n  Supervisor_8 = col_double(),\n  Supervisor_9 = col_double(),\n  Supervisor_10 = col_double(),\n  Supervisor_11 = col_double(),\n  Supervisor_12 = col_double(),\n  Supervisor_13 = col_double(),\n  Supervisor_14 = col_double(),\n  Supervisor_15_R = col_double()\n)\n\n\n\n\nIn the Global Environment, click on the object name data_prp. This action will open the data in a new tab. Hovering over the column headings with your mouse will also reveal their data type. However, it seems to be a fairly tedious process when you have loads of columns.\n\n\n\n\n\n\nHang on, where is the rest of my data? Why do I only see 50 columns?\n\n\n\n\n\nOne common source of confusion is not seeing all your columns when you open up a data object as a tab. This is because RStudio shows you a maximum of 50 columns at a time. If you have more than 50 columns, navigate with the arrows to see the remaining columns.\n\n\nShowing 50 columns at a time\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour Turn\n\n\n\nNow that you have tested out all the options in your own .Rmd file, you can probably answer the following questions:\n\nHow many observations? \n\nHow many variables? \n\nHow many columns are col_character or chr data type? \n\nHow many columns are col_double or dbl data type? \n\n\n\n\n\n\n\n\nExplain the solution\n\n\n\n\n\nThe visual inspections shows you the number of observations and variables. glimpse() also gives you that information but calls them rows and columns respectively.\nThe data type information actually comes from the output when using the read_csv() function. Did you notice the information on Column specification (see screenshot below)?\n\n\nmessage from read_csv() when reading in the data\n\nWhilst spec() is quite useful for data type information per individual column, it doesn’t give you the total count of each data type. So it doesn’t really help with answering the questions here - unless you want to count manually from its extremely long output.\n\n\n\nIn your .Rmd, include a new heading level 2 called “Information about the data” (or something equally meaningful) and jot down some notes about data_prp. You could include the citation and/or the abstract, and whatever information you think you should note about this dataset (e.g., any observations from looking at the codebook?). You could also include some notes on the functions used so far and what they do. Try to incorporate some bold, italic or bold and italic emphasis and perhaps a bullet point or two.\n\n\n\n\n\n\nPossible solution\n\n\n\n\n\n## Information about the data\nThe data is from Pownall et al. (2023), and I can find the paper here: https://doi.org/10.1177/25152459231202724.\nI’ve noticed in the prp codebook that the SATS-28 questionnaire has quite a few *reverse-coded items*, and the supervisor support questionnaire also has a reverse-coded item.\nSo far, I think I prefer **glimpse()** to show me some more detail about the data. Specs() is too text-heavy for me which makes it hard to read.\nThings to keep in mind:\n\n**don’t forget to load in tidyverse first!!!**\nalways read in the data with **read_csv**, ***never ever use read.csv***!!!\n\n\n\nThe output rendered in a knitted html file\n\n\n\n\n\n\n\n1.6.2 Data types\nEach variable has a data type, such as numeric (numbers), character (text), and logical (TRUE/FALSE values), or a special class of factor. As you have just seen, our data_prp only has character and numeric columns (so far).\nNumeric data can be double (dbl) or integer (int). Doubles can have decimal places (e.g., 1.1). Integers are the whole numbers (e.g., 1, 2, -1) and are displayed with the suffix L (e.g., 1L). This is not overly important but might leave you less puzzled the next time you see an L after a number.\nCharacters (also called “strings”) is anything written between quotation marks. This is usually text, but in special circumstances, a number can be a character if it placed within quotation marks. This can happen when you are recoding variables. It might not be too obvious at the time, but you won’t be able to calculate anything if the number is a character\n\n\nExample data types\nnumeric computation\ncharacter computation\n\n\n\n\ntypeof(1)\ntypeof(1L)\ntypeof(\"1\")\ntypeof(\"text\")\n\n[1] \"double\"\n[1] \"integer\"\n[1] \"character\"\n[1] \"character\"\n\n\n\n\nNo problems here…\n\n1+1\n\n[1] 2\n\n\n\n\nWhen the data type is incorrect, you won’t be able to compute anything, despite your numbers being shown as numeric values in the dataframe. The error message tells you exactly what’s wrong with it, i.e., that you have non-numeric arguments.\n\n\"1\"+\"1\" # ERROR\n\nError in \"1\" + \"1\": non-numeric argument to binary operator\n\n\n\n\n\nLogical data (also sometimes called “Boolean” values) are one of two values: TRUE or FALSE (written in uppercase). They become really important when we use filter() or mutate() with conditional statements such as case_when(). More about those in Chapter 3.\nSome commonly used logical operators:\n\n\noperator\ndescription\n\n\n\n&gt;\ngreater than\n\n\n&gt;=\ngreater than or equal to\n\n\n&lt;\nless than\n\n\n&lt;=\nless than or equal to\n\n\n==\nequal to\n\n\n!=\nnot equal to\n\n\n%in%\nTRUE if any element is in the following vector\n\n\n\nA factor is a specific type of integer or character that lets you assign the order of the categories. This becomes useful when you want to display certain categories in “the correct order” either in a dataframe (see arrange) or when plotting (see Chapter 4/ ?sec-dataviz2).\n\n1.6.3 Variable types\nYou’ve already encountered them in Level 1 but let’s refresh. Variables can be classified as continuous (numbers) or categorical (labels).\nCategorical variables are properties you can count. They can be nominal, where the categories don’t have an order (e.g., gender) or ordinal (e.g., Likert scales either with numeric values 1-7 or with character labels such as “agree”, “neither agree nor disagree”, “disagree”). Categorical data may also be factors rather than characters.\nContinuous variables are properties you can measure and calculate sums/ means/ etc. They may be rounded to the nearest whole number, but it should make sense to have a value between them. Continuous variables always have a numeric data type (i.e. integer or double).\n\n\n\n\n\n\nWhy is this important you may ask?\n\n\n\nKnowing your variable and data types will help later on when deciding on an appropriate plot (see Chapter 4 and ?sec-dataviz2) or which inferential test to run (?sec-nhstI to ?sec-factorial).\n\n\n\n\n\n\n\n\nYour Turn\n\n\n\nAs we’ve seen earlier, data_prp only had character and numeric variables which hardly tests your understanding to see if you can identify a variety of data types and variable types. So, for this little quiz, we’ve spiced it up a bit. We’ve selected a few columns, shortened some of the column names, and modified some of the data types. Here you can see the first few rows of the new object data_quiz. You can find the code with explanations at the end of this section.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nAge\nGender\nEthnicity\nSecondyeargrade\nQRP_item\nQRPs_mean\nUnderstanding_item\nQRP_item &gt; 4\n\n\n\nTr10\n22\n2\nWhite European\n60-69% (2:1 grade)\n5\n5.636364\n2\nTRUE\n\n\nBi07\n20\n2\nWhite British\n50-59% (2:2 grade)\n2\n5.454546\n2\nFALSE\n\n\nSK03\n22\n2\nWhite British\n≥ 70% (1st class grade)\n6\n6.272727\n6\nTRUE\n\n\nSM95\n26\n2\nWhite British\n60-69% (2:1 grade)\n2\n5.000000\n2\nFALSE\n\n\nSt01\n22\n2\nWhite British\n60-69% (2:1 grade)\n6\n5.545454\n6\nTRUE\n\n\n\n\n\n\n\nglimpse(data_quiz)\n\nRows: 89\nColumns: 9\n$ Code               &lt;chr&gt; \"Tr10\", \"Bi07\", \"SK03\", \"SM95\", \"St01\", \"St10\", \"Wa…\n$ Age                &lt;chr&gt; \"22\", \"20\", \"22\", \"26\", \"22\", \"20\", \"21\", \"21\", \"22…\n$ Gender             &lt;fct&gt; 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ Ethnicity          &lt;chr&gt; \"White European\", \"White British\", \"White British\",…\n$ Secondyeargrade    &lt;fct&gt; 60-69% (2:1 grade), 50-59% (2:2 grade), ≥ 70% (1st …\n$ QRP_item           &lt;dbl&gt; 5, 2, 6, 2, 6, 4, 6, 3, 7, 3, 3, 4, 4, 4, 4, 6, 3, …\n$ QRPs_mean          &lt;dbl&gt; 5.636364, 5.454545, 6.272727, 5.000000, 5.545455, 6…\n$ Understanding_item &lt;chr&gt; \"2\", \"2\", \"6\", \"2\", \"6\", \"Not at all confident\", \"4…\n$ `QRP_item &gt; 4`     &lt;lgl&gt; TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE,…\n\n\nSelect from the dropdown menu the variable type and their data types for each of the columns.\n\n\n\n\n\n\n\nColumn\nVariable type\nData type\n\n\n\nAge\n\ncontinuous\nnominal\nordinal\n\nnumeric\ncharacter\nlogical\nfactor\n\n\nGender\n\ncontinuous\nnominal\nordinal\n\nnumeric\ncharacter\nlogical\nfactor\n\n\nEthinicity\n\ncontinuous\nnominal\nordinal\n\nnumeric\ncharacter\nlogical\nfactor\n\n\nSecondyeargrade\n\ncontinuous\nnominal\nordinal\n\nnumeric\ncharacter\nlogical\nfactor\n\n\nQRP_item\n\ncontinuous\nnominal\nordinal\n\nnumeric\ncharacter\nlogical\nfactor\n\n\nQRPs_mean\n\ncontinuous\nnominal\nordinal\n\nnumeric\ncharacter\nlogical\nfactor\n\n\nUnderstanding_item\n\ncontinuous\nnominal\nordinal\n\nnumeric\ncharacter\nlogical\nfactor\n\n\nQRP_item &gt; 4\n\ncontinuous\nnominal\nordinal\n\nnumeric\ncharacter\nlogical\nfactor\n\n\n\n\n\n\n\n\n\n\n\nRevealing the mystery code that created data_quiz\n\n\n\n\n\nThe code might look a bit complex for the minute despite the line-by-line explanations below. Come back to it after completing chapter 2.\n\ndata_quiz &lt;- data_prp %&gt;% \n  select(Code, Age, Gender, Ethnicity, Secondyeargrade, QRP_item = QRPs_3_Time1, QRPs_mean = QRPs_Acceptance_Time2_mean, Understanding_item = Understanding_OS_1_Time1) %&gt;% \n  mutate(Gender = factor(Gender),\n         Secondyeargrade = factor(Secondyeargrade,\n                                  levels = c(1, 2, 3, 4, 5),\n                                  labels = c(\"≥ 70% (1st class grade)\", \"60-69% (2:1 grade)\", \"50-59% (2:2 grade)\", \"40-49% (3rd class)\", \"&lt; 40%\")),\n         `QRP_item &gt; 4` = case_when(\n           QRP_item &gt; 4 ~ TRUE, \n           .default = FALSE))\n\nLets go through this line by line:\n\n\nline 1: creates a new object called data_quiz and it is based on the already existing data object data_prp\n\n\nline 2: we are selecting a few variables of interest, such as Code, Age etc. Some of those variables were renamed in the process according to the structure new_name = old_name, for example QRP item 3 at time point 1 got renamed as QRP_item.\n\n\nline 3: The function mutate() is used to create a new column called Gender that turns the existing column Gender from a numeric value into a factor. R simply overwrites the existing column of the same name. If we had named the new column Gender_factor, we would have been able to retain the original Gender column and Gender_factor would have been added as the last column.\n\nline 4-6: See how the line starts with an indent which indicates we are still within the mutate() function. You can also see this by counting brackets - in line 3 there are 2 opening brackets but only 1 closes.\n\nSimilar to Gender, we are replacing the “old” Secondyeargrade with the new Secondyeargrade column that is now a factor.\nTurning our variable Secondyeargrade into a factor, spot the difference between this attempt and the one we used for Gender? Here we are using a lot more arguments in that factor function, namely levels and labels. Levels describes the unique values we have for that column, and in labels we want to define how these levels will be shown in the data object. If you don’t add the levels and labels argument, the labels will be the labels (as you can see in the Gender column in which we kept the numbers).\n\n\n\nline 7: Doesn’t start with a function name and has an indent, which means we are still within the mutate() function - count the opening and closing brackets to confirm.\n\nHere, we are creating a new column called QRP_item &gt; 4. Notice the two backticks we have to use to make this weird column name work? This is because it has spaces (and we did mention that R doesn’t like spaces). So the backticks help R to group it as a unit/ a single name.\nNext we have a case_when() function which helps executing conditional statements. We are using it to check whether a statement is TRUE or FALSE. Here, we ask whether the QRP item (column QRP_item) is larger than 4 (midpoint of the scale) using the Boolean operator &gt;. If the statement is TRUE, the label TRUE should appear in column QRP_item &gt; 4. Otherwise, if the value is equal to 4 or smaller, the label should read FALSE. We will come back to conditional statements in Chapter 2. But long story short, this Boolean expression created the only logical data type in data_quiz.\n\n\n\n\n\n\nAnd with this, we are done with the individual walkthrough. Well done :)"
  },
  {
    "objectID": "01-basics.html#pair-coding",
    "href": "01-basics.html#pair-coding",
    "title": "1  Projects and R Markdown",
    "section": "Pair-coding",
    "text": "Pair-coding\nThe data we will be using in the upcoming lab activities is a randomised controlled trials experiment by Binfet et al. (2021) that was conducted in Canada.\nCitation\n\nBinfet, J. T., Green, F. L. L., & Draper, Z. A. (2021). The Importance of Client–Canine Contact in Canine-Assisted Interventions: A Randomized Controlled Trial. Anthrozoös, 35(1), 1–22. https://doi.org/10.1080/08927936.2021.1944558\n\nAbstract\n\nResearchers have claimed that canine-assisted interventions (CAIs) contribute significantly to bolstering participants’ wellbeing, yet the mechanisms within interactions have received little empirical attention. The aim of this study was to assess the impact of client–canine contact on wellbeing outcomes in a sample of 284 undergraduate college students (77% female; 21% male, 2% non-binary). Participants self-selected to participate and were randomly assigned to one of two canine interaction treatment conditions (touch or no touch) or to a handler-only condition with no therapy dog present. To assess self-reports of wellbeing, measures of flourishing, positive and negative affect, social connectedness, happiness, integration into the campus community, stress, homesickness, and loneliness were administered. Exploratory analyses were conducted to assess whether these wellbeing measures could be considered as measuring a unidimensional construct. This included both reliability analysis and exploratory factor analysis. Based on the results of these analyses we created a composite measure using participant scores on a latent factor. We then conducted the tests of the four hypotheses using these factor scores. Results indicate that participants across all conditions experienced enhanced wellbeing on several measures; however, only those in the direct contact condition reported significant improvements on all measures of wellbeing. Additionally, direct interactions with therapy dogs through touch elicited greater wellbeing benefits than did no touch/indirect interactions or interactions with only a dog handler. Similarly, analyses using scores on the wellbeing factor indicated significant improvement in wellbeing across all conditions (handler-only, d = 0.18, p = 0.041; indirect, d = 0.38, p &lt; 0.001; direct, d = 0.78, p &lt; 0.001), with more benefit when a dog was present (d = 0.20, p &lt; 0.001), and the most benefit coming from physical contact with the dog (d = 0.13, p = 0.002). The findings hold implications for post-secondary wellbeing programs as well as the organization and delivery of CAIs.\n\nHowever, we accessed the data via Ciaran Evans’ github (https://github.com/ciaran-evans/dog-data-analysis). Evans et al. (2023) published a paper that reused the Binfet data for teaching statistics and research methods. If anyone is interested, the accompanying paper is:\n\nEvans, C., Cipolli, W., Draper, Z. A., & Binfet, J. T. (2023). Repurposing a Peer-Reviewed Publication to Engage Students in Statistics: An Illustration of Study Design, Data Collection, and Analysis. Journal of Statistics and Data Science Education, 31(3), 236–247. https://doi.org/10.1080/26939169.2023.2238018\n\nThere are a few changes that Evans and we made to the data:\n\nEvans removed the demographics ethnicity and gender to make the study data available while protecting participant privacy. Which means we’ll have limited demographic variables available, but we will make do with what we’ve got.\nWe modified some of the responses in the raw data csv - for example, we took out impossible response values and replaced them with NA.\nWe replaced some of the numbers with labels to increase the difficulty in the dataset for Chapter 2 and Chapter 3.\n\nTask 1: Create a project folder for the lab activities\nSince we will be working with the same data throughout semester 1, create a separate project for the lab data. Name it something useful, like lab_data or dogs_in_the_lab. Make sure you are not placing it within the project you have already created today. If you need guidance, see Section 1.2 above.\nTask 2: Create a new .Rmd file\n… and name it something useful. If you need help, have a look at Section 1.3.\nTask 3: Download the data\nDownload the data here: data_pair_ch1. The zip folder contains the raw data file with responses to individual questions, a cleaned version of the same data in long format and wide format, and the codebook describing the variables in the raw data file and the long format.\nUnzip the folder and place the data files in the same folder as your project.\nTask 4: Familiarise yourself with the data\nOpen the data files, look at the codebook, and perhaps skim over the original Binfet article (methods in particular) to see what kind of measures they used.\nRead in the raw data file as dog_data_raw and the cleaned-up data (long format) as dog_data_long. See if you can answer the following questions.\n\nlibrary(tidyverse)\n\ndog_data_raw &lt;- read_csv(\"dog_data_raw.csv\")\ndog_data_long &lt;- read_csv(\"dog_data_clean_long.csv\")\n\n\n\nRows: 284 Columns: 136\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (41): GroupAssignment, L2_1, L2_2, L2_3, L2_4, L2_5, L2_6, L2_7, L2_8, L...\ndbl (95): RID, Age_Yrs, Year_of_Study, Live_Pets, Consumer_BARK, S1_1, HO1_1...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 568 Columns: 16\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): GroupAssignment, Year_of_Study, Live_Pets, Stage\ndbl (12): RID, Age_Yrs, Consumer_BARK, Flourishing, PANAS_PA, PANAS_NA, SHS,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nHow many participants took part in the study? \n\n\n\n\n\n\n\n\nExplain this answer\n\n\n\n\n\nYou can see this from dog_data_raw. Each participant ID is on a single row meaning the number of observations is the number of participants.\nIf you look at dog_data_long, there are 568 observations. Each participant answered the questionnaires pre and post intervention, resulting in 2 rows per participant ID. This means you’d have to divide the number of observations by 2 to get to the number of participants.\n\n\n\n\nHow many different questionnaires did the participants answer? \n\n\n\n\n\n\n\n\nExplain this answer\n\n\n\n\n\nThe Binfet paper (e.g., Methods section and/or abstract) and the codebook show it’s 9 questionnaires - Flourishing scale (variable Flourishing), the UCLS Loneliness scale Version 3 (Loneliness), Positive and Negative affect scale (PANAS_PA and PANAS_NA), the Subjective Happiness scale (SHS), the Social connectedness scale (SCS), and 3 scales with 1 question each, i.e., perception of stress levels (Stress), self-reported level of homesickness (Homesick), and integration into the campus community (Engagement).\nHowever, if you thought PANAS_PA and PANAS_NA are a single questionnaire, 8 was also acceptable as an answer here."
  },
  {
    "objectID": "01-basics.html#test-your-knowledge",
    "href": "01-basics.html#test-your-knowledge",
    "title": "1  Projects and R Markdown",
    "section": "Test your knowledge",
    "text": "Test your knowledge\nAre you ready for some knowledge check questions to test your understanding of the chapter? We also have some faulty codes. See if you can spot what’s wrong with them.\nKnowledge check\nQuestion 1\nOne of the key first steps when we open RStudio is to:\n\nput on some music as we will be here a whileopen an existing project or create a new onemake a coffeecheck out the news\n\n\n\n\n\n\n\nExplain this answer\n\n\n\n\n\nOpening an existing project (e.g., when coming back to the same dataset) or creating a new project (e.g., for a new task or new dataset) ensures that subsequent .Rmd files, any output, figures, etc are saved within the same folder on your computer (i.e., the working directory). If the.Rmd files or data is not in the same folder as “the project icon”, things can get messy and code might not run.\n\n\n\nQuestion 2\nWhen using the default environment colour settings for RStudio, what colour would the background of a code chunk be in R Markdown? \nred\nwhite\ngreen\ngrey\nWhen using the default environment colour settings for RStudio, what colour would the background of normal text be in R Markdown? \nred\nwhite\ngreen\ngrey\n\n\n\n\n\n\nExplain this answer\n\n\n\n\n\nAssuming you have not changed any of the settings in RStudio, code chunks will tend to have a grey background and normal text will tend to have a white background. This is a good way to check that you have closed and opened code chunks correctly.\n\n\n\nQuestion 3\nCode chunks start and end with:\n\nthree single quotesthree backticksthree double quotesthree single asterisks\n\n\n\n\n\n\n\nExplain this answer\n\n\n\n\n\nCode chunks always take the same general format of three backticks followed by curly parentheses and a lower case r inside the parentheses ({r}). People often mistake these backticks for single quotes but that will not work. If you have set your code chunk correctly using backticks, the background colour should change to grey from white.\n\n\n\nQuestion 4\nWhat is the correct way to include a code chunk in RMarkdown that will be executed but neither the code nor its output will be shown in the final HTML document? \n{r, echo=FALSE}\n{r, eval=FALSE}\n{r, include=FALSE}\n{r, results=‘hide’}\n\n\n\n\n\n\nExplain this answer\n\n\n\n\n\nCheck the table of knitr display options in Section 1.3.2.\n\n{r, echo=FALSE} also executes the code and does not show the code, but it does display the result in the knitted html file. (matches 2/3 criteria)\n{r, eval=FALSE} does not show the results but does not execute the code and it does show it in the knitted file. (matches 1/3 criteria)\n{r, results=“hide”} executes the code and does not show results, however, it does include the code in the knitted html document. (matches 2/3 criteria)\n\n\n\n\nError mode\nSome of these codes have mistakes in them, other code chunks are not quite producing what was aimed for. Your task is to spot anything faulty, explain why the things happened, and perhaps try to fix them.\nQuestion 5\nYou want to read in data with the read_csv() function. You have just stated R, created a new .Rmd file, and typed the following code into your code chunk.\n\ndata &lt;- read_csv(\"data.csv\")\n\nHowever, R gives you an error message: could not find function \"read_csv\". What could be the reason?\n\n\n\n\n\n\nExplain the solution\n\n\n\n\n\n“Could not find function” is an indication that you have forgotten to load in tidyverse. Because read_csv() is a function in the tidyverse collection, R cannot find it.\nFIX: Add library(tidyverse) prior to reading in the data and run the code chunk again.\n\n\n\nQuestion 6\nYou want to read in data with the read_csv() function. This time, you are certain you have loaded in tidyverse first. The code is as follows:\n\nlibrary(tidyverse)\ndata &lt;- read_csv(\"data.csv\")\n\nThe error message shows 'data.csv' does not exist in current working directory. You check your folder and it looks like this:\n\nWhy is there an error message?\n\n\n\n\n\n\nExplain the solution\n\n\n\n\n\nR is looking for a csv file that is called data which is currently not in the working directory. We may assume it’s in the data folder. Perhaps that happened when unzipping the zip file. So instead of placing the csv file on the same level as the project icon, it was unzipped into a folder named data.\nFIX - option 1: Take the data.csv out of the data folder and place it next to the project icon and the .Rmd file.\nFIX - option 2: Modify your R code to tell R that the data is in a separate folder called data, e.g., …\n\nlibrary(tidyverse)\ndata &lt;- read_csv(\"data/data.csv\")\n\n\n\n\nQuestion 7\nYou want to load tidyverse into the library. The code is as follows:\n\nlibrary(tidyverse)\n\nThe error message says: Error in library(tidyverse) : there is no package called ‘tidyverse’\nWhy is there an error message and how can we fix this?\n\n\n\n\n\n\nExplain the solution\n\n\n\n\n\nIf R says there is no package called tidyverse, means you haven’t installed the package yet. This could be an error message you receive either after switching computers or a fresh install of R and RStudio.\nFIX: Type install.packages(\"tidyverse\") into your Console.\n\n\n\nQuestion 8\nYou knitted your .Rmd into a html but the output is not as expected. You see the following:\n\nWhy did the file not knit properly?\n\n\n\n\n\n\nExplain the solution\n\n\n\n\n\nThere is a backtick missing in the code chunk. If you check your .Rmd file, you can see that the code chunk does not show up in grey which means it’s one of the 3 backticks at the beginning of the chunk.\n\nFIX: Add a single backtick manually where it’s missing."
  },
  {
    "objectID": "02-wrangling.html#intended-learning-outcomes",
    "href": "02-wrangling.html#intended-learning-outcomes",
    "title": "2  Data wrangling I",
    "section": "Intended Learning Outcomes",
    "text": "Intended Learning Outcomes\nIn the next two chapters, we will build on the data wrangling skills from level 1. We will revisit all the functions you have already encountered (and might have forgotten over the summer break) and introduce 2 or 3 new functions. These two chapters will provide an opportunity to revise and apply the functions to a novel dataset.\nBy the end of this chapter, you should be able to:\n\napply familiar data wrangling functions to novel datasets\nread and interpret error messages\nrealise there are several ways of getting to the results\nexport data objects as csv files\n\nThe main purpose of this chapter and Chapter 3 is to wrangle your data into shape for data visualisation (Chapter 4 and ?sec-dataviz2). For the two chapters, we will:\n\ncalculate demographics\ntidy 3 different questionnaires with varying degrees of complexity\nsolve an error mode problem\njoin all data objects together"
  },
  {
    "objectID": "02-wrangling.html#individual-walkthrough",
    "href": "02-wrangling.html#individual-walkthrough",
    "title": "2  Data wrangling I",
    "section": "Individual Walkthrough",
    "text": "Individual Walkthrough\nBefore we start, we need to set up some things."
  },
  {
    "objectID": "02-wrangling.html#activity-1-setup",
    "href": "02-wrangling.html#activity-1-setup",
    "title": "2  Data wrangling I",
    "section": "\n2.1 Activity 1: Setup",
    "text": "2.1 Activity 1: Setup\n\nWe will be working on the dataset by Pownall et al. (2023) again, which means we can still use the project we created last week. The data files will already be there, so no need to download them again.\nTo open the project in RStudio, go to the folder in which you stored the project and the data last time, and double click on the project icon.\n\nCreate a new .Rmd file for chapter 2 and save it to your project folder. Name it something meaningful (e.g., “chapter_02.Rmd”, “02_data_wrangling.Rmd”). See Section 1.3 if you need some guidance.\nIn your newly created .Rmd file, delete everything below line 12 (after the set-up code chunk)."
  },
  {
    "objectID": "02-wrangling.html#activity-2-load-in-the-libraries-and-read-in-the-data",
    "href": "02-wrangling.html#activity-2-load-in-the-libraries-and-read-in-the-data",
    "title": "2  Data wrangling I",
    "section": "\n2.2 Activity 2: Load in the libraries and read in the data",
    "text": "2.2 Activity 2: Load in the libraries and read in the data\nWe will use tidyverse today, and we want to create a data object data_prp that stores the data from the file prp_data_reduced.csv.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nlibrary(???)\ndata_prp &lt;- read_csv(\"???\")\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(tidyverse)\ndata_prp &lt;- read_csv(\"prp_data_reduced.csv\")\n\n\n\n\nIf you need a quick reminder what the dataset was about, have a look at the abstract in Section 1.4. We also addressed the changes we made to the dataset there.\nAnd remember to have a quick glimpse() at your data."
  },
  {
    "objectID": "02-wrangling.html#activity-3-calculating-demographics",
    "href": "02-wrangling.html#activity-3-calculating-demographics",
    "title": "2  Data wrangling I",
    "section": "\n2.3 Activity 3: Calculating demographics",
    "text": "2.3 Activity 3: Calculating demographics\nLet’s start with some simple data-wrangling steps to compute demographics for our original dataset, data_prp. First, we want to determine how many participants took part in the study by Pownall et al. (2023) and compute the mean age and the standard deviation of age for the sample.\n\n2.3.1 … for the full sample using summarise()\n\nThe summarise() function is part of the “Wickham Six” alongside group_by(), select(), filter(), mutate(), and arrange(). You used them plenty of times last year.\nWithin summarise(), we can use the n() function, which calculates the number of rows in the dataset. Since each row corresponds to a unique participant, this gives us the total number of participants.\nTo calculate the mean age and the standard deviation of age, we need to use the functions mean() and sd() on the column Age respectively.\n\ndemo_total &lt;- data_prp %&gt;% \n  summarise(n = n(), # participant number\n            mean_age = mean(Age), # mean age\n            sd_age = sd(Age)) # standard deviation of age\n\nWarning: There were 2 warnings in `summarise()`.\nThe first warning was:\nℹ In argument: `mean_age = mean(Age)`.\nCaused by warning in `mean.default()`:\n! argument is not numeric or logical: returning NA\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\ndemo_total\n\n\n\n\nn\nmean_age\nsd_age\n\n\n89\nNA\nNA\n\n\n\n\n\nR did not give us an error message per se, but the output is not quite as expected either. There are NA values in the mean_age and sd_age columns. Looking at the warning message and at Age, can you explain what happened?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe warning message says: argument is not numeric or logical: returning NA If we look at the Age column more closely, we can see that it’s a character data type.\n\n\n\nFixing Age\n\nMight be wise to look at the unique answers in column Age to determine what is wrong. We can do that with the function distinct().\n\nage_distinct &lt;- data_prp %&gt;% \n  distinct(Age)\n\nage_distinct\n\n\n\n\n\n\n\nShow the unique values of Age.\n\n\n\n\n\n\n\n\n\n\nAge\n\n\n\n22\n\n\n20\n\n\n26\n\n\n21\n\n\n29\n\n\n23\n\n\n39\n\n\nNA\n\n\n24\n\n\n43\n\n\n31\n\n\n25 years\n\n\n\n\n\n\n\n\n\n\n\nOne cell has the string “years” added to their number 25, which has converted the entire column into a character column.\nWe can easily fix this by extracting only the numbers from the column and converting it into a numeric data type. The parse_number() function, which is part of the tidyverse package, handles both steps in one go (so there’s no need to load additional packages).\nWe will combine this with the mutate() function to create a new column called Age (containing those numeric values), effectively replacing the old Age column (which had the character values).\n\n\n\nparse_number() illustration by Allison Horst (see https://allisonhorst.com/r-packages-functions)\n\n\n\n\ndata_prp &lt;- data_prp %&gt;% \n  mutate(Age = parse_number(Age))\n\ntypeof(data_prp$Age) # fixed\n\n[1] \"double\"\n\n\nComputing summary stats\nExcellent. Now that the numbers are in a numeric format, let’s try calculating the demographics for the total sample again.\n\ndemo_total &lt;- data_prp %&gt;% \n  summarise(n = n(), # participant number\n            mean_age = mean(Age), # mean age\n            sd_age = sd(Age)) # standard deviation of age\n\ndemo_total\n\n\n\n\nn\nmean_age\nsd_age\n\n\n89\nNA\nNA\n\n\n\n\n\nEven though there’s no error or warning, the table still shows NA values for mean_age and sd_age. So, what could possibly be wrong now?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nDid you notice that the Age column in age_distinct contains some missing values (NA)? To be honest, it’s easier to spot this issue in the actual R output than in the printed HTML page.\n\n\n\nComputing summary stats - third attempt\nTo ensure R ignores missing values during calculations, we need to add the extra argument na.rm = TRUE to the mean() and sd() functions.\n\ndemo_total &lt;- data_prp %&gt;% \n  summarise(n = n(), # participant number\n            mean_age = mean(Age, na.rm = TRUE), # mean age\n            sd_age = sd(Age, na.rm = TRUE)) # standard deviation of age\n\ndemo_total\n\n\n\n\nn\nmean_age\nsd_age\n\n\n89\n21.88506\n3.485603\n\n\n\n\n\nFinally, we’ve got it! 🥳 Third time’s the charm!\n\n2.3.2 … per gender using summarise() and group_by()\n\nNow we want to compute the summary statistics for each gender. The code inside the summarise() function remains unchanged; we just need to use the group_by() function beforehand to tell R that we want to compute the summary statistics for each group separately. It’s also a good practice to use ungroup() afterwards, so you are not taking groupings forward unintentionally.\n\ndemo_by_gender &lt;- data_prp %&gt;% \n  group_by(Gender) %&gt;% # split data up into groups (here Gender)\n  summarise(n = n(), # participant number \n            mean_age = mean(Age, na.rm = TRUE), # mean age \n            sd_age = sd(Age, na.rm = TRUE)) %&gt;%  # standard deviation of age\n  ungroup()\n\ndemo_by_gender\n\n\n\n\nGender\nn\nmean_age\nsd_age\n\n\n\n1\n17\n23.31250\n5.770254\n\n\n2\n69\n21.57353\n2.738973\n\n\n3\n3\n21.33333\n1.154700\n\n\n\n\n\n\n\n2.3.3 Adding percentages\nSometimes, it may be useful to calculate percentages, such as for the gender split. You can do this by adding a line within the summarise() function to perform the calculation. All we need to do is take the number of female, male, and non-binary participants (stored in the n column of demo_by_gender), divide it by the total number of participants (stored in the n column of demo_total), and multiply by 100. Let’s add percentage to the summarise() function of demo_by_gender. Make sure that the code for percentages is placed after the value for n has been computed.\nAccessing the value of n for the different gender categories is straightforward because we can refer back to it directly. However, since the total number of participants is stored in a different data object, we need to use a base R function to access it – specifically the $ operator. To do this, you simply type the name of the data object (in this case, demo_total), followed by the $ symbol (with no spaces), and then the name of the column you want to retrieve (in this case, n). The general pattern is data$column.\n\ndemo_by_gender &lt;- data_prp %&gt;% \n  group_by(Gender) %&gt;% \n  summarise(n = n(), \n            # n from the line above divided by n from demo_total *100\n            percentage = n/demo_total$n *100, \n            mean_age = mean(Age, na.rm = TRUE), \n            sd_age = sd(Age, na.rm = TRUE)) %&gt;% \n  ungroup()\n\ndemo_by_gender\n\n\n\n\nGender\nn\npercentage\nmean_age\nsd_age\n\n\n\n1\n17\n19.101124\n23.31250\n5.770254\n\n\n2\n69\n77.528090\n21.57353\n2.738973\n\n\n3\n3\n3.370786\n21.33333\n1.154700\n\n\n\n\n\n\n\n\n\n\n\n\nTip for decimal places - use round()\n\n\n\n\n\nNot super important, because you could round the values by yourself when writing up your reports, but if you wanted to tidy up the decimal places in the output, you can do that using the round() function. You would need to “wrap” it around your computations and specify how many decimal places you want to display (for example mean(Age) would turn into round(mean(Age), 1)). It may look odd for percentage, just make sure the number that specifies the decimal places is placed within the round function. The default value is 0 (meaning no decimal spaces).\n\ndemo_by_gender &lt;- data_prp %&gt;% \n  group_by(Gender) %&gt;% \n  summarise(n = n(), \n            percentage = round(n/demo_total$n *100, 2), # percentage with 2 decimal places\n            mean_age = round(mean(Age, na.rm = TRUE), 1), # mean Age with 1 decimal place\n            sd_age = round(sd(Age, na.rm = TRUE), 3)) %&gt;% # sd Age with 3 decimal places\n  ungroup()\n\ndemo_by_gender\n\n\n\n\nGender\nn\npercentage\nmean_age\nsd_age\n\n\n\n1\n17\n19.10\n23.3\n5.770\n\n\n2\n69\n77.53\n21.6\n2.739\n\n\n3\n3\n3.37\n21.3\n1.155"
  },
  {
    "objectID": "02-wrangling.html#sec-ch2_act4",
    "href": "02-wrangling.html#sec-ch2_act4",
    "title": "2  Data wrangling I",
    "section": "\n2.4 Activity 4: Questionable Research Practices (QRPs)",
    "text": "2.4 Activity 4: Questionable Research Practices (QRPs)\nThe main goal is to compute the mean QRP score per participant for time point 1.\nAt the moment, the data is in wide format. The table below shows data from the first 3 participants:\n\nhead(data_prp, n = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nGender\nAge\nEthnicity\nSecondyeargrade\nOpptional_mod\nOpptional_mod_1_TEXT\nResearch_exp\nResearch_exp_1_TEXT\nPlan_prereg\nSATS28_1_Affect_Time1\nSATS28_2_Affect_Time1\nSATS28_3_Affect_Time1\nSATS28_4_Affect_Time1\nSATS28_5_Affect_Time1\nSATS28_6_Affect_Time1\nSATS28_7_CognitiveCompetence_Time1\nSATS28_8_CognitiveCompetence_Time1\nSATS28_9_CognitiveCompetence_Time1\nSATS28_10_CognitiveCompetence_Time1\nSATS28_11_CognitiveCompetence_Time1\nSATS28_12_CognitiveCompetence_Time1\nSATS28_13_Value_Time1\nSATS28_14_Value_Time1\nSATS28_15_Value_Time1\nSATS28_16_Value_Time1\nSATS28_17_Value_Time1\nSATS28_18_Value_Time1\nSATS28_19_Value_Time1\nSATS28_20_Value_Time1\nSATS28_21_Value_Time1\nSATS28_22_Difficulty_Time1\nSATS28_23_Difficulty_Time1\nSATS28_24_Difficulty_Time1\nSATS28_25_Difficulty_Time1\nSATS28_26_Difficulty_Time1\nSATS28_27_Difficulty_Time1\nSATS28_28_Difficulty_Time1\nQRPs_1_Time1\nQRPs_2_Time1\nQRPs_3_Time1\nQRPs_4_Time1\nQRPs_5_Time1\nQRPs_6_Time1\nQRPs_7_Time1\nQRPs_8_Time1\nQRPs_9_Time1\nQRPs_10_Time1\nQRPs_11_Time1\nQRPs_12NotQRP_Time1\nQRPs_13NotQRP_Time1\nQRPs_14NotQRP_Time1\nQRPs_15NotQRP_Time1\nUnderstanding_OS_1_Time1\nUnderstanding_OS_2_Time1\nUnderstanding_OS_3_Time1\nUnderstanding_OS_4_Time1\nUnderstanding_OS_5_Time1\nUnderstanding_OS_6_Time1\nUnderstanding_OS_7_Time1\nUnderstanding_OS_8_Time1\nUnderstanding_OS_9_Time1\nUnderstanding_OS_10_Time1\nUnderstanding_OS_11_Time1\nUnderstanding_OS_12_Time1\nPre_reg_group\nOther_OS_behav_2\nOther_OS_behav_4\nOther_OS_behav_5\nClosely_follow\nSATS28_Affect_Time2_mean\nSATS28_CognitiveCompetence_Time2_mean\nSATS28_Value_Time2_mean\nSATS28_Difficulty_Time2_mean\nQRPs_Acceptance_Time2_mean\nTime2_Understanding_OS\nSupervisor_1\nSupervisor_2\nSupervisor_3\nSupervisor_4\nSupervisor_5\nSupervisor_6\nSupervisor_7\nSupervisor_8\nSupervisor_9\nSupervisor_10\nSupervisor_11\nSupervisor_12\nSupervisor_13\nSupervisor_14\nSupervisor_15_R\n\n\n\nTr10\n2\n22\nWhite European\n2\n1\nResearch methods in first year\n2\nNA\n1\n4\n5\n3\n4\n5\n5\n4\n2\n2\n6\n4\n3\n1\n7\n7\n2\n1\n3\n3\n2\n2\n3\n5\n2\n6\n4\n4\n1\n7\n7\n5\n7\n3\n4\n5\n7\n6\n7\n7\n2\n1\n1\n2\n2\n2\n2\n6\nEntirely confident\nEntirely confident\n6\n6\nEntirely confident\nEntirely confident\nEntirely confident\nEntirely confident\n1\n1\n1\nNA\n2\n3.500000\n4.166667\n3.000000\n2.857143\n5.636364\n5.583333\n5\n5\n6\n6\n5\n5\n1\n5\n6\n5\nNA\n4\n4\n5\n1\n\n\nBi07\n2\n20\nWhite British\n3\n2\nNA\n2\nNA\n3\n5\n6\n2\n5\n5\n6\n2\n2\n2\n7\n3\n5\n1\n7\n7\n1\n1\n6\n3\n1\n1\n2\n6\n2\n7\n2\n5\n7\n7\n7\n2\n7\n3\n7\n7\n7\n7\n6\n7\n2\n1\n4\n4\n2\nNot at all confident\nNot at all confident\nNot at all confident\n6\nEntirely confident\nNot at all confident\n3\n6\n6\n2\n2\n1\nNA\nNA\nNA\n2\n3.166667\n4.666667\n6.222222\n2.857143\n5.454546\n3.333333\n7\n6\n7\n7\n7\n7\n1\n5\n7\n7\n7\n5\n2\n7\n1\n\n\nSK03\n2\n22\nWhite British\n1\n2\nNA\n2\nNA\n1\n5\n3\n5\n2\n5\n2\n2\n2\n2\n6\n5\n3\n2\n6\n6\n3\n3\n5\n3\n4\n3\n5\n5\n2\n5\n2\n5\n5\n7\n7\n6\n6\n7\n6\n7\n7\n7\n5\n7\n1\n1\n3\n2\n6\n2\n3\n6\n6\n5\n2\n5\n5\n5\n4\n5\n1\nNA\nNA\nNA\n2\n4.833333\n6.166667\n6.000000\n4.000000\n6.272727\n5.416667\n7\n7\n7\n7\n7\n7\n1\n7\n7\n7\n7\n7\n5\n7\n1\n\n\n\n\n\n\n\n\nLooking at the QRP data at time point 1, you determine that\n\nindividual item columns are \nnumeric\ncharacter, and\naccording to the codebook, there are \nno\nsome reverse-coded items in this questionnaire.\n\nAccording to the codebook and the data table above, we just have to compute the average score for QRP items  to , since items  to  are distractor items. Seems quite straightforward.\nHowever, as you can see in the table above, each item is in a separate column, meaning the data is in wide format. It would be much easier to calculate the mean scores if the items were arranged in long format.\nLet’s tackle this problem step by step. It’s best to create a separate data object for this. If we tried to compute it within data_prp, it could quickly become messy.\n\n\nStep 1: Select the relevant columns Code, and QRPs_1_Time1 to QRPs_11_Time1 and store them in an object called qrp_t1\n\n\nStep 2: Pivot the data from wide format to long format using pivot_longer() so we can calculate the average score more easily (in step 3)\n\nStep 3: Calculate the average QRP score (QRPs_Acceptance_Time1_mean) per participant using group_by() and summarise()\n\n\n\nqrp_t1 &lt;- data_prp %&gt;% \n  #Step 1\n  select(Code, QRPs_1_Time1:QRPs_11_Time1) %&gt;%\n  # Step 2\n  pivot_longer(cols = -Code, names_to = \"Items\", values_to = \"Scores\") %&gt;% \n  # Step 3\n  group_by(Code) %&gt;% # grouping by participant id\n  summarise(QRPs_Acceptance_Time1_mean = mean(Scores)) %&gt;% # calculating the average Score\n  ungroup() # just make it a habit\n\n\n\n\n\n\n\nExplain the individual functions\n\n\n\n\n\n\n\nselect ()\npivot_longer()\ngroup_by() and summarise()\n\n\n\nThe select function allows to include or exclude certain variables (columns). Here we want to focus on the participant ID column (i.e., Code) and the QRP items at time point 1. We can either list them all individually, i.e., Code, QRPs_1_Time1, QRPs_2_Time1, QRPs_3_Time1, and so forth (you get the gist), but that would take forever to type.\nA shortcut is to use the colon operator :. It allows us to select all columns that fall within the range of first_column_name to last_column_name. We can apply this here since the QRP items (1 to 11) are sequentially listed in data_prp.\n\nqrp_step1 &lt;- data_prp %&gt;% \n  select(Code, QRPs_1_Time1:QRPs_11_Time1)\n\n# show first 5 rows of qrp_step1\nhead(qrp_step1, n = 5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nQRPs_1_Time1\nQRPs_2_Time1\nQRPs_3_Time1\nQRPs_4_Time1\nQRPs_5_Time1\nQRPs_6_Time1\nQRPs_7_Time1\nQRPs_8_Time1\nQRPs_9_Time1\nQRPs_10_Time1\nQRPs_11_Time1\n\n\n\nTr10\n7\n7\n5\n7\n3\n4\n5\n7\n6\n7\n7\n\n\nBi07\n7\n7\n2\n7\n3\n7\n7\n7\n7\n6\n7\n\n\nSK03\n7\n7\n6\n6\n7\n6\n7\n7\n7\n5\n7\n\n\nSM95\n7\n7\n2\n6\n7\n5\n7\n7\n4\n2\n4\n\n\nSt01\n7\n7\n6\n7\n2\n7\n7\n7\n7\n5\n7\n\n\n\n\n\n\nHow many rows/observations and columns/variables do we have in qrp_step1?\n\nrows/observations: \n\ncolumns/variables: \n\n\n\n\nAs you can see, the table we got from Step 1 is in wide format. To get it into wide format, we need to define:\n\nthe columns that need to be reshuffled from wide into long format (col argument). Here we selected “everything except the Code column”, as indicated by -Code [minus Code]. However, QRPs_1_Time1:QRPs_11_Time1 would also work and give you the exact same result.\nthe names_to argument. R is creating a new column in which all the column names from the columns you selected in col will be stored in. Here we are naming this column “Items” but you could pick something equally sensible if you like.\nthe values_to argument. R creates this second column to store all responses the participants gave to the individual questions, i.e., all the numbers in this case. We named it “Scores” here, but you could have called it something different, like “Responses”\n\n\nqrp_step2 &lt;- qrp_step1 %&gt;% \n  pivot_longer(cols = -Code, names_to = \"Items\", values_to = \"Scores\")\n\n# show first 15 rows of qrp_step2\nhead(qrp_step2, n = 15)\n\n\n\n\nCode\nItems\nScores\n\n\n\nTr10\nQRPs_1_Time1\n7\n\n\nTr10\nQRPs_2_Time1\n7\n\n\nTr10\nQRPs_3_Time1\n5\n\n\nTr10\nQRPs_4_Time1\n7\n\n\nTr10\nQRPs_5_Time1\n3\n\n\nTr10\nQRPs_6_Time1\n4\n\n\nTr10\nQRPs_7_Time1\n5\n\n\nTr10\nQRPs_8_Time1\n7\n\n\nTr10\nQRPs_9_Time1\n6\n\n\nTr10\nQRPs_10_Time1\n7\n\n\nTr10\nQRPs_11_Time1\n7\n\n\nBi07\nQRPs_1_Time1\n7\n\n\nBi07\nQRPs_2_Time1\n7\n\n\nBi07\nQRPs_3_Time1\n2\n\n\nBi07\nQRPs_4_Time1\n7\n\n\n\n\n\n\nNow, have a look at qrp_step2. In total, we now have  rows/observations,  per participant, and  columns/variables.\n\n\nThis follows exactly the same sequence we used when calculating descriptive statistics by gender. The only difference is that we are now grouping the data by the participant’s Code instead of Gender.\nsummarise() works exactly the same way: summarise(new_column_name = function_to_calculate_something(column_name_of_numeric_values))\nThe function_to_calculate_something can be mean(), sd() or sum() for mean scores, standard deviations, or summed-up scores respectively. You could also use min() or max() if you wanted to determine the lowest or the highest score for each participant.\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou could rename the columns whilst selecting them. The pattern would be select(new_name = old_name). For example, if we wanted to select variable Code and rename it as Participant_ID, we could do that.\n\nrenaming_col &lt;- data_prp %&gt;% \n  select(Participant_ID = Code)\n\nhead(renaming_col, n = 5)\n\n\n\n\nParticipant_ID\n\n\n\nTr10\n\n\nBi07\n\n\nSK03\n\n\nSM95\n\n\nSt01"
  },
  {
    "objectID": "02-wrangling.html#activity-5-knitting",
    "href": "02-wrangling.html#activity-5-knitting",
    "title": "2  Data wrangling I",
    "section": "\n2.5 Activity 5: Knitting",
    "text": "2.5 Activity 5: Knitting\nOnce you’ve completed your R Markdown file, the final step is to “knit” it, which converts the .Rmd file into a HTML file. Knitting combines your code, text, and output (like tables and plots) into a single cohesive document. This is a really good way to check your code is working.\nTo knit the file, click the Knit button at the top of your RStudio window. The document will be generated and, depending on your setting, automatically opened in the viewer in the Output pane or an external browser window.\nIf any errors occur during knitting, RStudio will show you an error message with details to help you troubleshoot.\nIf you want to intentionally keep any errors we tackled today to keep a reference on how you solved them, you could add error=TRUE or eval=FALSE to the code chunk that isn’t running."
  },
  {
    "objectID": "02-wrangling.html#activity-6-export-a-data-object-as-a-csv",
    "href": "02-wrangling.html#activity-6-export-a-data-object-as-a-csv",
    "title": "2  Data wrangling I",
    "section": "\n2.6 Activity 6: Export a data object as a csv",
    "text": "2.6 Activity 6: Export a data object as a csv\nTo avoid having to repeat the same steps in the next chapter, it’s a good idea to save the data objects you’ve created today as csv files. You can do this by using the write_csv() function from the readr package. The csv files will appear in your project folder.\nThe basic syntax is:\n\nwrite_csv(data_object, \"filename.csv\")\n\nNow, let’s export the objects data_prp and qrp_t1.\n\nwrite_csv(data_prp, \"data_prp_for_ch3.csv\")\n\nHere we named the file data_prp_for_ch3.csv, so we wouldn’t override the original data csv file prp_data_reduced.csv. However, feel free to choose a name that makes sense to you.\n\n\n\n\n\n\nYour Turn\n\n\n\nExport qrp_t1.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nwrite_csv(qrp_t1, \"qrp_t1.csv\")\n\n\n\n\n\n\nCheck that your csv files have appeared in your project folder, and you’re all set!\nThat’s it for Chapter 2: Individual Walkthrough."
  },
  {
    "objectID": "02-wrangling.html#pair-coding",
    "href": "02-wrangling.html#pair-coding",
    "title": "2  Data wrangling I",
    "section": "Pair-coding",
    "text": "Pair-coding\nWe will continue working with the data from Binfet et al. (2021), focusing on the randomised controlled trial of therapy dog interventions. Today, our goal is to calculate an average Flourishing score for each participant at time point 1 (pre-intervention) using the raw data file dog_data_raw. Currently, the data looks like this:\n\n\n\n\n\nRID\nF1_1\nF1_2\nF1_3\nF1_4\nF1_5\nF1_6\nF1_7\nF1_8\n\n\n\n1\n6\n7\n5\n5\n7\n7\n6\n6\n\n\n2\n5\n7\n6\n5\n5\n5\n5\n4\n\n\n3\n5\n5\n5\n6\n6\n6\n5\n5\n\n\n4\n7\n6\n7\n7\n7\n6\n7\n4\n\n\n5\n5\n5\n4\n6\n7\n7\n7\n6\n\n\n\n\n\n\nHowever, we want the data to look like this:\n\n\n\n\n\nRID\nFlourishing_pre\n\n\n\n1\n6.125\n\n\n2\n5.250\n\n\n3\n5.375\n\n\n4\n6.375\n\n\n5\n5.875\n\n\n\n\n\n\nTask 1: Open the R project you created last week\nIf you haven’t created an R project for the lab yet, please do so now. If you already have one set up, go ahead and open it.\nTask 2: Open your .Rmd file from last week\nSince we haven’t used it much yet, feel free to continue using the .Rmd file you created last week in Task 2.\nTask 3: Load in the library and read in the data\nThe data should be in your project folder. If you didn’t download it last week, or if you’d like a fresh copy, you can download the data again here: data_pair_ch1.\nWe will be using the tidyverse package today, and the data file we need to read in is dog_data_raw.csv.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n# loading tidyverse into the library\nlibrary(???)\n\n# reading in `dog_data_raw.csv`\ndog_data_raw &lt;- read_csv(\"???\")\n\n\n\n\nTask 4: Calculating the mean for Flourishing_pre\n\n\n\nStep 1: Select all relevant columns from dog_data_raw, including participant ID and all items from the Flourishing questionnaire completed before the intervention. Store this data in an object called data_flourishing.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nLook at the codebook. Try to determine:\n\nThe variable name of the column where the participant ID is stored.\nThe items related to the Flourishing scale at the pre-intervention stage.\n\n\n\n\n\n\n\nMore concrete hint\n\n\n\n\n\nFrom the codebook, we know that:\n\nThe participant ID column is called RID.\nThe Flourishing items at the pre-intervention stage start with F1_.\n\n\ndata_flourishing &lt;- ??? %&gt;% \n  select(???, F1_???:F1_???)\n\n\n\n\n\n\n\n\n\nStep 2: Pivot the data from wide format to long format so we can calculate the average score more easily (in step 3).\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nWhich pivot function should you use? We have pivot_wider() and pivot_longer() to choose from.\nWe also need 3 arguments in that function:\n\nThe columns you want to select (e.g., all the Flourishing items),\nThe name of the column where the current column headings will be stored (e.g., “Questionnaire”),\nThe name of the column that should store all the values (e.g., “Responses”).\n\n\n\n\n\n\n\nMore concrete hint\n\n\n\n\n\nWe need pivot_longer(). You already encountered pivot_longer() in first year (or in the individual walkthrough if you have already completed this Chapter). The 3 arguments was also a give-away; pivot_wider() only requires 2 arguments.\n\n  pivot_longer(cols = ???, names_to = \"???\", values_to = \"???\")\n\n\n\n\n\n\n\n\n\nStep 3: Calculate the average Flourishing score per participant and name this column Flourishing_pre to match the table above.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nBefore summarising the mean, you may need to group the data.\n\n\n\n\n\n\nMore concrete hint\n\n\n\n\n\nTo compute an average score per participant, we would need to group by participant ID first.\n\n  group_by(???) %&gt;% \n  summarise(Flourishing_pre = mean(???)) %&gt;% \n  ungroup()\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# loading tidyverse into the library\nlibrary(tidyverse)\n\n# reading in `dog_data_raw.csv`\ndog_data_raw &lt;- read_csv(\"dog_data_raw.csv\")\n\n# Task 4: Tidying \ndata_flourishing &lt;- dog_data_raw %&gt;% \n  # Step 1\n  select(RID, F1_1:F1_8) %&gt;% \n  # Step 2\n  pivot_longer(cols = -RID, names_to = \"Questionnaire\", values_to = \"Responses\") %&gt;% \n  # Step 3\n  group_by(RID) %&gt;% \n  summarise(Flourishing_pre = mean(Responses)) %&gt;% \n  ungroup()"
  },
  {
    "objectID": "02-wrangling.html#test-your-knowledge-and-challenge-yourself",
    "href": "02-wrangling.html#test-your-knowledge-and-challenge-yourself",
    "title": "2  Data wrangling I",
    "section": "Test your knowledge and challenge yourself",
    "text": "Test your knowledge and challenge yourself\nKnowledge check\nQuestion 1\nWhich function of the Wickham Six would you use to include or exclude certain variables (columns)? \nselect()\nfilter()\nmutate()\narrange()\ngroup_by()\nsummarise()\nQuestion 2\nWhich function of the Wickham Six would you use to create new columns or modify existing columns in a dataframe? \nselect()\nfilter()\nmutate()\narrange()\ngroup_by()\nsummarise()\nQuestion 3\nWhich function of the Wickham Six would you use to organise data into groups based on one or more columns? \nselect()\nfilter()\nmutate()\narrange()\ngroup_by()\nsummarise()\nQuestion 4\nWhich function of the Wickham Six would you use to sort the rows of a dataframe based on the values in one or more columns? \nselect()\nfilter()\nmutate()\narrange()\ngroup_by()\nsummarise()\nQuestion 5\nWhich function of the Wickham Six would NOT modify the original dataframe? \nselect()\nfilter()\nmutate()\narrange()\ngroup_by()\nsummarise()\n\n\n\n\n\n\nExplain these answers\n\n\n\n\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\nselect()\nInclude or exclude certain variables/columns\n\n\nfilter()\nInclude or exclude certain observations/rows\n\n\nmutate()\nCreates new columns or modifies existing ones\n\n\narrange()\nChanges the order of the rows\n\n\ngroup_by()\nSplit data into groups based on one or more variables\n\n\nsummarise()\nCreates a new dataframe returning one row for each combination of grouping variables\n\n\n\nTechnically, the first five functions operate on the existing data object, making adjustments like sorting the data (e.g., with arrange()), reducing the number of rows (e.g., with filter()), reducing the number of columns (e.g., with select()), or adding new columns (e.g., with mutate()). In contrast, summarise() fundamentally alters the structure of the original dataframe by generating a completely new dataframe that contains only summary statistics, rather than retaining the original rows and columns.\n\n\n\nError mode\nSome of the code chunks contain mistakes and result in errors, while others do not produce the expected results. Your task is to identify any issues, explain why they occurred, and, if possible, fix them.\nWe will use a few built-in datasets, such as billboard and starwars, to help you replicate the errors in your own R environment. You can view the data either by typing the dataset name directly into your console or by storing the data as a separate object in your Global Environment.\n\nbillboard\n\nstarwars_data = starwars\n\nQuestion 6\nCurrently, the weekly song rankings for Billboard Top 100 in 2000 are in wide format, with each week in a separate column. The following code is supposed to transpose the wide-format billboard data into long format:\n\nlong_data &lt;- billboard %&gt;% \n  pivot_longer(names_to = \"weeks\", values_to = \"rank\")\n\nError in `pivot_longer()`:\n! `cols` must select at least one column.\n\n\nWhat does this error message mean and how do you fix it?\n\n\n\n\n\n\nExplain the solution\n\n\n\n\n\nThe error message indicates that the cols argument is missing in the function. This means the function doesn’t know which columns to transpose from wide format to long format.\nFIX: Add cols = wk1:wk76 to the function to select columns from wk1 to wk76. Alternatively, cols = starts_with(\"wk\") would also work since all columns start with the letter combination “wk”.\n\nlong_data &lt;- billboard %&gt;% \n  pivot_longer(cols = wk1:wk76, names_to = \"weeks\", values_to = \"rank\")\n# OR\nlong_data &lt;- billboard %&gt;% \n  pivot_longer(cols = starts_with(\"wk\"), names_to = \"weeks\", values_to = \"rank\")\n\n\n\n\nQuestion 7\nThe following code is intended to calculate the mean height of all the characters in the built-in starwars dataset, grouped by their gender.\n\nsummary_data &lt;- starwars %&gt;%\n  group_by(gender) %&gt;%\n  summarise(mean_height = height)\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\n\nThe code runs, but it’s giving us some weird warning and the output is also not as expected. What steps should we take to fix this?\n\n\n\n\n\n\nExplain the solution\n\n\n\n\n\nThe aggregation function mean() is missing from within summarise(). Without it, the function does not perform any aggregation and returns all rows with only the columns for gender and height.\nFIX: Wrap the mean() function around the variable you want to aggregate, here height.\n\nsummary_data &lt;- starwars %&gt;%\n  group_by(gender) %&gt;%\n  summarise(mean_height = mean(height))\n\n\n\n\nQuestion 8\nFollowing up on Question 7, we now have summary_data that looks approximately correct - it has the expected rows and column numbers, however, the cell values are “weird”.\n\nsummary_data\n\n\n\n\ngender\nmean_height\n\n\n\nfeminine\nNA\n\n\nmasculine\nNA\n\n\nNA\n175\n\n\n\n\n\n\nCan you explain what is happening here? And how can we modify the code to fix this?\n\n\n\n\n\n\nExplain the solution\n\n\n\n\n\nLook at the original starwars data. You will notice that some of the characters with feminine and masculine gender entries have missing height values. However, all four characters without a specified gender have provided their height.\nFIX: We need to add na.rm = TRUE to the mean() function to ensure that R ignores missing values before aggregating the data.\n\nsummary_data &lt;- starwars %&gt;%\n  group_by(gender) %&gt;%\n  summarise(mean_height = mean(height, na.rm = TRUE))\n\nsummary_data\n\n\n\n\ngender\nmean_height\n\n\n\nfeminine\n166.5333\n\n\nmasculine\n176.5323\n\n\nNA\n175.0000\n\n\n\n\n\n\n\n\n\nChallenge yourself\nIf you want to challenge yourself and further apply the skills from Chapter 2, you can wrangle the data from dog_data_raw for additional questionnaires from either the pre- and/or post-intervention stages:\n\nCalculate the mean score for flourishing_post for each participant.\nCalculate the mean score for the PANAS (Positive and/or Negative Affect) per participant\nCalculate the mean score for happiness (SHS) per participant\n\nThe 3 steps are equivalent for those questionnaires - select, pivot, group_by and summarise; you just have to “replace” the questionnaire items involved.\n\n\n\n\n\n\nSolution for Challenge yourself\n\n\n\n\n\nFlourishing post-intervention\n\n## flourishing_post\nflourishing_post &lt;- dog_data_raw %&gt;% \n  # Step 1\n  select(RID, starts_with(\"F2\")) %&gt;% \n  # Step 2\n  pivot_longer(cols = -RID, names_to = \"Names\", values_to = \"Response\") %&gt;% \n  # Step 3\n  group_by(RID) %&gt;% \n  summarise(Flourishing_post = mean(Response)) %&gt;% \n  ungroup()\n\nThe PANAS could be solved more concisely with the skills we learn in Chapter 3, but for now, you would have solved it this way:\n\n# PANAS - positive affect pre\nPANAS_PA_pre &lt;- dog_data_raw %&gt;% \n  # Step 1\n  select(RID, PN1_3, PN1_5, PN1_7, PN1_8, PN1_10) %&gt;% \n  # Step 2\n  pivot_longer(cols = -RID, names_to = \"Items\", values_to = \"Scores\") %&gt;% \n  # Step 3\n  group_by(RID) %&gt;% \n  summarise(PANAS_PA_pre = mean(Scores)) %&gt;% \n  ungroup()\n\n# PANAS - positive affect post\nPANAS_PA_post &lt;- dog_data_raw %&gt;% \n  # Step 1\n  select(RID, PN2_3, PN2_5, PN2_7, PN2_8, PN2_10) %&gt;% \n  # Step 2\n  pivot_longer(cols = -RID, names_to = \"Items\", values_to = \"Scores\") %&gt;% \n  # Step 3\n  group_by(RID) %&gt;% \n  summarise(PANAS_PA_post = mean(Scores)) %&gt;% \n  ungroup()\n\n# PANAS - negative affect pre\nPANAS_NA_pre &lt;- dog_data_raw %&gt;% \n  # Step 1\n  select(RID, PN1_1, PN1_2, PN1_4, PN1_6, PN1_9) %&gt;% \n  # Step 2\n  pivot_longer(cols = -RID, names_to = \"Items\", values_to = \"Scores\") %&gt;% \n  # Step 3\n  group_by(RID) %&gt;% \n  summarise(PANAS_NA_pre = mean(Scores)) %&gt;% \n  ungroup()\n\n# PANAS - negative affect post\nPANAS_NA_post &lt;- dog_data_raw %&gt;% \n  # Step 1\n  select(RID, PN2_1, PN2_2, PN2_4, PN2_6, PN2_9) %&gt;% \n  # Step 2\n  pivot_longer(cols = -RID, names_to = \"Items\", values_to = \"Scores\") %&gt;% \n  # Step 3\n  group_by(RID) %&gt;% \n  summarise(PANAS_NA_post = mean(Scores)) %&gt;% \n  ungroup()\n\nHappiness scale\n\n# happiness_pre\nhappiness_pre &lt;- dog_data_raw %&gt;% \n  # Step 1\n  select(RID, HA1_1, HA1_2, HA1_3) %&gt;% \n  # Step 2\n  pivot_longer(cols = -RID, names_to = \"Item\", values_to = \"Score\") %&gt;% \n  # Step 3\n  group_by(RID) %&gt;% \n  summarise(SHS_pre = mean(Score)) %&gt;% \n  ungroup()\n\n#happiness_post\nhappiness_post &lt;- dog_data_raw %&gt;% \n  # Step 1\n  select(RID, HA2_1, HA2_2, HA2_3) %&gt;% \n  # Step 2\n  pivot_longer(cols = -RID, names_to = \"Item\", values_to = \"Score\") %&gt;% \n  # Step 3\n  group_by(RID) %&gt;% \n  summarise(SHS_post = mean(Score)) %&gt;% \n  ungroup()"
  },
  {
    "objectID": "03-wrangling2.html#intended-learning-outcomes",
    "href": "03-wrangling2.html#intended-learning-outcomes",
    "title": "3  Data wrangling II",
    "section": "Intended Learning Outcomes",
    "text": "Intended Learning Outcomes\nBy the end of this chapter, you should be able to:\n\napply familiar data wrangling functions to novel datasets\nread and interpret error messages\nrealise there are several ways of getting to the results\n\nIn this chapter, we will pick up where we left off in Chapter 2. We will calculate average scores for two of the questionnaires, address an error mode problem, and finally, join all data objects together. This will finalise our data for the upcoming data visualization sections (Chapter 4 and ?sec-dataviz2)."
  },
  {
    "objectID": "03-wrangling2.html#individual-walkthrough",
    "href": "03-wrangling2.html#individual-walkthrough",
    "title": "3  Data wrangling II",
    "section": "Individual Walkthrough",
    "text": "Individual Walkthrough"
  },
  {
    "objectID": "03-wrangling2.html#activity-1-setup",
    "href": "03-wrangling2.html#activity-1-setup",
    "title": "3  Data wrangling II",
    "section": "\n3.1 Activity 1: Setup",
    "text": "3.1 Activity 1: Setup\n\nGo to the project folder we have been using in the last two weeks and double-click on the project icon to open the project in RStudio\nEither Create a new .Rmd file for chapter 3 and save it to your project folder or continue the one from last week. See Section 1.3 if you need some guidance."
  },
  {
    "objectID": "03-wrangling2.html#activity-2-load-in-the-libraries-and-read-in-the-data",
    "href": "03-wrangling2.html#activity-2-load-in-the-libraries-and-read-in-the-data",
    "title": "3  Data wrangling II",
    "section": "\n3.2 Activity 2: Load in the libraries and read in the data",
    "text": "3.2 Activity 2: Load in the libraries and read in the data\nToday, we will be using tidyverse along with the two csv files created at the end of the last chapter: data_prp_for_ch3.csv and qrp_t1.csv. If you need to download them again for any reason, click on the following links: data_prp_for_ch3.csv and qrp_t1.csv.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nlibrary(???)\ndata_prp &lt;- read_csv(\"???\")\nqrp_t1 &lt;- read_csv(\"???\")\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(tidyverse)\ndata_prp &lt;- read_csv(\"prp_data_reduced.csv\")\nqrp_t1 &lt;- read_csv(\"qrp_t1.csv\")\n\n\n\n\nIf you need a quick reminder what the dataset was about, have a look at the abstract in Section 1.4. We also addressed the changes we made to the dataset there.\nAnd remember to have a quick glimpse() at your data."
  },
  {
    "objectID": "03-wrangling2.html#activity-3-confidence-in-understanding-open-science-practices",
    "href": "03-wrangling2.html#activity-3-confidence-in-understanding-open-science-practices",
    "title": "3  Data wrangling II",
    "section": "\n3.3 Activity 3: Confidence in understanding Open Science practices",
    "text": "3.3 Activity 3: Confidence in understanding Open Science practices\nThe main goal is to compute the mean Understanding score per participant.\nThe mean Understanding score for time point 2 has already been calculated (in the Time2_Understanding_OS column), but we still need to compute it for time point 1.\nLooking at the Understanding data at time point 1, you determine that\n\nindividual item columns are \nnumeric\ncharacter, and\naccording to the codebook, there are \nno\nsome reverse-coded items in this questionnaire.\n\nThe steps are quite similar to those for QRP, but we need to add an extra step: converting the character labels into numbers.\nAgain, let’s do this step by step:\n\n\nStep 1: Select the relevant columns Code, and every Understanding column from time point 1 (e.g., from Understanding_OS_1_Time1 to Understanding_OS_12_Time1) and store them in an object called understanding_t1\n\n\nStep 2: Pivot the data from wide format to long format using pivot_longer() so we can recode the labels into values (step 3) and calculate the average score (in step 4) more easily\n\nStep 3: Recode the values “Not at all confident” as 1 and “Entirely confident” as 7. All other values are already numbers. We can use functions mutate() in combination with case_match() for that\n\nStep 4: Calculate the average QRP score (QRPs_Acceptance_Time1_mean) per participant using group_by() and summarise()\n\nSteps 1 and 2: Select and pivot\nHow about you try the first 2 steps yourself using the code from Chapter 2 Activity 4 (Section 2.4) as a template?\n\nunderstanding_t1 &lt;- data_prp %&gt;% \n  select(???) %&gt;% # Step 1\n  pivot_longer(cols = ???, names_to = \"???\", values_to = \"???\") # Step 2\n\n\n\n\n\n\n\nSolution for steps 1 and 2\n\n\n\n\n\n\nunderstanding_t1 &lt;- data_prp %&gt;% \n  # Step 1\n  select(Code, Understanding_OS_1_Time1:Understanding_OS_12_Time1) %&gt;% \n  # Step 2 - I picked different column labels this time for some variety\n  pivot_longer(cols = Understanding_OS_1_Time1:Understanding_OS_12_Time1, names_to = \"Understanding_Qs\", values_to = \"Responses\") \n\n\n\n\nStep 3: recoding the values\nOK, we now want to recode the values in the Responses column (or whatever name you picked for your column that has some of the numbers in it) so that “Not at all confident” = 1 and “Entirely confident” = 7. We want to keep all other values as they are (2-6 look already quite “numeric”).\nLet’s create a new column Responses_corrected that stores the new values with mutate(). Then we can combine that with the case_match() function.\n\nThe first argument in case_match() is the column name of the variable you want to recode.\nThen you can start recoding the values in the way of CurrentValue ~ NewValue (~ is a tilde). Make sure you use the ~ and not =!\nLastly, the .default argument tells R what to do with values that are neither “Not at all confident” nor “Entirely confident”. Here, we want to replace them with the original value of the Responses column. In other datasets, you may want to set the default to NA for missing values, a character string or a number, and case_match() is happy to oblige.\n\n\nunderstanding_t1 &lt;- understanding_t1 %&gt;% \n  mutate(Responses_corrected = case_match(Responses, # column of the values to recode\n                                          \"Not at all confident\" ~ 1, # values to recode\n                                          \"Entirely confident\" ~ 7,\n                                          .default = Responses # all other values taken from column Responses\n  ))\n\nError in `mutate()`:\nℹ In argument: `Responses_corrected = case_match(...)`.\nCaused by error in `case_match()`:\n! Can't combine `..1 (right)` &lt;double&gt; and `.default` &lt;character&gt;.\n\n\n\n\n\n\n\n\nError!!! Can you explain what is happening here?\n\n\n\n\n\nHave a look at the error message. It’s pretty helpful this time. It says Can't combine ..1 (right) &lt;double&gt; and .default &lt;character&gt;. It means that the replacement values are expected to be data type character since the original column type was type character.\n\n\n\nSo how do we fix this? Actually, there are several ways this could be done. Click on the tabs below to check out 3 possible solutions.\n\n\nFix option 1\nFix option 2\nFix option 3\n\n\n\nOne option is to modify the .default argument Responses so that the values are copied over from the original column but as a number rather than the original character value. The function as.numeric() does the conversion.\n\nunderstanding_t1_step3_v1 &lt;- understanding_t1 %&gt;% \n  mutate(Responses_corrected = case_match(Responses, # column of the values to recode\n                                          \"Not at all confident\" ~ 1, # values to recode\n                                          \"Entirely confident\" ~ 7,\n                                          .default = as.numeric(Responses) # all other values taken from column Responses but as numeric data type \n  ))\n\n\n\nChange the numeric values on the right side of the ~ to character. Then in a second step, we would need to turn the character column into a numeric type. Again, we have several options to do so. We could either use the parse_number() function we encountered earlier during the demographics wrangling or the as.numeric() function.\n\nV1: Responses_corrected = parse_number(Responses_corrected)\n\nV2: Responses_corrected = as.numeric(Responses_corrected)\n\n\nJust pay attention that you are still working within the mutate() function.\n\nunderstanding_t1_step3_v2 &lt;- understanding_t1 %&gt;% \n  mutate(Responses_corrected = case_match(Responses, # column of the values to recode\n                                          \"Not at all confident\" ~ \"1\",\n                                          \"Entirely confident\" ~ \"7\",\n                                          .default = Responses # all other values taken from column Responses (character)\n  ),\n  Responses_corrected = parse_number(Responses_corrected)) # turning Responses_corrected into a numeric column\n\n\n\nIf you recode all the labels into numbers (e.g., “2” into 2, “3” into 3, etc.) from the start, you won’t need to perform any additional conversions later.\n\nunderstanding_t1_step3_v2 &lt;- understanding_t1 %&gt;% \n  mutate(Responses_recoded = case_match(Responses, # column of the values to recode\n                                        \"Not at all confident\" ~ 1, # recode all of them\n                                        \"2\" ~ 2,\n                                        \"3\" ~ 3,\n                                        \"4\" ~ 4,\n                                        \"5\" ~ 5,\n                                        \"6\" ~ 6,\n                                        \"Entirely confident\" ~ 7))\n\n\n\n\n\n\n\n\n\n\nYour Turn\n\n\n\nChoose the option that works best for you to modify the code above that didn’t work. You should now be able to calculate the mean Understanding Score per participant. Store the average scores in a variable called Time1_Understanding_OS. If you need help, refer to the hint below or use Chapter 2 Activity 4 (Section 2.4) as guidance.\n\n\n\n\n\n\nOne solution for Steps 3 and 4\n\n\n\n\n\n\nunderstanding_t1 &lt;- understanding_t1 %&gt;% \n  mutate(Responses_corrected = case_match(Responses, # column of the values to recode\n                                          \"Not at all confident\" ~ 1, # values to recode\n                                          \"Entirely confident\" ~ 7,\n                                          .default = as.numeric(Responses) # all other values taken from column Responses but as numeric data type \n  )) %&gt;% \n  # Step 4: calculating averages per participant\n  group_by(Code) %&gt;%\n  summarise(Time1_Understanding_OS = mean(Responses_corrected)) %&gt;%\n  ungroup()\n\n\n\n\n\n\nOf course, this could have been written up as a single pipe.\n\n\n\n\n\n\nSingle pipe of activity 3\n\n\n\n\n\n\nunderstanding_t1 &lt;- data_prp %&gt;% \n  # Step 1\n  select(Code, Understanding_OS_1_Time1:Understanding_OS_12_Time1) %&gt;% \n  # Step 2\n  pivot_longer(cols = -Code, names_to = \"Understanding_Qs\", values_to = \"Responses\") %&gt;% \n  # Step 3\n  mutate(Responses_corrected = case_match(Responses, # column of the values to recode\n                                          \"Not at all confident\" ~ 1, # values to recode\n                                          \"Entirely confident\" ~ 7,\n                                          .default = as.numeric(Responses) # all other values taken from column Responses but as numeric data type \n  )) %&gt;% \n  # Step 4\n  group_by(Code) %&gt;%\n  summarise(Time1_Understanding_OS = mean(Responses_corrected)) %&gt;%\n  ungroup()"
  },
  {
    "objectID": "03-wrangling2.html#activity-4-survey-of-attitudes-toward-statistics-sats-28",
    "href": "03-wrangling2.html#activity-4-survey-of-attitudes-toward-statistics-sats-28",
    "title": "3  Data wrangling II",
    "section": "\n3.4 Activity 4: Survey of Attitudes Toward Statistics (SATS-28)",
    "text": "3.4 Activity 4: Survey of Attitudes Toward Statistics (SATS-28)\nThe main goal is to compute the mean SATS-28 score for each of the 4 subscales per participant for time point 1.\nLooking at the SATS data at time point 1, you determine that\n\nindividual item columns are \nnumeric\ncharacter, and\naccording to the codebook, there are \nno\nsome reverse-coded items in this questionnaire.\nAdditionally, we are looking to compute the means for the 4 different subscales of the SAT-28 which are , , , and .\n\nThis scenario is slightly more tricky than the previous ones due to the reverse-coding and the 4 subscales. So, let’s tackle this step by step again:\n\n\nStep 1: Select the relevant columns Code, and every SATS28 column from time point 1 (e.g., from SATS28_1_Affect_Time1 to SATS28_28_Difficulty_Time1) and store them in an object called sats_t1\n\n\nStep 2: Pivot the data from wide format to long format using pivot_longer() so we can recode the labels into values (step 3) and calculate the average score (in step 4) more easily\n\nStep 3: We need to know which items belong to which subscale - fortunately, we have that information in the variable name and can use the separate() function to access it.\n\nStep 4: We need to know which items are reverse-coded and then reverse-score them - unfortunately, the info is only in the codebook and we need to find a work-around. case_when() can help identify and re-score the reverse-coded items.\n\nStep 5: Calculate the average SATS score per participant and subscale using group_by() and summarise()\n\n\nStep 6: use pivot_wider() to spread out the dataframe into wide format and rename() to tidy up the column names\nSteps 1 and 2: select and pivot\nThe selecting and pivoting are exactly the same way as we already practiced in the other 2 questionnaires. Apply them here to this questionnaire.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nsats_t1 &lt;- data_prp %&gt;% \n  select(???) %&gt;% # Step 1\n  pivot_longer(cols = ???, names_to = \"???\", values_to = \"???\") # Step 2\n\n\n\n\n\n\n\nSolution for steps 1 and 2\n\n\n\n\n\n\nsats_t1 &lt;- data_prp %&gt;% \n  select(Code, SATS28_1_Affect_Time1:SATS28_28_Difficulty_Time1) %&gt;% # Step 1\n  pivot_longer(cols = -Code, names_to = \"Items\", values_to = \"Response\") # Step 2\n\n\n\n\n\n\n\nStep 3: separate Subscale information\nIf you look at the Items column more closely, you can see that there is information on the Questionnaire, the Item_number, the Subscale, and the Timepoint the data was collected at.\nWe can separate the information into separate columns using the separate() function. The function’s first argument is the column to separate, then define into which columns you want the original column to split up, and lastly, define the separator sep (here an underscore). For our example, we would write:\n\nV1: separate(Items, into = c(\"SATS\", \"Item_number\", \"Subscale\", \"Time\"), sep = \"_\")\n\n\nHowever, we don’t need all of those columns, so we could just drop the ones we are not interested in by replacing them with NA.\n\nV2: separate(Items, into = c(NA, \"Item_number\", \"Subscale\", NA), sep = \"_\")\n\n\nWe might also add an extra argument of convert = TRUE to have numeric columns (i.e., Item_number) converted to numeric as opposed to keeping them as characters. Saves us typing a few quotation marks later in Step 4.\n\nsats_t1 &lt;- sats_t1 %&gt;% \n  # Step 3\n  separate(Items, into = c(NA, \"Item_number\", \"Subscale\", NA), sep = \"_\", convert = TRUE)\n\nStep 4: identifying reverse-coded items and then correct them\nWe can use case_when() within the mutate() function here to create a new column FW_RV that stores information on whether the item is a reverse-coded item or not.\ncase_when() works similarly to case_match(), however case_match() only allows us to “recode” values (i.e., replace one value with another), whereas case_when() is more flexible. It allows us to use conditional statements on the left side of the tilde which is useful when you want to change only some of the data based on specific conditions.\nLooking at the codebook, it seems that items 2, 3, 4, 6, 7, 8, 9, 12, 13, 16, 17, 19, 20, 21, 23, 25, 26, 27, and 28 are reverse-coded. The rest are forward-coded.\nWe want to tell R now, that\n\n\nif the Item_number is any of those numbers listed above, R should write “Reverse” into the new column FW_RV we are creating. Since we have a few possible matches for Item_number, we need the Boolean expression %in% rather than ==.\n\nif Item_number is none of those numbers, then we would like the word “Forward” in the FW_RV column to appear. We can achieve that by specifying a .default argument again, but this time we want a “word” rather than a value from another column.\n\n\nsats_t1 &lt;- sats_t1 %&gt;% \n  mutate(FW_RV = case_when(\n    Item_number %in% c(2, 3, 4, 6, 7, 8, 9, 12, 13, 16, 17, 19, 20, 21, 23, 25, 26, 27, 28) ~ \"Reverse\",\n    .default = \"Forward\"\n  ))\n\nMoving on to correcting the scores: Once again, we can use case_when () within the mutate() function to create another conditional statement. This time, the condition is:\n\n\nif FW_RV column has a value of “Reverse” then we would like to turn all 1 into 7, 2 into 6, etc.\n\nif FW_RV column has a value of “Forward” then we would like to keep the score from the Response column\n\nThere is a quick way and a not-so-quick way to achieve the actual reverse-coding.\n\n\nOption 1 (quick): The easiest way to reverse-code scores is to take the maximum value of the scale, add 1 unit, and subtract the original value. For example, on a 5-point Likert scale, it would be 6 minus the original rating; for a 7-point Likert scale, 8 minus the original rating, etc. (see Option 1 tab).\n\nOption 2 (not so quick): This involves using two conditional statements (see Option 2 tab).\n\nUse the one you find more intuitive.\n\n\nOption 1\nOption 2\n\n\n\nHere we are using a Boolean expression to check if the string “Reverse” is present in the FW_RV column. If this condition is TRUE, the value in the new column we’re creating, Scores_corrected, will be calculated as 8 minus the value from the Response column. If the condition is FALSE (handled by the .default argument), the original values from the Response column will be retained.\n\nsats_t1 &lt;- sats_t1 %&gt;% \n  mutate(Scores_corrected = case_when(\n    FW_RV == \"Reverse\" ~ 8-Response,\n    .default = Response\n  ))\n\n\n\nAs stated above, the longer approach involves using two conditional statements. The first condition checks if the value in the FW_RV column is “Reverse”, while the second condition checks if the value in the Response column equals a specific number. When both conditions are met, the corresponding value on the right side of the tilde is placed in the newly created Scores_corrected_v2 column.\nFor example, line 3 would read: if the value in the FW_RV column is “Reverse” AND the value in the Response column is 1, then assign a value of 7 to the Scores_corrected_v2 column.\n\nsats_t1 &lt;- sats_t1 %&gt;% \n  mutate(Scores_corrected_v2 = case_when(\n    FW_RV == \"Reverse\" & Response == 1 ~ 7,\n    FW_RV == \"Reverse\" & Response == 2 ~ 6,\n    FW_RV == \"Reverse\" & Response == 3 ~ 5,\n    # no need to recode 4 as 4\n    FW_RV == \"Reverse\" & Response == 5 ~ 3,\n    FW_RV == \"Reverse\" & Response == 6 ~ 2,\n    FW_RV == \"Reverse\" & Response == 7 ~ 1,\n    .default = Response\n  ))\n\nAs you can see now in sats_t1, both columns Scores_corrected and Scores_corrected_v2 are identical.\n\n\n\nOne way to check whether our reverse-coding worked is by examining the distinct values in the original Response column and comparing them with the Scores_corrected. We should also retain the FW_RV column to observe how the reverse-coding applied.\nTo see the patterns more clearly, we can use arrange() to sort the values in a meaningful order. Remember, the default sorting order is ascending, so if you want to sort values in descending order, you’ll need to wrap your variable in the desc() function.\n\ncheck_coding &lt;- sats_t1 %&gt;% \n  distinct(FW_RV, Response, Scores_corrected) %&gt;% \n  arrange(desc(FW_RV), Response)\n\n\n\n\n\n\n\nShow check_coding output\n\n\n\n\n\n\ncheck_coding\n\n\n\n\nFW_RV\nResponse\nScores_corrected\n\n\n\nReverse\n1\n7\n\n\nReverse\n2\n6\n\n\nReverse\n3\n5\n\n\nReverse\n4\n4\n\n\nReverse\n5\n3\n\n\nReverse\n6\n2\n\n\nReverse\n7\n1\n\n\nForward\n1\n1\n\n\nForward\n2\n2\n\n\nForward\n3\n3\n\n\nForward\n4\n4\n\n\nForward\n5\n5\n\n\nForward\n6\n6\n\n\nForward\n7\n7\n\n\n\n\n\n\n\n\n\nStep 5\nNow that we know everything worked out as intended, we can calculate the mean scores of each subscale for each participant in sats_t1.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nsats_t1 &lt;- sats_t1 %&gt;% \n  group_by(???, ???) %&gt;% \n  summarise(mean_score = ???(???)) %&gt;% \n  ungroup()\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsats_t1 &lt;- sats_t1 %&gt;% \n  group_by(Code, Subscale) %&gt;% \n  summarise(mean_score = mean(Scores_corrected)) %&gt;% \n  ungroup()\n\n`summarise()` has grouped output by 'Code'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\nStep 6\nThe final step is to transform the data back into wide format, ensuring that each subscale has its own column. This will make it easier to join the data objects later on. In pivot_wider(), the first argument, names_from, specifies the column you want to use for your new column headings. The second argument, values_from, tells R which column should provide the cell values.\nWe should also rename the column names to match those in the codebook. Conveniently, we can use a function called rename() that works exactly like select() (following the pattern new_name = old_name), but it keeps all other column names the same rather than reducing the number of columns.\n\nsats_t1 &lt;- sats_t1 %&gt;% \n  pivot_wider(names_from = Subscale, values_from = mean_score) %&gt;% \n  rename(SATS28_Affect_Time1_mean = Affect,\n         SATS28_CognitiveCompetence_Time1_mean = CognitiveCompetence,\n         SATS28_Value_Time1_mean = Value,\n         SATS28_Difficulty_Time1_mean = Difficulty)\n\n\n\n\n\n\n\nShow final sats_t1 output\n\n\n\n\n\n\nhead(sats_t1, n = 5)\n\n\n\n\n\n\n\n\n\n\n\nCode\nSATS28_Affect_Time1_mean\nSATS28_CognitiveCompetence_Time1_mean\nSATS28_Difficulty_Time1_mean\nSATS28_Value_Time1_mean\n\n\n\nAD03\n2.333333\n3.833333\n3.428571\n5.555556\n\n\nAD05\n3.500000\n5.000000\n2.142857\n4.777778\n\n\nAb01\n5.166667\n5.666667\n4.142857\n5.444444\n\n\nAl05\n2.166667\n2.666667\n2.857143\n3.777778\n\n\nAm05\n4.166667\n5.666667\n5.571429\n4.888889\n\n\n\n\n\n\n\n\n\nAgain, this could have been written up as a single pipe.\n\n\n\n\n\n\nSingle pipe of activity 4\n\n\n\n\n\n\nsats_t1 &lt;- data_prp %&gt;% \n  # Step 1\n  select(Code, SATS28_1_Affect_Time1:SATS28_28_Difficulty_Time1) %&gt;% \n  # Step 2\n  pivot_longer(cols = -Code, names_to = \"Items\", values_to = \"Response\") %&gt;% \n  # Step 3\n  separate(Items, into = c(NA, \"Item_number\", \"Subscale\", NA), sep = \"_\", convert = TRUE) %&gt;% \n  # step 4\n  mutate(FW_RV = case_when(\n    Item_number %in% c(2, 3, 4, 6, 7, 8, 9, 12, 13, 16, 17, 19, 20, 21, 23, 25, 26, 27, 28) ~ \"Reverse\",\n    .default = \"Forward\"\n  ),\n    Scores_corrected = case_when(\n      FW_RV == \"Reverse\" ~ 8-Response,\n      .default = Response\n  )) %&gt;% \n  # step 5\n  group_by(Code, Subscale) %&gt;% \n  summarise(mean_score = mean(Scores_corrected)) %&gt;% \n  ungroup() %&gt;% \n  # step 6\n  pivot_wider(names_from = Subscale, values_from = mean_score) %&gt;% \n  rename(SATS28_Affect_Time1_mean = Affect,\n         SATS28_CognitiveCompetence_Time1_mean = CognitiveCompetence,\n         SATS28_Value_Time1_mean = Value,\n         SATS28_Difficulty_Time1_mean = Difficulty)"
  },
  {
    "objectID": "03-wrangling2.html#activity-5-error-mode-perceptions-of-supervisory-support",
    "href": "03-wrangling2.html#activity-5-error-mode-perceptions-of-supervisory-support",
    "title": "3  Data wrangling II",
    "section": "\n3.5 Activity 5 (Error Mode): Perceptions of supervisory support",
    "text": "3.5 Activity 5 (Error Mode): Perceptions of supervisory support\nThe main goal is to compute the mean score for perceived supervisory support per participant.\nLooking at the supervisory support data, you determine that\n\nindividual item columns are \nnumeric\ncharacter, and\naccording to the codebook, there are \nno\nsome reverse-coded items in this questionnaire.\n\nI have outlined my steps as follows:\n\n\nStep 1: Reverse-code the single column first because that’s less hassle than having to do that with conditional statements (Supervisor_15_R). mutate() is my friend.\n\nStep 2: I want to filter out everyone who failed the attention check in Supervisor_7. I can do this with a Boolean expression within the filter() function. The correct response was “completely disagree” which is 1.\n\nStep 3: Select their id from time point 2 and all the columns that start with the word “super”, apart from Supervisor_7 and the original Supervisor_15_R column\n\nStep 4: pivot into long format so I can calculate the averages better\n\nStep 5: calculate the average scores per participant\n\nI’ve started coding but there are some errors in my code. Help me find and fix all of them. Try to go through the code line by line and read the error messages.\n\nsuper &lt;- data_ppr %&gt;% \n  mutate(Supervisor_15 = 9-supervisor_15_R) %&gt;% \n  filter(Supervisor_7 = 1) %&gt;% \n  select(Code, starts_with(\"Super\"), -Supervisor_7, -Supervisor_15_R) \npivot_wider(cols = -Code, names_to = \"Item\", values_to = \"Response\") %&gt;% \n  group_by(Time2_Code) %&gt;% \n  summarise(Mean_Supervisor_Support = mean(Score_corrected, na.rm = TRUE)) %&gt;% \n  ungroup()\n\n\n\n\n\n\n\nHow many mistakes am I supposed to find?\n\n\n\n\n\nThere are 8 mistakes in the code.\n\n\n\n\n\n\n\n\n\nReveal solution\n\n\n\n\n\nDid you spot all 8 mistakes? Let’s go through them line by line.\n\nsuper &lt;- data_prp %&gt;% # spelling mistake in data object\n  mutate(Supervisor_15 = 8-Supervisor_15_R) %&gt;% # semantic error: 8 minus response for a 7-point scale and supervisor_15_R needs a capital S\n  filter(Supervisor_7 == 1) %&gt;% # needs a Boolean expression == instead of =\n  select(Code, starts_with(\"Super\"), -Supervisor_7, -Supervisor_15_R) %&gt;% # no pipe at the end, the rest is actually legit\n  pivot_longer(cols = -Code, names_to = \"Item\", values_to = \"Response\") %&gt;% # pivot_longer instead of pivot_wider\n  group_by(Code) %&gt;% # Code rather than Time2_Code - the reduced dataset does not contain Time2_Code\n  summarise(Mean_Supervisor_Support = mean(Response, na.rm = TRUE)) %&gt;% # Score_corrected doesn't exist; needs to be Response\n  ungroup()\n\n\nNote that the semantic error in line 2 will not give you an error message.\nWere you thrown off by the starts_with(\"Super\") expression in line 4? starts_with() and ends_with() are great alternatives to selecting columns via : But, using select(Code, Supervisor_1:Supervisor_6, Supervisor_8:Supervisor_14) would have given us the same result. [I admit, that one was perhaps a bit mean]"
  },
  {
    "objectID": "03-wrangling2.html#activity-6-join-everything-together-with-_join",
    "href": "03-wrangling2.html#activity-6-join-everything-together-with-_join",
    "title": "3  Data wrangling II",
    "section": "\n3.6 Activity 6: Join everything together with ???_join()\n",
    "text": "3.6 Activity 6: Join everything together with ???_join()\n\nTime to join all the relevant data files into a single dataframe, which will be used in the next chapters on data visualization. There are four ways to join data: inner_join(), left_join(), right_join(), and full_join(). Each function behaves differently in terms of what information is retained from the two data objects. Here is a quick overview:\n\n\n\n\n\n\nInfo on mutating joins\n\n\n\nYou have 4 types of join functions you could make use of. Click on the panels to know more\n\n\ninner_join()\nleft_join()\nright_join()\nfull_join()\n\n\n\ninner_join() returns only the rows where the values in the column specified in the by = statement match in both tables.\n\n\ninner_join(): gif by Garrick Aden-Buie\n\n\n\nleft_join() retains the complete first (left) table and adds values from the second (right) table that have matching values in the column specified in the by = statement. Rows in the left table with no match in the right table will have missing values (NA) in the new columns.\n\n\nleft_join(): gif by Garrick Aden-Buie\n\n\n\nright_join() retains the complete second (right) table and adds values from the first (left) table that have matching values in the column specified in the by = statement. Rows in the right table with no match in the left table will have missing values (NA) in the new columns.\n\n\nright_join(): gif by Garrick Aden-Buie\n\n\n\nfull_join() returns all rows and all columns from both tables. NA values fill unmatched rows.\n\n\nfull_join(): gif by Garrick Aden-Buie\n\n\n\n\n\n\nFrom our original data_prp, we need to select demographics data and all summarised questionnaire data from time point 2. Next, we will join this with all other aggregated datasets from time point 1 which are currently stored in separate data objects in the Global Environment.\nWhile you may be familiar with inner_join() from last year, for this task, we want to retain all data from all the data objects. Therefore, we will use full_join(). Keep in mind, you can only join two data objects at a time, so the upcoming code chunk will involve a fair bit of piping and joining.\nNote: Since I (Gaby) like my columns arranged in a meaningful way, I will use select() at the end to order them better.\n\ndata_prp_final &lt;- data_prp %&gt;% \n  select(Code:Plan_prereg, Other_OS_behav_2:Time2_Understanding_OS) %&gt;% \n  full_join(qrp_t1) %&gt;% \n  full_join(understanding_t1) %&gt;% \n  full_join(sats_t1) %&gt;% \n  full_join(super) %&gt;% \n  select(Code:Plan_prereg, Pre_reg_group, SATS28_Affect_Time1_mean, SATS28_CognitiveCompetence_Time1_mean, SATS28_Value_Time1_mean, SATS28_Difficulty_Time1_mean, QRPs_Acceptance_Time1_mean, Time1_Understanding_OS, Other_OS_behav_2:Time2_Understanding_OS, Mean_Supervisor_Support)\n\n\n\n\n\n\n\nNo by argument in the code above?\n\n\n\nNote how I didn’t include a by argument in the code above. If you leave by = out, R will join the 2 data objects by ALL columns that have the same name.\nSpecial case 1: matching column names but different values\nIf you want more control, you should include the by argument; for example, if both data objects include a column age but data was recorded at 2 different time points. In that case, the information from both age columns should be retained and the by argument would not include age.\nSpecial case 2: different column names but matching values\nAnother special case presents when both data objects contain identical information but the variable names don’t match. Let’s say, both data objects contain gender information, but in one data object the variable is named gender and in the other one gender_label. In that case, your by argument needs to be modified as: by = join_by(gender == gender_label).\nMore info on joins can be found https://www.tidyverse.org/blog/2023/01/dplyr-1-1-0-joins/\n\n\nAnd this is basically the dataset we need for Chapter 4 and ?sec-dataviz2."
  },
  {
    "objectID": "03-wrangling2.html#activity-7-knit-and-export",
    "href": "03-wrangling2.html#activity-7-knit-and-export",
    "title": "3  Data wrangling II",
    "section": "\n3.7 Activity 7: Knit and export",
    "text": "3.7 Activity 7: Knit and export\nKnit the .Rmd file to ensure everything runs as expected. Once it does, export the data object data_prp_final as a csv for use in the Chapter 4. Name it something meaningful, something like data_prp_for_ch4.csv.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nwrite_csv(data_prp_final, \"data_prp_for_ch4.csv\")"
  },
  {
    "objectID": "03-wrangling2.html#pair-coding",
    "href": "03-wrangling2.html#pair-coding",
    "title": "3  Data wrangling II",
    "section": "Pair-coding",
    "text": "Pair-coding\nWe will once again be working with data from Binfet et al. (2021), which focuses on the randomised controlled trials data involving therapy dog interventions. Today, our goal is to calculate the average Loneliness score for each participant measured at time point 1 (pre-intervention) using the raw data file dog_data_raw. Currently, the data looks like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRID\nL1_1\nL1_2\nL1_3\nL1_4\nL1_5\nL1_6\nL1_7\nL1_8\nL1_9\nL1_10\nL1_11\nL1_12\nL1_13\nL1_14\nL1_15\nL1_16\nL1_17\nL1_18\nL1_19\nL1_20\n\n\n\n1\n3\n3\n4\n3\n2\n3\n1\n2\n3\n4\n3\n1\n3\n1\n2\n3\n2\n3\n2\n4\n\n\n2\n3\n2\n3\n3\n4\n3\n2\n2\n4\n3\n2\n2\n1\n2\n4\n3\n3\n2\n4\n3\n\n\n3\n3\n3\n2\n3\n3\n4\n2\n3\n3\n3\n2\n2\n2\n2\n3\n3\n4\n3\n3\n3\n\n\n4\n4\n2\n2\n3\n4\n4\n1\n3\n3\n4\n2\n1\n2\n2\n4\n4\n3\n3\n4\n3\n\n\n5\n2\n3\n3\n3\n4\n3\n2\n2\n3\n2\n4\n4\n4\n3\n2\n2\n3\n4\n3\n2\n\n\n\n\n\n\nBut we want the data to look like this:\n\n\n\n\n\nRID\nLoneliness_pre\n\n\n\n1\n2.25\n\n\n2\n1.90\n\n\n3\n2.25\n\n\n4\n1.75\n\n\n5\n2.85\n\n\n\n\n\n\nThis task is a bit more challenging compared to last week’s lab activity, as the Loneliness scale includes some reverse-coded items.\nTask 1: Open the R project for the lab\nTask 2: Open your .Rmd file from last week or create a new .Rmd file\nYou could continue the .Rmd file you used last week, or create a new .Rmd. If you need some guidance, have a look at Section 1.3.\nTask 3: Load in the library and read in the data\nThe data should already be in your project folder. If you want a fresh copy, you can download the data again here: data_pair_ch1.\nWe are using the package tidyverse today, and the datafile we should read in is dog_data_raw.csv.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n# loading tidyverse into the library\nlibrary(???)\n\n# reading in `dog_data_raw.csv`\ndog_data_raw &lt;- read_csv(\"???\")\n\n\n\n\nTask 4: Calculating the mean for Loneliness_pre\n\n\n\nStep 1: Select all relevant columns, such as the participant ID and all 20 items of the Loneliness questionnaire completed by participants before the intervention. Store this data in an object called data_loneliness.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nLook at the codebook. Try to figure out\n\nthe variable name of the column in which the participant id is stored, and\nwhich items relate to the Loneliness scale at Stage “pre”\n\n\n\n\n\n\n\nMore concrete hint\n\n\n\n\n\n\nthe participant id column is called RID\n\nThe Loneliness items at pre-intervention stage start with L1_\n\n\n\n\n\n\n\n\n\n\nStep 2: Pivot the data from wide format to long format so we can reverse-score and calculate the average score more easily (in step 3)\n\n\n\n\n\n\n\nHint\n\n\n\n\n\npivot_\nWe also need 3 arguments in that function:\n\nthe columns we want to select (e.g., all the loneliness items),\nthe name of the column in which the current column headings will be stored (e.g., “Qs”), and\nthe name of the column that should store all the values (e.g., “Responses”).\n\n\n\n\n\n\n\nMore concrete hint\n\n\n\n\n\n\n  pivot_longer(cols = ???, names_to = \"???\", values_to = \"???\")\n\n\n\n\n\n\n\n\n\nStep 3: Reverse-scoring\n\nIdentify the items on the Loneliness scale that are reverse-coded, and then reverse-score them accordingly.\n\n\n\n\n\n\nHint\n\n\n\n\n\nWe need to figure out:\n\nwhich are the items of the loneliness scale we need to reverse-score\nwhat is the measuring scale of loneliness so we can determine the new values\nwhich function to use to create a new column that has the corrected scores in it\nwhich one of the case_ functions will get us there\n\n\n\n\n\n\n\nMore concrete hint\n\n\n\n\n\n\nThe items to be reverse-coded items can be found in the codebook: L1_1, L1_5, L1_6, L1_9, L1_10, L1_15, L1_16, L1_19, L1_20\nthe loneliness scale ranges from 1 to 4, so we need to replace 1 with 4, 2 with 3, 3 with 2, and 4 with 1\nthe function to create a new column mutate()\n\nit’s a conditional statement rather than “just” replacing values, hence we need case_when()\n\n\n\n  mutate(Score_corrected = case_when(\n    ??? ~ ???,\n    .default = ???\n    ))\n\n\n\n\n\n\n\n\n\nStep 4: Calculate the average Loneliness score per participant. To match with the table above, we want to call this column Loneliness_pre\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\ngrouping and summarising\n\n\n\n\n\n\nMore concrete hint\n\n\n\n\n\n\n  group_by(???) %&gt;% \n  summarise(Loneliness_pre = ???(???)) %&gt;% \n  ungroup()\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# loading tidyverse into the library\nlibrary(tidyverse)\n\n# reading in `dog_data_raw.csv`\ndog_data_raw &lt;- read_csv(\"dog_data_raw.csv\")\n\n# Task 4: Tidying \nloneliness_tidy &lt;- dog_data_raw %&gt;% \n  # Step 1\n  select(RID, starts_with(\"L1\")) %&gt;% # select(RID, L1_1:L1_20) also works\n  # Step 2\n  pivot_longer(cols = -RID, names_to = \"Qs\", values_to = \"Response\") %&gt;% \n  # Step 3\n  mutate(Score_corrected = case_when(\n    Qs %in% c(\"L1_1\", \"L1_5\", \"L1_6\", \"L1_9\", \"L1_10\", \"L1_15\", \"L1_16\", \"L1_19\", \"L1_20\") ~ 5-Response,\n    .default = Response\n    )) %&gt;% \n  # Step 4\n  group_by(RID) %&gt;% \n  summarise(Loneliness_pre = mean(Score_corrected, na.rm = TRUE)) %&gt;% \n  ungroup()"
  },
  {
    "objectID": "03-wrangling2.html#test-your-knowledge-and-challenge-yourself",
    "href": "03-wrangling2.html#test-your-knowledge-and-challenge-yourself",
    "title": "3  Data wrangling II",
    "section": "Test your knowledge and challenge yourself",
    "text": "Test your knowledge and challenge yourself\nKnowledge check\nQuestion 1\nWhen using mutate(), which additional function could you use to recode an existing variable? \narrange()\nfilter()\ncase_match()\ncase_when()\nQuestion 2\nWhen using mutate(), which additional function could you use to create a new variable based on one or multiple conditional statements? \narrange()\nfilter()\ncase_match()\ncase_when()\nQuestion 3\nWhich of the following functions would you use if you wanted to join two data sets by their shared identifier? \ninner_join()\nleft_join()\nright_join()\nfull_join()\nQuestion 4\nYour data object contains a column Score with numbers, but they have been read in incorrectly as a character datatype. Which of the following functions would not work for fixing this issue? \nparse_number()\nfactor(Score)\nmutate(Score = as.numeric(Score))\nas.numeric()\n\n\n\n\n\n\nExplain this answer\n\n\n\n\n\n\n\nparse_number() from the readr package extracts numeric values from strings, so this would work.\n\nfactor(Score): This would not work as expected because it converts the column into a factor, not a numeric datatype, leading to incorrect results if numeric operations are needed.\n\nmutate(Score = as.numeric(Score)): This would work too because mutate() can be used in combination with as.numeric() to create a new numeric column or override the existing character column.\n\nas.numeric(): This would also work to convert a character column to numeric. Without mutate, you could use it in a BaseR way, e.g., data$Score &lt;- as.numeric(data$Score) (shudder, BaseR!!! But effective)\n\n\n\n\nChallenge yourself\nIf you want to challenge yourself and further apply the skills from Chapter 3, you could wrangle the data from dog_data_raw for one of the other questionnaires. There are plenty of options to choose from:\n\n\n\n\n\n\nDifficulty level: easy\n\n\n\n\n\n\nrecode column Live_Pets so the values read yes and no rather than 1 and 2\nrecode Year_of_Study so they have the labels from the codebook rather than the numbers\nreverse-code the Homesickness scale for _pre and _post\n\nrenaming the columns of the other one-item scales as Stress_pre, Stress_post, Engagement_pre and Engagement_post\n\n\nAny of these tasks should be doable in one step. No need to select or pivot anything. You could just modify dog_data_raw.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nFor the recoding tasks, you need to work out which function to use to recode one value as another - just plain replacing, no conditional statements\nThe reverse-coding might sound daunting to do in one step, but it is only a single value that needs to be recoded. Take some inspiration from Activity 5 (error mode).\nFor the renaming tasks, check how you would change column names without reducing the number of columns overall\n\n\n\n\n\n\n\n\n\n\nSolution for Challenge yourself - easy\n\n\n\n\n\n\n## Live_Pets\ndog_data_raw &lt;- dog_data_raw %&gt;%\n  mutate(Live_Pets = case_match(Live_Pets,\n                                1 ~ \"yes\",\n                                2 ~ \"no\"))\n\n\n## Year of Study\ndog_data_raw &lt;- dog_data_raw %&gt;%\n  mutate(Year_of_Study = case_match(Year_of_Study,\n                                    1 ~ \"First\",\n                                    2 ~ \"Second\",\n                                    3 ~ \"Third\",\n                                    4 ~ \"Fourth\",\n                                    5 ~ \"Fifth or above\"))\n\n\n## Reverse-coding of homesickness pre and post. It's a 5-point scale, hence you'd calculate 6-the original response column\ndog_data_raw &lt;- dog_data_raw %&gt;% \n  mutate(Homesick_pre = 6-HO1_1,\n         Homesick_post = 6-HO2_1)\n\n\n## Renaming of Stress and Engagement\ndog_data_raw &lt;- dog_data_raw %&gt;% \n  rename(Stress_pre = S1_1, Stress_post = S2_1, Engagement_pre = HO1_2, Engagement_post = HO2_2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nDifficulty level: medium\n\n\n\n\n\n\nreverse-code the Social connectedness scale (pre-intervention) and compute a mean score per participant\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nThis task would take 4 steps to complete. These are the exact same steps we applied to Loneliness_pre in the lab activity. You would just need to figure out which items are related to the Social connectedness scale (pre-intervention) and which ones of those are reverse-coded. The codebook has all the answers.\n\n\n\n\n\n\n\n\n\nSolution for Challenge yourself - medium\n\n\n\n\n\n\n## SCS pre\nscs_pre &lt;- dog_data_raw %&gt;% \n  select(RID, starts_with(\"SC1\")) %&gt;% \n  pivot_longer(cols = -RID, names_to = \"Names\", values_to = \"Response\") %&gt;% \n  mutate(Score_corrected = case_when(\n    Names %in% c(\"SC1_3\", \"SC1_6\", \"SC1_7\", \"SC1_9\", \"SC1_11\", \"SC1_13\", \"SC1_15\", \"SC1_17\", \"SC1_18\", \"SC1_20\") ~ 7-Response,\n    .default = Response\n    )) %&gt;% \n  group_by(RID) %&gt;% \n  summarise(SCS_pre = mean(Score_corrected, na.rm = TRUE)) %&gt;% \n  ungroup()\n\n\n\n\n\n\n\n\n\n\n\n\n\nDifficulty level: hard\n\n\n\n\n\n\nreverse-code the Loneliness scale (post-intervention) and compute a mean score per participant\nreverse-code the Social connectedness scale (post-intervention) and compute a mean score per participant\n\nBoth activities are similar to Activity 3 from the individual walkthrough and would take about 5 steps to complete. Start by mapping out the steps.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\n\nStep 1: Select all relevant columns, such as participant ID and all the items that belong to the questionnaire that participants completed after the intervention\n\nStep 2: Pivot the data from wide format to long format so we can reverse-score and calculate the average score more easily\n\nStep 3: Recode the initial responses so that the new column has numbers instead of labels\n\nStep 4: Reverse-score the items that are labelled as “Reverse” in the codebook and then reverse-score them\n\nStep 5: Group by and summarise to calculate the mean Score\n\n\n\n\n\n\n\n\n\n\nSolution for Challenge yourself - hard\n\n\n\n\n\n\n## loneliness post\nlonely_post &lt;- dog_data_raw %&gt;% \n  # Step 1\n  select(RID, starts_with(\"L2\")) %&gt;% \n  # Step 2\n  pivot_longer(cols = -RID, names_to = \"Names\", values_to = \"Response\") %&gt;% \n  # Step 3\n  mutate(Score = case_match(Response,\n                            \"never\" ~ 1,\n                            \"rarely\" ~ 2,\n                            \"sometimes\" ~ 3,\n                            \"often\" ~ 4,\n                            .default = NA\n  ),\n  # Step 4 - we are still in the same mutate function (count the brackets)\n        Score_corrected = case_when(\n          Names %in% c(\"L2_1\", \"L2_5\", \"L2_6\", \"L2_9\", \"L2_10\", \"L2_15\", \"L2_16\", \"L2_19\", \"L2_20\") ~ 5-Score,\n          .default = Score\n  )) %&gt;% \n  # Step 5\n  group_by(RID) %&gt;% \n  summarise(Loneliness_post = mean(Score_corrected, na.rm = TRUE)) %&gt;% \n  ungroup()\n\n\n## SCS post\nscs_post &lt;- dog_data_raw %&gt;% \n  # Step 1\n  select(RID, starts_with(\"SC2\")) %&gt;% \n  # Step 2\n  pivot_longer(cols = -RID, names_to = \"Names\", values_to = \"Response\") %&gt;% \n  # Step 3\n  mutate(Response = case_match(Response,\n                               \"strongly disagree\" ~ \"1\",\n                               \"strongly agree\" ~ \"6\",\n                               .default = Response),\n         Response = parse_number(Response),\n  # Step 4 - we are still in the same mutate function (count the brackets)\n         Score_corrected = case_when(\n           Names %in% c(\"SC2_3\", \"SC2_6\", \"SC2_7\", \"SC2_9\", \"SC2_11\", \"SC2_13\", \"SC2_15\", \"SC2_17\", \"SC2_18\", \"SC2_20\") ~ 7-Response,\n           .default = Response\n         )) %&gt;% \n  # Step 5\n  group_by(RID) %&gt;% \n  summarise(SCS_post = mean(Score_corrected, na.rm = TRUE)) %&gt;% \n  ungroup()\n\n\n\n\n\n\n\n\n\n\n\n\n\nDifficulty level: extra hard\n\n\n\n\n\n\nPANAS: positive and negative affect of pre- and post-intervention in a single pipe rather than in 4 different data objects (see last week’s)\n\nThis task would take about 7 steps to get it from\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRID\nPN1_1\nPN1_2\nPN1_3\nPN1_4\nPN1_5\nPN1_6\nPN1_7\nPN1_8\nPN1_9\nPN1_10\nPN2_1\nPN2_2\nPN2_3\nPN2_4\nPN2_5\nPN2_6\nPN2_7\nPN2_8\nPN2_9\nPN2_10\n\n\n\n1\n1\n1\n1\n1\n4\n1\n4\n3\n1\n4\n2\n1\n3\n1\n4\n1\n4\n4\n1\n4\n\n\n2\n1\n2\n3\n2\n1\n3\n3\n4\n1\n4\n1\n1\n2\n1\n3\n1\n3\n4\n1\n4\n\n\n3\n1\n1\n3\n1\n2\n4\n4\n3\n1\n2\n2\n2\n3\n1\n3\n2\n4\n3\n1\n2\n\n\n4\n1\n1\n5\n1\n4\n3\n5\n5\n3\n2\n1\n1\n5\n1\n4\n3\n4\n4\n2\n2\n\n\n5\n2\n3\n5\n2\n3\n2\n3\n4\n2\n2\n1\n2\n5\n2\n3\n2\n4\n5\n1\n3\n\n\n\n\n\n\nto\n\n\n\n\n\nRID\nStage\nPANAS_NA\nPANAS_PA\n\n\n\n1\npost\n1.2\n3.8\n\n\n1\npre\n1.0\n3.2\n\n\n2\npost\n1.0\n3.2\n\n\n2\npre\n1.8\n3.0\n\n\n3\npost\n1.6\n3.0\n\n\n\n\n\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nStart by mapping out the steps\n\n\nStep 1: select all relevant columns, such as participant ID and all the items that belong to PANAs scale (pos, neg, pre, and post)\n\nStep 2: pivot the data from wide format to long format. You want to do that for ALL columns that are not the participant id. The data object should have 3 columns and 5680 observations, i.e. each participant has 20 rows.\n\nStep 3: All of the items will have the structure PN1_1. Use separate to split the information across 2 columns. First column has information about the Stage, second column should turn into an Item_number and it should convert into a numeric column in the process to save you typing quotation marks in Step 5.\n\n\nStep 4: recode the Stage column you just created so that everything that starts with PN1 relates to “pre” and PN2 as post.\n\nStep 5: identify the subscales positive affect (PA) and negative affect (NA) by item number and recode them. This requires a conditional statement.\n\nStep 6: group by and summarise to calculate the mean Score\n\nStep 7: pivot, so that you have the 2 PANAS subscales presented in separate columns (see table above). You might need an extra step if the columns aren’t labelled exactly as shown in the table above.\n\n\n\n\n\n\n\n\n\n\nSolution for Challenge yourself - extra hard\n\n\n\n\n\n\nPANAS &lt;- dog_data_raw %&gt;% \n  # Step 1\n  select(RID, starts_with(\"PN\")) %&gt;% \n  # Step 2\n  pivot_longer(cols = -RID, names_to = \"Items\", values_to = \"Scores\") %&gt;% \n  # Step 3\n  separate(Items, into = c(\"Stage\", \"Item_number\"), sep = \"_\", convert = TRUE) %&gt;% \n  # Step 4 recode Stage\n  mutate(Stage = case_match(Stage,\n                            \"PN1\" ~ \"pre\",\n                            \"PN2\" ~ \"post\")) %&gt;% \n  # Step 5 identify subscales by item number\n  mutate(Subscales = case_when(\n    Item_number %in% c(3, 5, 7, 8, 10) ~ \"PANAS_PA\",\n    .default = \"PANAS_NA\"\n  )) %&gt;% \n  # Step 6 \n  group_by(RID, Stage, Subscales) %&gt;% \n  summarise(Score = mean(Scores)) %&gt;% \n  ungroup() %&gt;% \n  # Step 7 - to make the data look like the data in `dog_data_clean_long.csv`\n  pivot_wider(names_from = Subscales, values_from = Score)"
  },
  {
    "objectID": "04-dataviz.html#intended-learning-outcomes",
    "href": "04-dataviz.html#intended-learning-outcomes",
    "title": "4  Data viz I",
    "section": "Intended Learning Outcomes",
    "text": "Intended Learning Outcomes\nBy the end of this chapter, you should be able to:\n\nexplain the layered grammar of graphics\nchoose an appropriate plot for categorical variables\ncreate a basic version of an appropriate plot\napply additional layers to modify the appearance of the plot\n\nIt is time to think about selecting the most appropriate plot for your data. Different types of variables call for different kinds of plots, which depends on how many variables you’re aiming to plot and what their data types are. In this chapter, we will focus on plots for categorical data. Next week, we will explore plots for continuous variables and learn which plots work best when combining continuous and categorical data."
  },
  {
    "objectID": "04-dataviz.html#individual-walkthrough",
    "href": "04-dataviz.html#individual-walkthrough",
    "title": "4  Data viz I",
    "section": "Individual Walkthrough",
    "text": "Individual Walkthrough"
  },
  {
    "objectID": "04-dataviz.html#building-plots",
    "href": "04-dataviz.html#building-plots",
    "title": "4  Data viz I",
    "section": "\n4.1 Building plots",
    "text": "4.1 Building plots\nWe are using the package ggplot2 to create data visualisations. It’s part of the tidyverse package. Actually, most people call th package ggplot but it’s official name is ggplot2.\nWe’ll be using the ggplot2 package to create data visualisations. It’s part of the tidyverse suite of packages. Although many people refer to it simply as ggplot, its official name is ggplot2.\n\n\nggplot2 uses a layered grammar of graphics, where plots are constructed through a series of layers. You start with a base layer (by calling ggplot), then add data and aesthetics, followed by selecting the appropriate geometries for the plot.\nThese first 3 layers will give you the most simple version of a complete plot. However, you can enhance the plot’s clarity and appearance by adding additional layers such as scales, facets, coordinates, labels and themes.\n\n\n\n\ngg layers (Presentation by Ryan Safner)\n\n\n\nTo give you a brief overview of the layering system, we will use the palmerpenguins package (https://allisonhorst.github.io/palmerpenguins/). This dataset contains information about penguins, including bill length and depth, flipper length, body mass, and more.\n\nhead(penguins)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n2007\n\n\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n2007\n\n\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n2007\n\n\nAdelie\nTorgersen\nNA\nNA\nNA\nNA\nNA\n2007\n\n\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n2007\n\n\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n2007\n\n\n\n\n\n\nLet’s build a basic scatterplot to show the relationship between flipper_length and body_mass. We will customise plots further later on in the individual plots. This is just a quick overview of the different layers.\nLet’s build a basic scatterplot to show the relationship between flipper_length and body_mass. We will further customise the plots in subsequent sections, but for now, this will provide a quick overview of the different layers.\n\n\nLayer 1 creates the base plot that we build upon.\n\nLayer 2 adds the data and some aesthetics:\n\nThe data is passed as the first argument.\nAesthetics are added via the mapping argument, where you define your variables (e.g., x or both x and y). This also allows you to specify general properties, like the color for grouping variables, etc.\n\n\n\nLayer 3 adds geometries, or geom_? for short. This tells ggplot how to display the data points. Remember to add these layers with a +, rather than using a pipe (%&gt;%). You can also add multiple geoms if needed, for example, combining a violin plot with a boxplot.\n\nLayer 4 includes scale_? functions, which let you customise aesthetics like color. You can do much more with scales, but we’ll explore later.\n\nLayer 5 introduces facets, such as facet_wrap(), allowing you to add an extra dimension to your plot by showing the relationship you are interested in for each level of a categorical variable.\n\nLayer 6 involves coordinates, where coord_cartesian() controls the limits for the x- and y-axes (xlim and ylim), enabling you to zoom in or out of the plot.\n\nLayer 7 helps you modify axis labels.\n\nLayer 8 controls the overall style of the plot, including background color, text size, and borders. R provides several predefined themes, such as theme_classic, theme_bw, theme_minimal, and theme_light.\n\nClick on the tabs below to see how each layer contributes to refining the plot.\n\n\nLayer 1\nLayer 2\nLayer 3\nLayer 4\nLayer 5\nLayer 6\nLayer 7\nLayer 8\n\n\n\n\nggplot()\n\n\n\n\n\n\n\nThere’s not much to see at this stage - this is basically an empty plot layer.\n\n\n\nggplot(data = penguins, mapping = aes(x = body_mass_g, y = flipper_length_mm))\n\n\n\n\n\n\n\nYou won’t see any data points yet because we haven’t specified how to display them. However, we have mapped the aesthetics, indicating that we want to plot body_mass on the x-axis and flipper_length on the y-axis. This also sets the axis titles, as well as the axis values and breakpoints.\n\n\n\n\n\n\nTip\n\n\n\nYou won’t need to add data = or mapping = if you keep those arguments in exactly that order. Likewise, the first column name you enter within the aes() function will always be interpreted as x, and the second as y, so you could omit them if you wish.\nYou don’t need to include data = or mapping = if you keep those arguments in the default order. Similarly, the first column name you enter in the aes() function will automatically be interpreted as the x variable, and the second as y, so you can omit specifying x and y if you prefer.\n\nggplot(penguins, aes(body_mass_g, flipper_length_mm))\n\nwill give you the same output as the code above.\n\n\n\n\n\nggplot(data = penguins, mapping = aes(x = body_mass_g, y = flipper_length_mm, colour = sex)) +\n  geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nHere we are telling ggplot to add a scatterplot. You may notice a warning indicating that some rows were removed due to missing values.\nThe colour argument adds colour to the points based on a grouping variable (in this case, sex). If you want all the points to be black — representing only two dimensions rather than three — simply omit the colour argument.\n\n\n\nggplot(data = penguins, mapping = aes(x = body_mass_g, y = flipper_length_mm, colour = sex)) +\n  geom_point() +\n  # changes colour palette\n  scale_colour_brewer(palette = \"Dark2\") + \n  # add breaks from 2500 to 6500 in increasing steps of 500\n  scale_x_continuous(breaks = seq(from = 2500, to = 6500, by = 500)) \n\nWarning: Removed 11 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nThe scale_? functions allow us to modify the color palette of the plot, adjust axis breaks, and more. You could change the axis labels within scale_x_continuous() as well or leave it for Layer 7.\n\n\n\nggplot(data = penguins, mapping = aes(x=body_mass_g, y=flipper_length_mm, colour=sex)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") + \n  # split main plot up into different subplots by species \n  facet_wrap(~ species) \n\nWarning: Removed 11 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nIn this step, we’re using faceting to split the plot by species.\n\n\n\nggplot(data = penguins, mapping = aes(x=body_mass_g, y=flipper_length_mm, colour=sex)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") + \n  facet_wrap(~ species) +\n  # limits the range of the y axis\n  coord_cartesian(ylim = c(0, 250)) \n\nWarning: Removed 11 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nHere we adjust the limits of the y-axis to zoom out of the plot. If you want to zoom in or out of the x-axis, you can add the xlim argument to the coord_cartesian() function.\n\n\n\nggplot(data = penguins, mapping = aes(x=body_mass_g, y=flipper_length_mm, colour=sex)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") + \n  facet_wrap(~ species) +\n  labs(x = \"Body Mass (in g)\", # labels the x axis\n       y = \"Flipper length (in mm)\", # labels the y axis\n       colour = \"Sex\") # labels the grouping variable in the legend\n\nWarning: Removed 11 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nYou can change the axis labels using the labs() function, or you can modify them when adjusting the scales (e.g., within the scale_x_continuous() function).\n\n\n\nggplot(data = penguins, mapping = aes(x=body_mass_g, y=flipper_length_mm, colour=sex)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") + \n  facet_wrap(~ species) +\n  labs(x = \"Body Mass (in g)\", \n       y = \"Flipper length (in mm)\",\n       colour = \"Sex\") +\n  # add a theme\n  theme_classic()\n\nWarning: Removed 11 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nThe theme_classic() function is applied to change the overall appearance of the plot.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nYou need to stick to the first three layers to create your base plot. Everything else is optional, meaning you don’t need to use all eight layers. Additionally, layers 4-8 can be added in any order (more or less), whereas layers 1-3 must follow a fixed sequence."
  },
  {
    "objectID": "04-dataviz.html#activity-1-set-up-and-data-for-today",
    "href": "04-dataviz.html#activity-1-set-up-and-data-for-today",
    "title": "4  Data viz I",
    "section": "\n4.2 Activity 1: Set-up and data for today",
    "text": "4.2 Activity 1: Set-up and data for today\n\nWe are still working with the data from Pownall et al. (2023), so open your project.\nHowever, let’s start with a fresh R Markdown file: Create a new .Rmd file and save it in your project folder. Give it a meaningful name (e.g., “chapter_04.Rmd” or “04_data_viz.Rmd”). If you need guidance, refer to Section 1.3. Delete everything below line 12, but keep the setup code chunk.\nWe previously aggregated the data in Chapter 2 and Chapter 3. If you want a fresh copy, download the data here: data_prp_for_ch4.csv. Make sure to place the csv file in the project folder.\nIf you need a reminder about the data and variables, check the codebook or refer back to Section 1.4."
  },
  {
    "objectID": "04-dataviz.html#activity-2-load-in-libraries-read-in-data-and-adjust-data-types",
    "href": "04-dataviz.html#activity-2-load-in-libraries-read-in-data-and-adjust-data-types",
    "title": "4  Data viz I",
    "section": "\n4.3 Activity 2: Load in libraries, read in data, and adjust data types",
    "text": "4.3 Activity 2: Load in libraries, read in data, and adjust data types\nToday, we will be using the tidyverse package and the dataset data_prp_for_ch4.csv.\n\n## packages \n???\n\n## data\ndata_prp_viz &lt;- read_csv(???)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(tidyverse)\ndata_prp_viz &lt;- read_csv(\"data_prp_for_ch4.csv\")\n\n\n\n\nAs mentioned in Section 1.6, it is always a good idea to take a glimpse at the data to see how many variables and observations are in the dataset, as well as the data types.\n\n\n\n\n\n\nglimpse output\n\n\n\n\n\n\nglimpse(data_prp_viz)\n\nRows: 89\nColumns: 28\n$ Code                                  &lt;chr&gt; \"Tr10\", \"Bi07\", \"SK03\", \"SM95\", …\n$ Gender                                &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2,…\n$ Age                                   &lt;dbl&gt; 22, 20, 22, 26, 22, 20, 21, 21, …\n$ Ethnicity                             &lt;chr&gt; \"White European\", \"White British…\n$ Secondyeargrade                       &lt;dbl&gt; 2, 3, 1, 2, 2, 2, 2, 2, 1, 1, 1,…\n$ Opptional_mod                         &lt;dbl&gt; 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2,…\n$ Opptional_mod_1_TEXT                  &lt;chr&gt; \"Research methods in first year\"…\n$ Research_exp                          &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ Research_exp_1_TEXT                   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Plan_prereg                           &lt;dbl&gt; 1, 3, 1, 2, 1, 1, 3, 3, 2, 2, 2,…\n$ Pre_reg_group                         &lt;dbl&gt; 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 2,…\n$ SATS28_Affect_Time1_mean              &lt;dbl&gt; 4.000000, 3.833333, 5.000000, 5.…\n$ SATS28_CognitiveCompetence_Time1_mean &lt;dbl&gt; 5.166667, 5.166667, 5.666667, 4.…\n$ SATS28_Value_Time1_mean               &lt;dbl&gt; 6.000000, 6.666667, 5.222222, 5.…\n$ SATS28_Difficulty_Time1_mean          &lt;dbl&gt; 3.571429, 2.428571, 3.571429, 3.…\n$ QRPs_Acceptance_Time1_mean            &lt;dbl&gt; 5.909091, 6.090909, 6.545455, 5.…\n$ Time1_Understanding_OS                &lt;dbl&gt; 5.500000, 3.166667, 4.500000, 3.…\n$ Other_OS_behav_2                      &lt;dbl&gt; 1, NA, NA, NA, 1, NA, NA, 1, NA,…\n$ Other_OS_behav_4                      &lt;dbl&gt; 1, NA, NA, NA, NA, NA, NA, NA, N…\n$ Other_OS_behav_5                      &lt;dbl&gt; NA, NA, NA, NA, 1, 1, NA, NA, NA…\n$ Closely_follow                        &lt;dbl&gt; 2, 2, 2, NA, 3, 3, 3, NA, NA, 2,…\n$ SATS28_Affect_Time2_mean              &lt;dbl&gt; 3.500000, 3.166667, 4.833333, 4.…\n$ SATS28_CognitiveCompetence_Time2_mean &lt;dbl&gt; 4.166667, 4.666667, 6.166667, 5.…\n$ SATS28_Value_Time2_mean               &lt;dbl&gt; 3.000000, 6.222222, 6.000000, 4.…\n$ SATS28_Difficulty_Time2_mean          &lt;dbl&gt; 2.857143, 2.857143, 4.000000, 2.…\n$ QRPs_Acceptance_Time2_mean            &lt;dbl&gt; 5.636364, 5.454545, 6.272727, 5.…\n$ Time2_Understanding_OS                &lt;dbl&gt; 5.583333, 3.333333, 5.416667, 4.…\n$ Mean_Supervisor_Support               &lt;dbl&gt; 5.230769, 6.285714, 6.857143, 2.…\n\n\n\n\n\nWe can see that some of the categorical data in data_prp_viz was read in as numeric variables which makes them continuous. This will haunt us big time when building the plots. We would be better off addressing these changes in the dataset before we start plotting (and potentially getting frustrated with R and data viz in general).\nLet’s convert some of the categorical variables into factors.\n\ndata_prp_viz &lt;- data_prp_viz %&gt;% \n  mutate(Gender = factor(Gender,\n                         levels = c(2, 1, 3),\n                         labels = c(\"females\", \"males\", \"non-binary\")),\n         Secondyeargrade = factor(Secondyeargrade,\n                                  levels = c(1, 2, 3, 4, 5),\n                                  labels = c(\"≥ 70% (1st class grade)\", \"60-69% (2:1 grade)\", \"50-59% (2:2 grade)\", \"40-49% (3rd class)\", \"&lt; 40%\")),\n         Plan_prereg = factor(Plan_prereg,\n                              levels = c(1, 3, 2),\n                              labels = c(\"Yes\", \"Unsure\", \"No\")),\n         Closely_follow = factor(Closely_follow,\n                                 levels = c(2, 3),\n                                 labels = c(\"Followed it somewhat\", \"Followed it exactly\")),\n         Research_exp = factor(Research_exp),\n         Pre_reg_group = factor(Pre_reg_group))"
  },
  {
    "objectID": "04-dataviz.html#activity-3-barchart-geom_bar",
    "href": "04-dataviz.html#activity-3-barchart-geom_bar",
    "title": "4  Data viz I",
    "section": "\n4.4 Activity 3: Barchart (geom_bar())",
    "text": "4.4 Activity 3: Barchart (geom_bar())\nA bar chart is the best choice when you want to plot a single categorical variable.\nFor example, let’s say we want to count some demographic data, such as gender. To visualise the gender counts, we would use a barplot. This is done with geom_bar() in the third layer. Since the counting is done automatically in the background, the aes() function only requires an x value (i.e., the name of your variable).\n\nggplot(data_prp_viz, aes(x = Gender)) +\n  geom_bar() \n\n\n\nFigure 4.1: Default barchart\n\n\n\nThis is the base plot done. You can customise it by adding different layers. For example, the labels could be clearer, or you might want to add a splash colour. Click on the tabs below to see examples of additional customisations, and try applying them to your base plot in your own .Rmd file.\n\n\nColour\nAxes labels & margins\nLegend\nThemes\n\n\n\nWe can change the colour by adding a fill argument in the aes(). If we want to modify these colours further, we would add a scale_fill_? argument. If you have specific colours in mind, you would use scale_fill_manual(), or if you prefer to stick with pre-defined options like viridis, you can use scale_fill_viridis_d().\n\nggplot(data_prp_viz, aes(x = Gender, fill = Gender)) +\n  geom_bar() +\n  # customise colour\n  scale_fill_viridis_d()\n\n\n\n\n\n\n\n\n\nThe x-axis label is fine, but the categories need to be relabelled. You can achieve this with the scale_x_discrete() function and the labels = argument. Just make sure to order the labels according to the order in the dataframe.\nThere is also a gap between the bottom of the chart and the bars that looks a bit odd. You can remove it by using the expansion() function.\n\nggplot(data_prp_viz, aes(x = Gender, fill = Gender)) +\n  geom_bar() +\n  scale_fill_viridis_d() +\n  # changing group labels on the breaks of the x axis\n  scale_x_discrete(labels = c(\"Female\", \"Male\", \"Non-Binary\")) + \n  scale_y_continuous(\n    # changing name of the y axis\n    name = \"Count\",\n    # remove the space below the bars (first number), but keep a tiny bit (5%) above (second number)\n    expand = expansion(mult = c(0, 0.05))\n  )\n\n\n\n\n\n\n\n\n\nThe legend does not add any useful information because the labels are already provided on the x-axis. We can remove the legend by adding the argument guide = \"none\" to the scale_fill function.\n\nggplot(data_prp_viz, aes(x = Gender, fill = Gender)) +\n  geom_bar() +\n  scale_fill_viridis_d(\n    # remove the legend\n    guide = \"none\") +\n  scale_x_discrete(labels = c(\"Female\", \"Male\", \"Non-Binary\")) +\n  scale_y_continuous(\n    name = \"Count\",\n    expand = expansion(mult = c(0, 0.05))\n  )\n\n\n\n\n\n\n\n\n\nLet’s experiment with the themes. For this plot we have chosen theme_minimal().\n\nggplot(data_prp_viz, aes(x = Gender, fill = Gender)) +\n  geom_bar() +\n  scale_fill_viridis_d(\n    guide = \"none\") +\n  scale_x_discrete(labels = c(\"Female\", \"Male\", \"Non-Binary\")) +\n  scale_y_continuous(\n    name = \"Count\",\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  # pick a theme\n  theme_minimal()"
  },
  {
    "objectID": "04-dataviz.html#activity-4-column-plot-geom_col",
    "href": "04-dataviz.html#activity-4-column-plot-geom_col",
    "title": "4  Data viz I",
    "section": "\n4.5 Activity 4: Column plot (geom_col())",
    "text": "4.5 Activity 4: Column plot (geom_col())\nIf the counts had already been summarised for you, geom_bar() would not work. Instead, you’d need to use geom_col() to display the pre-aggregated data.\n\ngender_count &lt;- data_prp_viz %&gt;% \n  count(Gender)\n\ngender_count\n\n\n\n\nGender\nn\n\n\n\nfemales\n69\n\n\nmales\n17\n\n\nnon-binary\n3\n\n\n\n\n\n\nThe mapping for geom_col() requires both x and y aesthetics. In this example, x would represent the categorical variable (e.g., Gender), while y would refer to the column storing the summarised values (e.g., n). Notice how the axis title now reflects n instead of count in the base version.\n\nggplot(gender_count, aes(x = Gender, y = n, fill = Gender)) +\n  geom_col()\n\n\n\nFigure 4.2: Column plot with different coloured bars\n\n\n\n\n\n\n\n\n\nYour Turn: Make the column plot pretty\n\n\n\nThe other layers to change the colour scheme, axes labels and margins, removing the legend and altering the theme require exactly the same functions as with the boxplot above. Test yourself to see if you can…\n\n\nchange the colour scheme (e.g., viridis or any other colour palettes)\n\nremove the legend\n\nchange the titles of the x and y axes\n\nmake the bars start directly on the x-axis\n\nadd a theme of your liking\n\n\n\n\n\n\n\nPossible solution code for the column plot (with a different colour palette and a different theme)\n\n\n\n\n\n\nggplot(gender_count, aes(x = Gender, y = n, fill = Gender)) +\n  geom_col() +\n  # replaced vidiris with the brewer palette\n  scale_fill_brewer(\n    palette = \"Set1\", # try \"Set2\" or \"Dark2\" for some variety\n    guide = \"none\") + # legend removed\n  # labels of the categories changed\n  scale_x_discrete(labels = c(\"Male\", \"Female\", \"Non-Binary\")) + \n  scale_y_continuous(\n    # change y axis label\n    name = \"Count\",\n    # starts bars on x axis without any gaps but leaves some space at the top (this time 10%)\n    expand = expansion(mult = c(0, 0.1)) \n  ) +\n  # different theme\n  theme_light()"
  },
  {
    "objectID": "04-dataviz.html#activity-5-stacked-percent-stacked-and-grouped-barchart",
    "href": "04-dataviz.html#activity-5-stacked-percent-stacked-and-grouped-barchart",
    "title": "4  Data viz I",
    "section": "\n4.6 Activity 5: Stacked, Percent Stacked, and Grouped Barchart",
    "text": "4.6 Activity 5: Stacked, Percent Stacked, and Grouped Barchart\nWhen dealing with two categorical variables, you have three options for displaying stacked barcharts: the “normal” Stacked Barchart (the default option), a Percent Stacked Barchart, or a Grouped Barchart.\nFor this activity, we will explore the variable Plan_prereg, which measures whether students planned to pre-register their undergraduate dissertation at time point 1, and Pre_reg_group, which tracks whether they actually followed through with a pre-registration for their dissertation.\nOne way to display this data is by creating either a Stacked Barchart (the default) or a Percent Stacked Barchart. In both cases, the subgroups are displayed on top of each other. To make comparison easier, we will place the two plots side by side and move the legend to the bottom of the chart.\n\n## Stacked barchart\nggplot(data_prp_viz, aes(x = Plan_prereg, fill = Pre_reg_group)) +\n  geom_bar() + # no position argument added\n  theme(legend.position = \"bottom\") # move legend to the bottom\n\n## Percent stacked barchart\nggplot(data_prp_viz, aes(x = Plan_prereg, fill = Pre_reg_group)) +\n  geom_bar(position = \"fill\") + # add position argument here\n  theme(legend.position = \"bottom\") # move legend to the bottom\n\n\n\n\n\nFigure 4.3: Stacked barchart (left), and Percent stacked barchart (right)\n\n\n\nIn the stacked barchart (Figure 4.3, left plot), you can display participant numbers. From this, we can see that the highest number of students were unsure whether they wanted to pre-register their dissertation, followed closely by those who answered “yes.” We also see that the number of students who did not end up with a pre-registered dissertation (blue category) is the same for both those who had planned to pre-register and those who did not want to. However, since the “No” category has significantly fewer participants than the other two, it’s difficult to tell if the ratio remains consistent across all three groups.\nIf we want to highlight this ratio, a Percent Stacked Barchart (Figure 4.3, right plot) would be more appropriate. This plot shows that approximately 80% of the students who had planned to pre-register their dissertations, 50% of the students who were initially unsure, and only 33% of the students who had no plan to pre-register ended up with a pre-registered dissertation. BUT! We would lose the information about the raw values in the sample.\nIt’s all a trade-off, and the plot you choose depends on the “story” you want the data to tell.\n\n\n\n\n\n\nNote\n\n\n\nThe position argument position = \"stack\" is the default. Adding this argument to the code for the left plot in Figure 4.3 would produce the same plot as leaving the argument out.\n\n\nThe other option is a Grouped Barchart, which displays the bars next to each other. You can achieve this by changing the position argument to \"dodge\". You can see the default version of the plot in Figure 4.4 on the left, and one with additional layers on the right.\nInstead of using a pre-existing colour palette, we manually changed the colours using hex codes. These are some of the colours Gaby used in her PhD thesis, but you can:\n\ncreate your own colour hex codes by using this website, OR\nuse pre-defined colour names like “green” or “purple” instead. See a full list here.\n\nFeel free to explore.\nSince the legend title for the second plot is a bit long, we displayed the legend content across two rows by adding the layer guides(fill = guide_legend(nrow = 2)) at the end.\n\n## Default grouped barchart\nggplot(data_prp_viz, aes(x = Plan_prereg, fill = Pre_reg_group)) +\n  geom_bar(position = \"dodge\") + # add position argument here\n  theme(legend.position = \"bottom\") # move legend to the bottom\n\n## Prettier grouped barchart\nggplot(data_prp_viz, aes(x = Plan_prereg, fill = Pre_reg_group)) +\n  geom_bar(position = \"dodge\") + # add position argument here\n  # changing labels for x, y, and fill category - alternative method\n  labs(x = \"Pre-registration planned\", y = \"Count\", fill = \"Pre-registered dissertation\") +\n  # manual colour change for values\n  scale_fill_manual(values = c('#648FFF', '#DC267F'),\n                    labels = c(\"Yes\", \"No\")) +\n  scale_y_continuous(\n    # remove the space below the bars, but keep a tiny bit (5%) above\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  # pick a theme\n  theme_classic() + \n  # need to move this following line to the end otherwise the `theme_*` overrides it\n  theme(legend.position = \"bottom\") + \n  # display across 2 rows\n  guides(fill = guide_legend(nrow = 2))\n\n\n\n\n\nFigure 4.4: Default grouped barchart (left) and one with a few more layers added (right)\n\n\n\n\n\n\n\n\n\nSpecial case: Categorical variables with missing values\n\n\n\n\n\nIf we had chosen a different categorical variable that contains missing values, such as Closely_follow, our plots would have included those missing values by default. To change the colour of the missing value bars, you would need to specify this using the na.value = argument within the scale_fill() function. Here’s an example of a grouped barchart.\n\n# default grouped barchart with missing values\nggplot(data_prp_viz, aes(x = Plan_prereg, fill = Closely_follow)) +\n  geom_bar(position = \"dodge\") + \n  theme(legend.position = \"bottom\") + \n  guides(fill = guide_legend(nrow = 3)) # display across 3 rows\n\n## Prettier grouped barchart with missing values\nggplot(data_prp_viz, aes(x = Plan_prereg, fill = Closely_follow)) +\n  geom_bar(position = \"dodge\") + \n  labs(x = \"Pre-registration planned\", y = \"Count\", fill = \"Pre-registration followed\") +\n  # manual colour change for values of the factor and the NA responses\n  scale_fill_manual(values = c('#648FFF', '#DC267F'), na.value = '#FFB000') +\n  scale_y_continuous(\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  theme_classic() + \n  theme(legend.position = \"bottom\") + \n  guides(fill = guide_legend(nrow = 3)) # display across 3 rows\n\n\n\n\n\nFigure 4.5: Default grouped barchart (left) and one with a few more layers added (right) for a variable with missing values\n\n\n\nIf you don’t want the missing values to appear in the plot, you will need to do some data wrangling to remove them first. The function for this is drop_na(). Here we applied drop_na() to Closely_follow only.\n\n# remove NA\nprereg_plan_follow &lt;- data_prp_viz %&gt;% \n  select(Code, Plan_prereg, Closely_follow) %&gt;% \n  drop_na(Closely_follow)\n\n\n\n\n\n\n\ncheck NAs have been removed\n\n\n\n\n\n\n# check NA have been removed\nprereg_plan_follow %&gt;% \n  distinct(Plan_prereg, Closely_follow) %&gt;% \n  arrange(Plan_prereg, Closely_follow)\n\n\n\n\nPlan_prereg\nClosely_follow\n\n\n\nYes\nFollowed it somewhat\n\n\nYes\nFollowed it exactly\n\n\nUnsure\nFollowed it somewhat\n\n\nUnsure\nFollowed it exactly\n\n\nNo\nFollowed it somewhat\n\n\nNo\nFollowed it exactly\n\n\n\n\n\n\n\n\n\nBut keep in mind that it could misrepresent the data, e.g., giving a wrong impression about proportions. As a comparison…\n\n# with NA\nggplot(data_prp_viz, aes(x = Plan_prereg, fill = Closely_follow)) +\n  geom_bar(position = \"fill\") + # add position argument here\n  theme(legend.position = \"bottom\") + # move legend to the bottom\n  guides(fill = guide_legend(nrow = 2)) # display across 2 rows\n\n# without NA\nggplot(prereg_plan_follow, aes(x = Plan_prereg, fill = Closely_follow)) +\n  geom_bar(position = \"fill\") + # add position argument here\n  theme(legend.position = \"bottom\") + # move legend to the bottom\n  guides(fill = guide_legend(nrow = 2)) # display across 2 rows\n\n\n\n\n\nFigure 4.6: Percent stacked barchart with (left) and without missing values (right)"
  },
  {
    "objectID": "04-dataviz.html#activity-6-save-your-plots",
    "href": "04-dataviz.html#activity-6-save-your-plots",
    "title": "4  Data viz I",
    "section": "\n4.7 Activity 6: Save your plots",
    "text": "4.7 Activity 6: Save your plots\nYou can save your figures using the ggsave() function, which will save them to your project folder.\nThere are two ways to use ggsave(). If you don’t specify which plot to save, by default it will save the last plot you created. In our case, the last plot was the one without NA from the special case scenario (Figure 4.6). However, if you did not follow along with the special case scenario, your last plot will be the grouped bar chart on the right from Figure 4.4.\n\nggsave(filename = \"last_plot.png\")\n\n\n\n\n\n\n\nOur last plot saved\n\n\n\n\n\n\n\n\n\nThe second option is to save the plot as an object and refer to the object within ggsave(). As an example, let’s save the grouped barchart that contained missing values (Figure 4.4) as an object called grouped_bar.\nThe second option is to save the plot as an object and then refer to that object within ggsave(). For example, let’s save the grouped barchart that contained missing values (Figure 4.4) as an object called grouped_bar.\n\ngrouped_bar &lt;- ggplot(data_prp_viz, aes(x = Plan_prereg, fill = Closely_follow)) +\n  geom_bar(position = \"dodge\") + \n  labs(x = \"Pre-registration planned\", y = \"Count\", fill = \"Pre-registration followed\") +\n  # manual colour change for values of the factor and the NA responses\n  scale_fill_manual(values = c('#648FFF', '#DC267F'), na.value = '#FFB000') +\n  scale_y_continuous(\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  theme_classic() + \n  theme(legend.position = \"bottom\") + \n  guides(fill = guide_legend(nrow = 3)) # display across 3 rows\n\nThen, you can run the following line:\n\nggsave(filename = \"grouped_bar.png\", \n       plot = grouped_bar)\n\n\n\nSaving 7 x 5 in image\n\n\nThe filename is the name you want your PNG file to have, and plot refers to the name of the plot object.\n\n\n\n\n\n\nOur saved grouped_bar.png would look like this:\n\n\n\n\n\n\n\n\n\nThis is the plot saved with the default settings. If you like it, feel free to keep it as is. However, if it seems a bit “off”, you can adjust the width, height, and units (e.g., “cm”, “mm”, “in”, “px”). You might need to experiment with the dimensions until it feels about right.\n\nggsave(filename = \"grouped_bar2.png\", \n       plot = grouped_bar, \n       width = 16, height = 9, units = \"cm\")\n\n\n\n\n\n\n\ngrouped_bar.png with different dimensions"
  },
  {
    "objectID": "04-dataviz.html#pair-coding",
    "href": "04-dataviz.html#pair-coding",
    "title": "4  Data viz I",
    "section": "Pair-coding",
    "text": "Pair-coding\nTask 1: Open the R project for the lab\nTask 2: Create a new .Rmd file\n… and name it something useful. If you need help, have a look at Section 1.3.\nTask 3: Load in the library and read in the data\nThe data should already be in your project folder. If you want a fresh copy, you can download the data again here: data_pair_ch1.\nWe are using the package tidyverse today, and the datafile we should read in is dog_data_clean_wide.csv.\nTask 4: Create an appropriate plot\nPick any single or two categorical variables from the Binfet dataset and choose one of the appropriate plot choices. Things to think about:\n\n\nSelect your categorical variable(s): GroupAssignment, Year_of_Study, Live_Pets, and/or Consumer_BARK\n\n\nDecide on the plot you want to display: barchart, stacked barchart, percent stacked barchart, or grouped barchart\n\nYou may need to convert your variables into factors\n\nThink about what you want to do with missing data\n\nPick a colour scheme (manual or pre-defined colour palette)\n\nTidy the axes labels\n\nDecide whether you need a legend or not, and if so, where you would want to place it\n\nRemove the gap between the bottom of the chart and the bars\n\nPick a theme\n\n\n\n\n\n\n\nPossible solution for a plot with 1 categorical variable\n\n\n\n\n\nConverting some variables into factors\n\ndog_data_wide &lt;- dog_data_wide %&gt;% \n  mutate(Year_of_Study = factor(Year_of_Study,\n                                levels = c(\"First\", \"Second\", \"Third\", \"Fourth\", \"Fifth or above\")))\n\nNow we can plot\n\nggplot(dog_data_wide, aes(x = Year_of_Study, fill = Year_of_Study)) +\n  geom_bar() + \n  scale_fill_brewer(\n    palette = \"Dark2\",\n    guide = \"none\") + \n  scale_x_discrete(name = \"Year of Study\") + \n  scale_y_continuous(name = \"Count\",\n                     expand = expansion(mult = c(0, 0.05))) + \n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPossible solution for a plot with 2 categorical variables\n\n\n\n\n\nConverting some variables into factors\n\ndog_data_wide &lt;- dog_data_wide %&gt;% \n  mutate(GroupAssignment = factor(GroupAssignment,\n                                  levels = c(\"Direct\", \"Indirect\", \"Control\")))\n\nNow we can plot\n\nggplot(dog_data_wide, aes(x = GroupAssignment , fill = Live_Pets)) +\n  geom_bar(position = \"fill\") + \n  labs(x = \"Experimental Group\", y = \"Count\", fill = \"Pets at Home\") +\n  scale_fill_manual(values = c('deeppink', 'springgreen2'), na.value = 'orangered',\n                    labels = c(\"Yes\", \"No\")) +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +\n  theme_classic() + \n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "04-dataviz.html#test-your-knowledge",
    "href": "04-dataviz.html#test-your-knowledge",
    "title": "4  Data viz I",
    "section": "Test your knowledge",
    "text": "Test your knowledge\nLet’s go back to the palmerpenguins package (https://allisonhorst.github.io/palmerpenguins/), and assume you have the following data available:\n\nlibrary(palmerpenguins)\n\npenguin_selection &lt;- penguins %&gt;% \n  group_by(species, island) %&gt;% \n  summarise(penguin_count = n())\n\npenguin_selection\n\n\n\n\nspecies\nisland\npenguin_count\n\n\n\nAdelie\nBiscoe\n44\n\n\nAdelie\nDream\n56\n\n\nAdelie\nTorgersen\n52\n\n\nChinstrap\nDream\n68\n\n\nGentoo\nBiscoe\n124\n\n\n\n\n\n\nKnowledge check\nQuestion 1\nWhat geom would you use to plot penguin count for each species? \ngeom_bar\ngeom_col\nQuestion 2\nWhat mapping would you use to display penguin count across species?\n\naes(x = penguin_count, y = species)aes(x = species, y = penguin_count)aes(x = species)aes(x = penguin_count)\n\nQuestion 3\nWhat geom would you use to count the number of species on each island? \ngeom_bar\ngeom_col\nQuestion 4\nWhat mapping would you use to display the count of species per island?\n\naes(x = island, y = species)aes(x = species, y = island)aes(x = island)aes(x = species)\n\n\n\n\n\n\n\nExplain these answers\n\n\n\n\n\nQuestion 1: geom_col() is the appropriate choice for bar charts with predefined y-values, such as penguin_count.\nQuestion 2: The correct aesthetic mapping places the categorical variable (species) on the x-axis and the numeric variable (number of observed penguins) on the y-axis. Using aes(x = penguin_count, y = species) would flip the axes, placing the number of penguins on the x-axis and species on the y-axis, which doesn’t match the conventional structure of a bar chart.\nQuestion 3: geom_bar() is the appropriate choice when you want to automatically count the number of observations within each category, such as counting the number of penguin species on each island.\nQuestion 4: For a simple count of species per island, you only need to map the categorical variable (island) to the x-axis. The y-axis will automatically represent counts when using geom_bar().\n\n\n\nError mode\nSome of the code chunks contain mistakes and result in errors, while others do not produce the expected results. Your task is to identify any issues, explain why they occurred, and, if possible, fix them.\nQuestion 5\nWe want to plot the number of penguins across the different islands.\n\nggplot(penguins, aes(x = islands)) +\n  geom_bar()\n\nError in `geom_bar()`:\n! Problem while computing aesthetics.\nℹ Error occurred in the 1st layer.\nCaused by error in `check_aesthetics()`:\n! Aesthetics must be either length 1 or the same as the data (344).\n✖ Fix the following mappings: `x`.\n\n\nWhat does this error message mean and how do you fix it?\n\n\n\n\n\n\nExplain the solution\n\n\n\n\n\nThe error message consists of 2 parts. Part 1 is perhaps a bit trickier to interpret, but part 2 gives some useful hints:\n\n\n“Aesthetics must be either length 1 or the same as the data (344)”: This means that the variable mapped to x should either be a constant (like a single value) or a column that has 344 entries (matching the number of rows in the penguins dataset).\n\n“Fix the following mappings: x”: The issue is specifically with the x aesthetic, meaning islands is either misspelled or doesn’t exist in the dataset.\n\nTo check the penguins data, you can use glimpse().\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nTo fix the error, you need to correct the column name. The correct column in the penguins dataset is called island (without the “s” at the end). The island column has 344 entries, just like the rest of the dataset, so the mapping now works properly.\n\nggplot(penguins, aes(x = island)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\nQuestion 6\nNext, we want to create a grouped bar chart displaying species per island, using the viridis color palette.\n\nggplot(penguins, aes(x = island, fill = species)) +\n  geom_bar(position = \"dodge\") +\n  scale_fill_viridis()\n\nError in scale_fill_viridis(): could not find function \"scale_fill_viridis\"\n\n\nWhat does this error message mean and how do you fix it?\n\n\n\n\n\n\nExplain the solution\n\n\n\n\n\nThe function scale_fill_viridis() is incorrect; the correct function is called scale_fill_viridis_d().\nFIX: correct the function name to display the grouped bar chart with the viridis color palette.\n\nggplot(penguins, aes(x = island, fill = species)) +\n  geom_bar(position = \"dodge\") +\n  scale_fill_viridis_d()\n\n\n\n\n\n\n\n\n\n\nQuestion 7\nWe want to create a grouped bar chart showing the number of penguins on each island, broken down by year.\n\nggplot(penguins, aes(x = island, fill = year)) +\n  geom_bar(position = \"dodge\")\n\nWarning: The following aesthetics were dropped during statistical transformation: fill.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\n\n\n\nHmmm. We got a plot, but certainly not the one we intended. The warning message mentions something about the grouping structure and gives some additional hints.\n\n\n\n\n\n\nExplain the solution\n\n\n\n\n\nThe grouping variable needs to be a factor. R helpfully asks if we’ve forgotten to convert a numerical variable into a factor!!! Oh, let’s check that in the penguins data using the glimpse() function.\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nIndeed, year is currently stored as a numeric (integer) variable. To fix this, we need to convert year to a factor. We can do this directly within the ggplot() function.\n\nggplot(penguins, aes(x = island, fill = as.factor(year))) +\n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n\nQuestion 8\nWe want to create a percent stacked bar chart that displays the ratio of penguins’ sex on each island, using a manual color palette. Female penguins should be displayed in blue, males in green, and NA values in red.\nNote: This task is trickier than it looks. Although the code runs and produces a plot, there are three mistakes to identify and fix.\n\nggplot(penguins, aes(x = sex, fill = island)) +\n  geom_bar(position = \"dodge\") +\n  scale_fill_manual(values = c(\"blue\", \"green\", \"red\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplain the solution\n\n\n\n\n\n\n\n\n\n\n\nHint for Mistake 1\n\n\n\n\n\nThe task was to create a percent stacked barchart, but the current plot is displaying a grouped barchart. You will need to adjust the argument that defines the type of plot to achieve the correct visualisation.\n\n\n\n\n\n\nFixing of Mistake 1\n\n\n\n\n\n\nggplot(penguins, aes(x = sex, fill = island)) +\n  geom_bar(position = \"fill\") +\n  scale_fill_manual(values = c(\"blue\", \"green\", \"red\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint for Mistake 2\n\n\n\n\n\nYou may have also noticed that the colours are mapped to the islands, not to the penguins’ sex, which needs to be corrected.\n\n\n\n\n\n\nFixing of Mistake 2\n\n\n\n\n\nWe need to switch the columns mapped to the x and fill aesthetics.\n\nggplot(penguins, aes(x = island, fill = sex)) +\n  geom_bar(position = \"fill\") +\n  scale_fill_manual(values = c(\"blue\", \"green\", \"red\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint for Mistake 3\n\n\n\n\n\nNow that the correct variables are mapped to the x-axis and the fill argument, the colour scheme no longer matches the instructions. According to the guidelines, missing values should be displayed in red.\n\n\n\n\n\n\nFixing of Mistake 3\n\n\n\n\n\nChanging the colour of missing values is a special case that requires the argument na.value =.\n\nggplot(penguins, aes(x = island, fill = sex)) +\n  geom_bar(position = \"fill\") +\n  scale_fill_manual(values = c(\"blue\", \"green\"), na.value = \"red\")"
  },
  {
    "objectID": "appendix-a-installing-r.html#how-to-install-r-and-rstudio",
    "href": "appendix-a-installing-r.html#how-to-install-r-and-rstudio",
    "title": "Appendix A — Installing R",
    "section": "How to install R and RStudio",
    "text": "How to install R and RStudio\nThe RSetGo book provides detailed instructions on how to install R and RStudio on your computer. It also includes links to walkthroughs for installing R on different types of computers and operating systems."
  },
  {
    "objectID": "appendix-b-updating-packages.html#updating-rstudio",
    "href": "appendix-b-updating-packages.html#updating-rstudio",
    "title": "Appendix B — Updating R, RStudio, and packages",
    "section": "\nB.1 Updating RStudio",
    "text": "B.1 Updating RStudio\nRStudio is the easiest component to update. Typically, updates to RStudio won’t affect your code, instead they add in new features, like spell-check or upgrades to what RStudio can do. There’s usually very little downside to updating RStudio and it’s easy to do.\nClick Help &gt; Check for updates\n\n\n\n\nUpdating RStudio\n\n\n\nIf an update is available, it will prompt you to download it and you can install it as usual."
  },
  {
    "objectID": "appendix-b-updating-packages.html#updating-r",
    "href": "appendix-b-updating-packages.html#updating-r",
    "title": "Appendix B — Updating R, RStudio, and packages",
    "section": "\nB.2 Updating R",
    "text": "B.2 Updating R\nFinally, you may also wish to update R itself. The key thing to be aware of is that when you update R, if you just download the latest version from the website, you will lose all your packages.\n\nB.2.1 Windows\nThe easiest way to update R on Windows and not cause yourself a huge headache is to use the installr package. When you use the updateR() function, a series of dialogue boxes will appear. These should be fairly self-explanatory but there is a full step-by-step guide available for how to use installr, the important bit is to select “Yes” when it asked if you would like to copy your packages from the older version of R.\n\n# Install the installr package\ninstall.packages(\"installr\")\n\n# Run the update function\ninstallR::updateR()\n\n\nB.2.2 Mac\nFor a Mac, you can use the updateR package. You’ll need to install this from GitHub. You will be asked to type your system password (that you use to log into your computer) in the console pane. If relevant, it will ask you if you want to restore your packages for a new major version.\n\n# install from github\ndevtools::install_github(\"AndreaCirilloAC/updateR\")\n\n# update your R version, you will need your system password\nupdateR::updateR()"
  },
  {
    "objectID": "appendix-b-updating-packages.html#updating-packages",
    "href": "appendix-b-updating-packages.html#updating-packages",
    "title": "Appendix B — Updating R, RStudio, and packages",
    "section": "\nB.3 Updating packages",
    "text": "B.3 Updating packages\nPackage developers will occasionally release updates to their packages. This is typically to add in new functions to the package, or to fix or amend existing functions. Be aware that some package updates may cause your previous code to stop working. This does not tend to happen with minor updates to packages, but occasionally with major updates, you can have serious issues if the developer has made fundamental changes to how the code works. For this reason, we recommend updating all your packages once at the beginning of each academic year (or semester) - don’t do it before an assessment or deadline just in case!\nTo update an individual package, the easiest way is to use the install.packages() function, as this always installs the most recent version of the package.\n\ninstall.packages(\"tidyverse\")\n\nTo update multiple packages, or indeed all packages, RStudio provides helpful tools. Click Tools &gt; Check for Package Updates. A dialogue box will appear and you can select the packages you wish to update. Be aware that if you select all packages, this may take some time and you will be unable to use R whilst the process completes.\n\n\n\n\nUpdating packages with RStudio"
  },
  {
    "objectID": "appendix-b-updating-packages.html#sec-package-install-troubleshooting",
    "href": "appendix-b-updating-packages.html#sec-package-install-troubleshooting",
    "title": "Appendix B — Updating R, RStudio, and packages",
    "section": "\nB.4 Troubleshooting",
    "text": "B.4 Troubleshooting\nOccasionally, you might have a few problem packages that seemingly refuse to update. For me, rlang and vctrs cause me no end of trouble. These aren’t packages that you will likely every explicitly load, but they’re required beneath the surface for R to do things like knit your Markdown files etc.\n\nB.4.1 Non-zero exit status\nIf you try to update a package and get an error message that says something like Warning in install.packages : installation of package ‘vctrs’ had non-zero exit status or perhaps Error in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]) :  namespace 'rlang' 0.4.9 is being loaded, but &gt;= 0.4.10 is required one solution I have found is to manually uninstall the package, restart R, and then install the package new, rather than trying to update an existing version. The installr package also has a useful function for uninstalling packages.\n\n# Load installr\nlibrary(installr)\n\n# Uninstall the problem package\nuninstall.packages(\"package_name\")\n\n# Then restart R using session - restart R\n# Then install the package fresh\n\ninstall.packages(\"package\")\n\n\nB.4.2 Cannot open file\nYou may get the following error after trying to install any packages at all:\n\nError in install packages : Cannot open file ‘C:/…..’: Permission denied\n\nThis usually indicates a permissions problem with writing to the default library (the folder that packages are kept in). Sometimes this means that you need to install R and RStudio as administrator or run it as administrator.\nOne other fix may be to change the library location using the following code (check in “C:/Program Files/R” for what version you should have instead of “R-3.5.2”):\n\n# change the library path\n.libPaths(c(\"C:/Program Files/R/R-3.5.2/library\"))\n\nIf that works and you can install packages, set this library path permanently:\n\nInstall the usethis package\nRun usethis::edit_r_profile() in the console; it will open up a blank file\nPaste into the file (your version of): .libPaths(c(\"C:/Program Files/R/R-3.5.2/library\"))\n\nSave and close the file\nRestart R for changes to take effect\n\nThe code in your .Rprofile will now run every time you start up R.\nAs always, if you’re having issues, please ask on Teams or come to office hours."
  },
  {
    "objectID": "appendix-d-symbols.html",
    "href": "appendix-d-symbols.html",
    "title": "Appendix C — Symbols",
    "section": "",
    "text": "Symbol\npsyTeachR Term\nAlso Known As\n\n\n\n()\n(round) brackets\nparentheses\n\n\n[]\nsquare brackets\nbrackets\n\n\n{}\ncurly brackets\nsquiggly brackets\n\n\n&lt;&gt;\nchevrons\nangled brackets / guillemets\n\n\n&lt;\nless than\n\n\n\n&gt;\ngreater than\n\n\n\n&\nampersand\n“and” symbol\n\n\n#\nhash\npound / octothorpe\n\n\n/\nslash\nforward slash\n\n\n\\\nbackslash\n\n\n\n-\ndash\nhyphen / minus\n\n\n_\nunderscore\n\n\n\n*\nasterisk\nstar\n\n\n^\ncaret\npower symbol\n\n\n~\ntilde\ntwiddle / squiggle\n\n\n=\nequal sign\n\n\n\n==\ndouble equal sign\n\n\n\n.\nfull stop\nperiod / point\n\n\n!\nexclamation mark\nbang / not\n\n\n?\nquestion mark\n\n\n\n’\nsingle quote\nquote / apostrophe\n\n\n”\ndouble quote\nquote\n\n\n%&gt;%\npipe\nmagrittr pipe\n\n\n|\nvertical bar\npipe\n\n\n,\ncomma\n\n\n\n;\nsemi-colon\n\n\n\n:\ncolon\n\n\n\n@\n“at” symbol\nvarious hilarious regional terms\n\n\n…\nglossary(\"ellipsis\")\ndots\n\n\n\n\n\n\n\nImage by James Chapman/Soundimals"
  },
  {
    "objectID": "appendix-x-How-to-cite-R.html",
    "href": "appendix-x-How-to-cite-R.html",
    "title": "Appendix D — Citing R and RStudio",
    "section": "",
    "text": "How to cite R and RStudio\nYou may be some way off writing a scientific report where you have to cite and reference R, however, when the time comes it is important to do so to give the people who built it (most of them for free!) credit. You should provide separate citations for R, RStudio, and the packages you use.\nTo get the citation for the version of R you are using, simply run the citation() function which will always provide you with the most recent citation.\n\ncitation()\n\nTo cite R in publications use:\n\n  R Core Team (2024). _R: A Language and Environment for Statistical\n  Computing_. R Foundation for Statistical Computing, Vienna, Austria.\n  &lt;https://www.R-project.org/&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2024},\n    url = {https://www.R-project.org/},\n  }\n\nWe have invested a lot of time and effort in creating R, please cite it\nwhen using it for data analysis. See also 'citation(\"pkgname\")' for\nciting R packages.\n\n\nTo generate the citation for any packages you are using, you can also use the citation() function with the name of the package you wish to cite.\n\ncitation(\"tidyverse\")\n\nTo cite package 'tidyverse' in publications use:\n\n  Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R,\n  Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller\n  E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V,\n  Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). \"Welcome to\n  the tidyverse.\" _Journal of Open Source Software_, *4*(43), 1686.\n  doi:10.21105/joss.01686 &lt;https://doi.org/10.21105/joss.01686&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Article{,\n    title = {Welcome to the {tidyverse}},\n    author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},\n    year = {2019},\n    journal = {Journal of Open Source Software},\n    volume = {4},\n    number = {43},\n    pages = {1686},\n    doi = {10.21105/joss.01686},\n  }\n\n\nTo generate the citation for the version of RStudio you are using, you can use the RStudio.Vesion() function:\n\nRStudio.Version()\n\nFinally, here’s an example of how that might look in the write-up of your method section:\n\nAnalysis was conducted using R (R Core Team, 2020), RStudio (Rstudio Team, 2020), and the tidyverse package (Wickham, 2017).\n\nAs noted, you may not have to do this for a while, but come back to this when you do because it’s important to give the open-source community credit for their work."
  },
  {
    "objectID": "appendix-y-license.html",
    "href": "appendix-y-license.html",
    "title": "License",
    "section": "",
    "text": "This book is licensed under Creative Commons Attribution-ShareAlike 4.0 International License (CC-BY-SA 4.0). You are free to share and adapt this book. You must give appropriate credit, provide a link to the license, and indicate if changes were made. If you adapt the material, you must distribute your contributions under the same license as the original."
  },
  {
    "objectID": "01-basics.html#activity-4-installing-packages-loading-packages-and-reading-in-data",
    "href": "01-basics.html#activity-4-installing-packages-loading-packages-and-reading-in-data",
    "title": "1  Projects and R Markdown",
    "section": "\n1.5 Activity 4: Installing packages, loading packages, and reading in data",
    "text": "1.5 Activity 4: Installing packages, loading packages, and reading in data\n\n1.5.1 Installing packages\nWhen you install R and RStudio for the first time (or after an update), most of the packages we will be using won’t be pre-installed. Before you can load new packages like tidyverse, you will need to install them.\nIf you try to load a package that has not been installed yet, you will receive an error message that looks something like this: Error in library(tidyverse) : there is no package called 'tidyverse'.\nTo fix this, simply install the package first. In the console, type the command install.packages(\"tidyverse\"). This only needs to be done once after a fresh installation. After that, you will be able to load the tidyverse package into your library whenever you open RStudio.\n\n\n\n\n\n\nInstall packages from the console only\n\n\n\nNever include install.packages() in the Rmd. Only install packages from the console pane or the packages tab of the lower right pane!!!\n\n\nNote, there will be other packages used in later chapters that will also need to be installed before their first use, so this error is not limited to tidyverse.\n\n1.5.2 Loading packages and reading in data\nThe first step is to load in the packages we need and read in the data. Today, we’ll only be using tidyverse, and read_csv() will help us store the data from prp_data_reduced.csv in an object called data_prp.\nCopy the code into a code chunk in your .Rmd file and run it. You can either click the green error to run the entire code chunk, or use the shortcut Ctrl + Enter (Windows) or Cmd + Enter (Mac) to run a line of code/ pipe from the Rmd.\n\nlibrary(tidyverse)\ndata_prp &lt;- read_csv(\"prp_data_reduced.csv\")\n\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nRows: 89 Columns: 91\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (17): Code, Age, Ethnicity, Opptional_mod_1_TEXT, Research_exp_1_TEXT, U...\ndbl (74): Gender, Secondyeargrade, Opptional_mod, Research_exp, Plan_prereg,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  }
]